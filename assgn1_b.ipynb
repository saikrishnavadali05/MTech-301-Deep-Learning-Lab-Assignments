{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A 2 layer shallow network for binary classification\n",
    "In this assignment you will build a two layer network for the same cat vs non-cat binary classification problem. First, lets import the required packages. Note that we have copied the functions 'flatten', 'load_train_data' and 'load_test_data' functions to 'assign1_utils.py'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib # for plotting\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from assign1_utils import load_train_data, load_test_data, flatten\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture to be implemented is as follows:\n",
    "Ip layer(I)----->hidden layer(H)------>op layer(O)\n",
    "\n",
    "- Ip features shape: nx$^{[0]}$ x m (a batch of m samples each of dim nx. In this assignment, nx$^{[0]}$ will be 64\\*64*3 = 12288).\n",
    "- weights between I and H have shape: nx$^{[1]}$ x nx$^{[0]}$. nx$^{[1]}$ = 32.\n",
    "- bias vector at H has shape: nx$^{[1]}$ x 1\n",
    "- non-linearity at hidden layer is ReLU\n",
    "- weights between H and O have shape: nx$^{[2]}$ x nx$^{[1]}$. nx$^{[2]}$ = 1.\n",
    "- bias vector at H has shape: nx$^{[2]}$ x 1\n",
    "- non-linearity at output layer is Sigmoid\n",
    "\n",
    "The implementation will follow the python style pseudo-code listed in my lecture notes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will complete the function that intializes weights and biases and returns them. Weight matrices have to be initialized similar to how weight vector was initialized in logistic regression. Bias vectors have to be initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(nx):\n",
    "    \"\"\"\n",
    "      Function that intializes weights to scaled random std normal values and biases to zero and returns them\n",
    "      \n",
    "      nx: a list that contains number of nodes in each layer in order. For a l-layer network, len(nx) = l+1 \n",
    "          as it includes num of features in input layer also.\n",
    "          \n",
    "      returns W: list of numpy arrays of weight matrices\n",
    "              b: list of numpy arrays of bias vectors\n",
    "    \"\"\"\n",
    "    Wlist = []\n",
    "    blist = []\n",
    "    for i in range(1, len(nx)): \n",
    "        Wlist.append(np.random.randn(nx[i], nx[i-1]) * 0.01) # replace the ...; np.random.randn will be useful\n",
    "        blist.append(np.zeros((nx[i], 1))) # replace the ...; np.zeros will be useful\n",
    "    \n",
    "    return Wlist, blist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W[0]: (2, 3), Shape of b[0]: (2, 1)\n",
      "Shape of W[1]: (1, 2), Shape of b[1]: (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment the following two lines to test your function\n",
    "\n",
    "W, b = initialize_params([3, 2, 1])\n",
    "[print(f'Shape of W[{i}]: {W[i].shape}, Shape of b[{i}]: {b[i].shape}') for i in range(len(W))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will complete forward, backward, update_params and part of the main function. Functions f and df are already comlete. Look at the code to understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(z, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      computes and returns the non-linear function of z given the non-linearity\n",
    "      \n",
    "      z: numpy array of any shape on which the non-linearity will be applied elementwise\n",
    "      fname: a string that is name of the non-linearity. Defaults to 'ReLU'. Other valid values are\n",
    "             'Sigmoid', 'Tanh', and 'Linear'.\n",
    "      \n",
    "      returns f(z) f is the non-linear function whose name is fname\n",
    "    \"\"\"\n",
    "    if fname == 'ReLU':\n",
    "        return np.maximum(z, 0)\n",
    "    elif fname == 'Sigmoid':\n",
    "        return 1./(1+np.exp(-z))\n",
    "    elif fname == 'Tanh':\n",
    "        return np.tanh(z)\n",
    "    elif fname == 'Linear':\n",
    "        return z\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linear function error')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(z, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      computes and returns the derivative of the non-linear function of z with respect to z\n",
    "      \n",
    "      z: numpy array of any shape \n",
    "      fname: a string that is name of the non-linearity. Defaults to 'ReLU'. Other valid values are\n",
    "             'Sigmoid', 'Tanh', and 'Linear'.\n",
    "      \n",
    "      returns df/dz where f is the non-linear function of z. Name of the non-linear function is fname.\n",
    "    \"\"\"\n",
    "    if fname == 'ReLU':\n",
    "        return z>0\n",
    "    elif fname == 'Sigmoid':\n",
    "        sigma_z = 1./(1+np.exp(-z))\n",
    "        return sigma_z * (1-sigma_z)\n",
    "    elif fname == 'Tanh':\n",
    "        return 1 - np.tanh(z)**2\n",
    "    elif fname == 'Linear':\n",
    "        return np.ones(z.shape)\n",
    "    else:\n",
    "        raise ValueError('Unknown non-linear function error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(a, W, b, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      Forward propagates a through the current layer given W and b\n",
    "      a: I/p activation from previous activation layer l-1 of shape nx[l-1] x m\n",
    "      w: weight matrix of shape nx[l] x nx[l-1]\n",
    "      b: bias vector of shape nx[l+1] x 1\n",
    "      \n",
    "      returns anew: the output activation from current layer of shape nx[l] x m\n",
    "              cache: a tuple that contains current layer's linear computation z, previous layer's activation a,\n",
    "                     current layer's activation anew and weight matrix W\n",
    "    \"\"\"\n",
    "    # Fill rhs in the following 3 lines. No extra lines of code required.\n",
    "    \n",
    "    z = np.matmul(W, a) + b                       # np.dot or np.matmul or @ operator will be useful. Also understand numpy \n",
    "                               # broadcasting for adding vector b to product of W and a\n",
    "    anew = f(z, fname)                   # function f defined above will be useful\n",
    "    cache = (z, a, anew, W)                 # read the doc string for this function listed above and acoordingly fill rhs\n",
    "    return anew, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(da, cache, fname = 'ReLU'):\n",
    "    \"\"\"\n",
    "      Backward propagates da through the current layer given da, cache and the non-linearity at the current layer\n",
    "      da: derivative of loss with respect current layers activation a; shape is nx[l] x m\n",
    "      cache: a tuple that contains current layer's linear computation z, previous layer's activation aprev,\n",
    "                     current layer's activation a and weight matrix W between previous layer l-1 and current layer l\n",
    "      fname: name of the non-linearity at current layer l; this will be helpful for local gradient computation in \n",
    "             chain rule\n",
    "      \n",
    "      returns dW: derivative of loss with respect to W; shape is nx[l] x nx[l-1]\n",
    "              db: derivative of loss with respect to b; shape is nx[l] x 1\n",
    "    \"\"\"\n",
    "    # Fill rhs in the following 5 lines. No extra lines of code required.\n",
    "    \n",
    "    z, aprev, a, W = cache                     # extract from cache\n",
    "    dz = da * df(z, fname)                     # compute dz as incoming grad da * local grad. For local grad, function df defined \n",
    "                                               # above will be useful\n",
    "    dW = np.matmul(dz, aprev.T)                # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                                               # transposing\n",
    "    db = np.sum(dz, axis=1, keepdims=True)     # np.sum will be useful\n",
    "    daprev = np.matmul(W.T, dz)               # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                                               # transposing\n",
    "    return daprev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(Wlist, blist, dWlist, dblist, alpha):\n",
    "    \"\"\"\n",
    "      Updates all the parameters using gradient descent rule\n",
    "      \n",
    "      Wlist: a lsit of all weight matrices to be updated\n",
    "      blist: a list of bias vectors to be updated\n",
    "      dWlist: a list of gradients of loss with respect to weight matrices\n",
    "      dblist: a list of gradients of loss with respect to bias vectors\n",
    "      alpha: learning rate\n",
    "    \"\"\"\n",
    "    for i in range(len(Wlist)):\n",
    "        Wlist[i] -= alpha * dWlist[i]         # fill rhs\n",
    "        blist[i] -= alpha * dblist[i]         # fill rhs\n",
    "        # ADDED\n",
    "#         print(dWlist[0], dblist[0])\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(): # main function to train the model\n",
    "def train(lr = 0.001, iters = 5000, n_layers = 2, n_x = [64*64*3, 32, 1], fun_name_list = ['ReLU', 'Sigmoid']) :   \n",
    "    # load train data\n",
    "    a0, y = load_train_data()\n",
    "    a0 = flatten(a0)\n",
    "    a0 = a0/255. # normalize the data to [0, 1]    \n",
    "    \n",
    "    # set some hyperparameters and epsilon\n",
    "#     alpha = 0.001    \n",
    "    alpha = lr    \n",
    "#     miter = 5000\n",
    "    miter = iters\n",
    "    epsilon = 1e-6\n",
    "#     num_layers = 2\n",
    "    num_layers = n_layers\n",
    "#     nx = [a0.shape[0], 32, 1]\n",
    "    nx = n_x\n",
    "#     print('sai: ',nx)\n",
    "    m = a0.shape[1]\n",
    "#     fname_list = ['ReLU', 'Sigmoid']\n",
    "    fname_list = fun_name_list\n",
    "    \n",
    "    loss1, loss2, loss3 = np.inf, np.inf, np.inf\n",
    "    \n",
    "    # initialize weights and biases\n",
    "    np.random.seed(24)\n",
    "    Wlist, blist = initialize_params(nx)      # fill rhs \n",
    "    loss = []\n",
    "    # initialize list of caches from each layer, gradients of weights at each layer, gradients of biases at\n",
    "    # each layer to empty\n",
    "    cache, dWlist, dblist = ([None]*num_layers for i in range(3))\n",
    "    \n",
    "    for i in range(miter):\n",
    "        a = a0\n",
    "        # forward propagate through each layer\n",
    "        for l in range(num_layers):\n",
    "            a, cache[l] = forward(a, Wlist[l], blist[l], fname_list[l])                       # Fill rhs. call forward function with \n",
    "                                                # appropriate arguments\n",
    "\n",
    "        L  = (-1/m) * np.sum(y * np.log(a) + (1-y) * np.log(1-a), axis=1)                                    # Fill rhs. compute loss L\n",
    "        \n",
    "#         loss1 = loss2\n",
    "#         loss2 = loss3\n",
    "#         loss3 = L\n",
    "        \n",
    "#         if L > loss2 and loss2 < loss1:\n",
    "#             alpha = alpha / 2.0\n",
    "#             print(\"updated alpha: \", alpha)\n",
    "        if i+1 % 10000 == 0:\n",
    "            alpha = alpha / 2.0\n",
    "            print(\"updated alpha: \", alpha)\n",
    "            \n",
    "        da = (1/m) * (a - y) * (1 / ((a * (1 - a)) + epsilon))                                 # Fill rhs. compute da\n",
    "\n",
    "        # backward propagate through each layer to compute gradients\n",
    "        for l in range(num_layers-1, -1, -1):\n",
    "            da, dWlist[l], dblist[l] =  backward(da, cache[l], fname_list[l])                 # Fill rhs. call backward function with \n",
    "                                                        # appropriate arguments\n",
    "\n",
    "        # update_params\n",
    "        update_params(Wlist, blist, dWlist, dblist, alpha)          # Replace ...; call update_params function with appropriate arguments\n",
    "        loss.append(L)\n",
    "        if not i%100: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.8f}')\n",
    "                print(f'train accuracy: {test_model(a0, y, Wlist, blist, fun_name_list) * 100:.2f}%')\n",
    "                print('\\n')\n",
    "                \n",
    "    return Wlist, blist, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): # main function to train the model\n",
    "# def train(lr = 0.001, iters = 5000, n_layers = 2, n_x = [batch_size, 32, 1], fun_name_list = ['ReLU', 'Sigmoid'])    \n",
    "    # load train data\n",
    "    a0, y = load_train_data()\n",
    "    a0 = flatten(a0)\n",
    "    a0 = a0/255. # normalize the data to [0, 1]    \n",
    "    \n",
    "    # set some hyperparameters and epsilon\n",
    "    alpha = 0.01    \n",
    "    miter = 2000\n",
    "    epsilon = 1e-6\n",
    "    num_layers = 2\n",
    "    nx = [a0.shape[0], 32, 1]\n",
    "    m = a0.shape[1]\n",
    "    fname_list = ['ReLU', 'Sigmoid']\n",
    "    # initialize weights and biases\n",
    "    Wlist, blist = initialize_params(nx)      # fill rhs \n",
    "    \n",
    "    # initialize list of caches from each layer, gradients of weights at each layer, gradients of biases at\n",
    "    # each layer to empty\n",
    "    cache, dWlist, dblist = ([None]*num_layers for i in range(3))\n",
    "    dw_list = [[0] * miter, [0] * miter]\n",
    "    for i in range(miter):\n",
    "        a = a0\n",
    "        # forward propagate through each layer\n",
    "        for l in range(num_layers):\n",
    "            a, cache[l] = forward(a, Wlist[l], blist[l], fname_list[l])                       # Fill rhs. call forward function with \n",
    "                                                # appropriate arguments\n",
    "\n",
    "        L  = (-1/m) * np.sum(y * np.log(a) + (1-y) * np.log(1-a), axis=1)                              # Fill rhs. compute loss L\n",
    "        \n",
    "        \n",
    "        \n",
    "        da = (1/m) * (a - y) * (1 / ((a * (1 - a)) + epsilon))                                 # Fill rhs. compute da\n",
    "\n",
    "        # backward propagate through each layer to compute gradients\n",
    "        for l in range(num_layers-1, -1, -1):\n",
    "            da, dWlist[l], dblist[l] =  backward(da, cache[l], fname_list[l])                 # Fill rhs. call backward function with \n",
    "            dw_list[l][i] = np.linalg.norm(dWlist[l])                                            # appropriate arguments\n",
    "        # update_params\n",
    "        update_params(Wlist, blist, dWlist, dblist, alpha)          # Replace ...; call update_params function with appropriate arguments\n",
    "        \n",
    "        if not i%100: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "#                 for j in range(len(Wlist)):\n",
    "#                     print(\"Norm(dW)\",j,\": \",np.linalg.norm(dWlist[j]))\n",
    "    \n",
    "#     x_axis = [i for i in range(1, miter + 1)]\n",
    "#     colours = ['b', 'r']\n",
    "#     for i in range(num_layers):\n",
    "#         plt.plot(x_axis, dw_list[i], colours[i], label = 'dWlist['+str(i)+']')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel(\"\")\n",
    "#     plt.show()\n",
    "\n",
    "                \n",
    "    return Wlist, blist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6961\n",
      "Loss at iteration 100:\t0.6372\n",
      "Loss at iteration 200:\t0.5972\n",
      "Loss at iteration 300:\t0.5319\n",
      "Loss at iteration 400:\t0.4537\n",
      "Loss at iteration 500:\t0.4351\n",
      "Loss at iteration 600:\t0.3945\n",
      "Loss at iteration 700:\t0.3447\n",
      "Loss at iteration 800:\t0.2979\n",
      "Loss at iteration 900:\t0.2175\n",
      "Loss at iteration 1000:\t0.3820\n",
      "Loss at iteration 1100:\t0.1478\n",
      "Loss at iteration 1200:\t0.1013\n",
      "Loss at iteration 1300:\t0.0767\n",
      "Loss at iteration 1400:\t0.0637\n",
      "Loss at iteration 1500:\t0.0510\n",
      "Loss at iteration 1600:\t0.0427\n",
      "Loss at iteration 1700:\t0.0367\n",
      "Loss at iteration 1800:\t0.0318\n",
      "Loss at iteration 1900:\t0.0279\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Wlist, blist = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.00%\n",
      "test accuracy: 74.00%\n"
     ]
    }
   ],
   "source": [
    "fname_list = ['ReLU', 'Sigmoid']\n",
    "num_layers = 2\n",
    "def predict(a, Wlist, blist, fname_list):\n",
    "    for l in range(num_layers):\n",
    "            a, _ = forward(a, Wlist[l], blist[l], fname_list[l])\n",
    "    predictions = np.zeros_like(a)\n",
    "    predictions[a > 0.5] = 1\n",
    "    return predictions\n",
    "\n",
    "def test_model(a, y, Wlist, blist, fname_list):\n",
    "    predictions = predict(a, Wlist, blist, fname_list)\n",
    "    acc = np.mean(predictions == y)\n",
    "    acc = np.asscalar(acc)\n",
    "    return acc\n",
    "\n",
    "x, y = load_train_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'train accuracy: {test_model(x, y, Wlist, blist, fname_list) * 100:.2f}%')\n",
    "\n",
    "x, y = load_test_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'test accuracy: {test_model(x, y, Wlist, blist, fname_list) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "### 1. Why has the test accuracy not improved with this 2-layer network? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: We see that the train accuracy has reached 100% while the test accuracy remains below 80%. Clearly, this is the case of overfitting. I feel the reason for this can be either presence of noise or lack of representative samples."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does replacement of ReLU by Sigmoid at the hidden layer affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.69115135\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.64376420\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.64151756\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.63869933\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.63515441\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.63094570\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.62611108\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.62067883\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.61466883\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.60808107\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.60088547\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.59302904\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.58446466\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.57513294\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1400:\t0.56490314\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.55359772\n",
      "train accuracy: 65.07%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.54119290\n",
      "train accuracy: 65.07%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.52792026\n",
      "train accuracy: 67.46%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.51404625\n",
      "train accuracy: 69.38%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.49975307\n",
      "train accuracy: 73.21%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for train(lr = 0.001, iters = 5000, n_layers = 2, n_x = [64*64*3, 32, 1], fun_name_list = ['ReLU', 'Sigmoid'])\n",
    "n_layers = 2\n",
    "n_x = [64*64*3, 32, 1]\n",
    "fun_name_list = ['Sigmoid', 'Sigmoid']\n",
    "Wlist, blist, losses = train(lr = 0.01, iters = 2000, n_layers = n_layers, n_x = n_x, fun_name_list = fun_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 75.12%\n",
      "test accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "x, y = load_train_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'train accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')\n",
    "\n",
    "x, y = load_test_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'test accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the decrease in the loss has slowed down if we change ReLU to sigmoid. This is because of the fact that with increase in the absolute value of $z$,  $sigmoid(z)$ becomes increasingly small leading to smaller updates in weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Expand the 2 layer network to, say a 4 layer network of your choice. How does this model compare to logistic regresion and 2-layer network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.69314715\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.67409575\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.66251993\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.65545215\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.65111178\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.64843110\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.64676693\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.64572917\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.64507961\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.64467176\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.64441501\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.64425305\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.64415071\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.64408596\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1400:\t0.64404494\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.64401893\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.64400243\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.64399196\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.64398531\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.64398108\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2000:\t0.64397840\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2100:\t0.64397669\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2200:\t0.64397560\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2300:\t0.64397491\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2400:\t0.64397447\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2500:\t0.64397420\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2600:\t0.64397402\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2700:\t0.64397390\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2800:\t0.64397383\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 2900:\t0.64397379\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3000:\t0.64397376\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3100:\t0.64397374\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3200:\t0.64397373\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3300:\t0.64397372\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3400:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3500:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3600:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3700:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3800:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 3900:\t0.64397371\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4000:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4100:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4200:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4300:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4400:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4500:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4600:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4700:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4800:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 4900:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5000:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5100:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5200:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5300:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5400:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5500:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5600:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5700:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5800:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 5900:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6000:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6100:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6200:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6300:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6400:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6500:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6600:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6700:\t0.64397370\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6800:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 6900:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7000:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7100:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7200:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7300:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7400:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7500:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7600:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7700:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7800:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 7900:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8000:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8100:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8200:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8300:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8400:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8500:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8600:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8700:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8800:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 8900:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9000:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9100:\t0.64397369\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9200:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9300:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9400:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9500:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9600:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9700:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9800:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 9900:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10000:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10100:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10200:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10300:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10400:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10500:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10600:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10700:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10800:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 10900:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11000:\t0.64397368\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11100:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11200:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11300:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11400:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11500:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11600:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11700:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11800:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 11900:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12000:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12100:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12200:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12300:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12400:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12500:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12600:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12700:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12800:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 12900:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13000:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13100:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13200:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13300:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13400:\t0.64397367\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13500:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13600:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 13700:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13800:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 13900:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14000:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14100:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14200:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14300:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14400:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14500:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14600:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14700:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14800:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 14900:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15000:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15100:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15200:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15300:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15400:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15500:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15600:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15700:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15800:\t0.64397366\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 15900:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16000:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16100:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16200:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16300:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16400:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16500:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16600:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16700:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16800:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 16900:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17000:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17100:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17200:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17300:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17400:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17500:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17600:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17700:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17800:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 17900:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18000:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18100:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18200:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18300:\t0.64397365\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18400:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18500:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18600:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18700:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18800:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 18900:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19000:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19100:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19200:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19300:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19400:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19500:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19600:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19700:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19800:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 19900:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20000:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20100:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20200:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20300:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20400:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20500:\t0.64397364\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20600:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20700:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20800:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 20900:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21000:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21100:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21200:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21300:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21400:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21500:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21600:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21700:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21800:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 21900:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22000:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22100:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22200:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22300:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22400:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22500:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22600:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22700:\t0.64397363\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22800:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 22900:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23000:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23100:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23200:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23300:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23400:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23500:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23600:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23700:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23800:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 23900:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24000:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24100:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24200:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24300:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24400:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24500:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24600:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24700:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24800:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 24900:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25000:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25100:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25200:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25300:\t0.64397362\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25400:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25500:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25600:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25700:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25800:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 25900:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26000:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26100:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26200:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26300:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26400:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26500:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26600:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26700:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26800:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 26900:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27000:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27100:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 27200:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27300:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27400:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27500:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27600:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27700:\t0.64397361\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27800:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 27900:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28000:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28100:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28200:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28300:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28400:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28500:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28600:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28700:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28800:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 28900:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29000:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29100:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29200:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29300:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29400:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29500:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29600:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29700:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29800:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 29900:\t0.64397360\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for train(lr = 0.001, iters = 5000, n_layers = 2, n_x = [64*64*3, 32, 1], fun_name_list = ['ReLU', 'Sigmoid'])\n",
    "n_layers = 4\n",
    "np.random.seed(2)\n",
    "n_x = [64*64*3, 32, 8, 2, 1]\n",
    "fun_name_list = ['ReLU', 'ReLU', 'ReLU', 'Sigmoid']\n",
    "Wlist, blist, losses = train(lr = 0.01, iters = 30000, n_layers = n_layers, n_x = n_x, fun_name_list = fun_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 64.32%\n",
      "test accuracy: 49.75%\n"
     ]
    }
   ],
   "source": [
    "x, y = load_train_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'train accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')\n",
    "\n",
    "x, y = load_test_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'test accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that the train and test accuracies have gone down. \n",
    "#### This model is a much more complex model compared to both the single neuron as well as the 2-layer models and it has failed to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Play with a few learning rates and explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98aae51015b64e42a81685c58b576de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.69190731\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.64561201\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.63608484\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.61991669\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.59503117\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.56490897\n",
      "train accuracy: 66.51%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.53039603\n",
      "train accuracy: 71.29%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.49180244\n",
      "train accuracy: 76.56%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.45060302\n",
      "train accuracy: 80.38%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.40886277\n",
      "train accuracy: 86.60%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.36726491\n",
      "train accuracy: 88.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.32733606\n",
      "train accuracy: 90.43%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.28982319\n",
      "train accuracy: 93.30%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.25534617\n",
      "train accuracy: 94.26%\n",
      "\n",
      "\n",
      "Loss at iteration 1400:\t0.22429118\n",
      "train accuracy: 95.22%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.19853416\n",
      "train accuracy: 95.69%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.17116488\n",
      "train accuracy: 97.13%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.15282058\n",
      "train accuracy: 97.61%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.13582082\n",
      "train accuracy: 97.61%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.11666848\n",
      "train accuracy: 98.09%\n",
      "\n",
      "\n",
      "Loss at iteration 2000:\t0.10229960\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2100:\t0.09226882\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2200:\t0.08167306\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2300:\t0.07363584\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2400:\t0.06631302\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2500:\t0.05981141\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2600:\t0.05458313\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2700:\t0.04994722\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 2800:\t0.04586481\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2900:\t0.04217136\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3000:\t0.03900580\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3100:\t0.03621018\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3200:\t0.03372675\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3300:\t0.03145390\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3400:\t0.02943953\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3500:\t0.02761032\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3600:\t0.02596763\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3700:\t0.02449277\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3800:\t0.02312934\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3900:\t0.02189769\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4000:\t0.02075208\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4100:\t0.01971250\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4200:\t0.01875523\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4300:\t0.01787443\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4400:\t0.01706714\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4500:\t0.01630853\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4600:\t0.01560306\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4700:\t0.01495336\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4800:\t0.01434724\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4900:\t0.01378347\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5000:\t0.01325564\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5100:\t0.01275524\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5200:\t0.01229163\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5300:\t0.01185616\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5400:\t0.01144492\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5500:\t0.01106040\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5600:\t0.01069195\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5700:\t0.01034565\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5800:\t0.01002254\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5900:\t0.00971413\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For lr =  0.005\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 72.00%\n",
      "\n",
      "\n",
      "\n",
      "Loss at iteration 0:\t0.69190731\n",
      "train accuracy: 62.68%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.63617000\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.59532286\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.53103919\n",
      "train accuracy: 70.33%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.45542965\n",
      "train accuracy: 77.51%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.43220768\n",
      "train accuracy: 72.73%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.39669225\n",
      "train accuracy: 76.08%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.32516763\n",
      "train accuracy: 86.12%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.32567005\n",
      "train accuracy: 83.73%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.27127901\n",
      "train accuracy: 88.04%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.26536937\n",
      "train accuracy: 84.69%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.12936465\n",
      "train accuracy: 97.13%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.11378794\n",
      "train accuracy: 97.61%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.08566911\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1400:\t0.06707633\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.05164780\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.04367604\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.03696751\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.03209510\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.02806403\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2000:\t0.02476531\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2100:\t0.02208572\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2200:\t0.01987988\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2300:\t0.01797075\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2400:\t0.01635758\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2500:\t0.01498264\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2600:\t0.01378971\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2700:\t0.01274225\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2800:\t0.01181707\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2900:\t0.01101694\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3000:\t0.01029209\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3100:\t0.00965058\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3200:\t0.00907657\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3300:\t0.00855628\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3400:\t0.00808975\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3500:\t0.00766040\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3600:\t0.00727059\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3700:\t0.00691649\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3800:\t0.00659236\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3900:\t0.00628972\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4000:\t0.00601478\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4100:\t0.00575715\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4200:\t0.00551969\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4300:\t0.00529857\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4400:\t0.00509284\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4500:\t0.00490287\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4600:\t0.00472273\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4700:\t0.00455468\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4800:\t0.00439744\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4900:\t0.00424913\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5000:\t0.00411009\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5100:\t0.00397888\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5200:\t0.00385408\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5300:\t0.00373732\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5400:\t0.00362561\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5500:\t0.00352026\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5600:\t0.00342014\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5700:\t0.00332501\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5800:\t0.00323482\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5900:\t0.00314887\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For lr =  0.01\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 68.00%\n",
      "\n",
      "\n",
      "\n",
      "Loss at iteration 0:\t0.69190731\n",
      "train accuracy: 64.11%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.62027941\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.53271346\n",
      "train accuracy: 74.16%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.50108400\n",
      "train accuracy: 79.90%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.43695953\n",
      "train accuracy: 81.34%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.37155965\n",
      "train accuracy: 83.73%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.31711996\n",
      "train accuracy: 88.04%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.24280990\n",
      "train accuracy: 93.30%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.15094937\n",
      "train accuracy: 96.65%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.11257609\n",
      "train accuracy: 98.56%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.08054688\n",
      "train accuracy: 98.09%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.05902998\n",
      "train accuracy: 99.04%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.04383528\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.03493610\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 1400:\t0.02831880\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.02367308\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.02014927\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.01737470\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.01520032\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.01344417\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2000:\t0.01199731\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2100:\t0.01080615\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2200:\t0.00979381\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2300:\t0.00893974\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2400:\t0.00820463\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2500:\t0.00757072\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2600:\t0.00701505\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2700:\t0.00652606\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2800:\t0.00609553\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2900:\t0.00571451\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3000:\t0.00537051\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3100:\t0.00506259\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3200:\t0.00478375\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3300:\t0.00453236\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3400:\t0.00430145\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3500:\t0.00409361\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3600:\t0.00390050\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3700:\t0.00372301\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3800:\t0.00356061\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3900:\t0.00341027\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4000:\t0.00327027\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4100:\t0.00314136\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4200:\t0.00302034\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4300:\t0.00290748\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4400:\t0.00280257\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4500:\t0.00270402\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4600:\t0.00261171\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4700:\t0.00252484\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4800:\t0.00244297\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4900:\t0.00236529\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5000:\t0.00229269\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5100:\t0.00222385\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5200:\t0.00215860\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5300:\t0.00209640\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5400:\t0.00203794\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5500:\t0.00198204\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5600:\t0.00192907\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5700:\t0.00187830\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5800:\t0.00183017\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5900:\t0.00178457\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For lr =  0.015\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 72.00%\n",
      "\n",
      "\n",
      "\n",
      "Loss at iteration 0:\t0.69190731\n",
      "train accuracy: 64.59%\n",
      "\n",
      "\n",
      "Loss at iteration 100:\t0.59586247\n",
      "train accuracy: 65.55%\n",
      "\n",
      "\n",
      "Loss at iteration 200:\t0.52856486\n",
      "train accuracy: 68.42%\n",
      "\n",
      "\n",
      "Loss at iteration 300:\t0.45994380\n",
      "train accuracy: 72.25%\n",
      "\n",
      "\n",
      "Loss at iteration 400:\t0.40108015\n",
      "train accuracy: 78.47%\n",
      "\n",
      "\n",
      "Loss at iteration 500:\t0.32808388\n",
      "train accuracy: 83.25%\n",
      "\n",
      "\n",
      "Loss at iteration 600:\t0.35727003\n",
      "train accuracy: 78.47%\n",
      "\n",
      "\n",
      "Loss at iteration 700:\t0.17134116\n",
      "train accuracy: 94.74%\n",
      "\n",
      "\n",
      "Loss at iteration 800:\t0.10826061\n",
      "train accuracy: 98.56%\n",
      "\n",
      "\n",
      "Loss at iteration 900:\t0.06630002\n",
      "train accuracy: 99.04%\n",
      "\n",
      "\n",
      "Loss at iteration 1000:\t0.04219191\n",
      "train accuracy: 99.52%\n",
      "\n",
      "\n",
      "Loss at iteration 1100:\t0.03059738\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1200:\t0.02381563\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1300:\t0.01923350\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1400:\t0.01588643\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1500:\t0.01346827\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1600:\t0.01159816\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1700:\t0.01012925\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1800:\t0.00896119\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 1900:\t0.00799684\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2000:\t0.00721074\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2100:\t0.00654507\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2200:\t0.00597882\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2300:\t0.00549903\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2400:\t0.00507839\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2500:\t0.00471122\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2600:\t0.00438960\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2700:\t0.00410537\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2800:\t0.00385269\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 2900:\t0.00362528\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3000:\t0.00342194\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3100:\t0.00323757\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3200:\t0.00306900\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3300:\t0.00291729\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3400:\t0.00277935\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3500:\t0.00265077\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3600:\t0.00253338\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3700:\t0.00242541\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3800:\t0.00232436\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 3900:\t0.00223151\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4000:\t0.00214526\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4100:\t0.00206514\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4200:\t0.00198938\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4300:\t0.00191964\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4400:\t0.00185296\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4500:\t0.00179190\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4600:\t0.00173288\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4700:\t0.00167773\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4800:\t0.00162601\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 4900:\t0.00157709\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5000:\t0.00153086\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5100:\t0.00148696\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5200:\t0.00144525\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5300:\t0.00140569\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5400:\t0.00136777\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5500:\t0.00133218\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5600:\t0.00129806\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5700:\t0.00126535\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5800:\t0.00123419\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "Loss at iteration 5900:\t0.00120451\n",
      "train accuracy: 100.00%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For lr =  0.02\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 74.00%\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "iters = 6000\n",
    "n_x = [64*64*3, 32, 1]\n",
    "fun_name_list = ['ReLU', 'Sigmoid']\n",
    "lrs = [0.01 + i*0.005 for i in range(-1, 3, 1)]\n",
    "losses_for_lrs = [None] * len(lrs)\n",
    "for i in tqdm(range(len(lrs))):\n",
    "    Wlist, blist, losses_for_lrs[i] = train(lr = lrs[i], iters = iters, n_layers = 2, n_x = n_x, fun_name_list = fun_name_list)\n",
    "\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(\"\\n\\nFor lr = \", lrs[i])\n",
    "    print(f'train accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(x, y, Wlist, blist, fun_name_list) * 100:.2f}%')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(losses_for_lrs[0]))\n",
    "x_axis = [i for i in range(1, iters+1)]\n",
    "plt.plot(x_axis, losses_for_lrs[0], 'b', label = 'lr = ' + str(lrs[0]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(x_axis, losses_for_lrs[1], 'r', label = 'lr = ' + str(lrs[1]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(x_axis, losses_for_lrs[2], 'g', label = 'lr = ' + str(lrs[2]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(x_axis, losses_for_lrs[3], 'k', label = 'lr = ' + str(lrs[3]))\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that among the chosen learning rates, lr = 0.005 leads to least variations in the loss over the iterations. But, we also see that lr = 0.02 preforms best on the test data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: All questions will be answered in the jupyter notebook only. Wherever code is required, you write and run the code in a code cell. For text, write and render in a markdown cell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
