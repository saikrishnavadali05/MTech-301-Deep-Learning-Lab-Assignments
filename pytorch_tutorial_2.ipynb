{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduFJAHieKhC",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Tutorial - Part 2\n",
        "Having gone through part-1, you must be now familiar with creation of Tensor objects and operating on them. Just to remind you that indexing Tensor objects are similar to numpy indexing. Further, you must go through the tensor broadcasting rules (similar to broadcasting rules for numpy arrays) from https://pytorch.org/docs/stable/notes/broadcasting.html. Below is given a set of exercises on indexing and broadcasting semantics for you to attempt before you get into this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BswdoaVRf0ZT",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "1. Create a 2-d Tensor of size 8x3. Extract all the even numbered rows by slicing and print it.\n",
        "2. Create a 2-d Tensor of size 8x3. Extract all the even numbered rows in reverse order by slicing and print it.\n",
        "3. Create a 4-d tensor named t of size 2x3x4x2. What does t[0, ..., 1] extract?\n",
        "4. For the tensor in part 1, set the 2nd row (row index is 1) to the Tensor with data [2, 3, 4] with just one line of code.\n",
        "5. For the Tensor in part3, set the 4th element to -22.7. (only one line of code)\n",
        "6. For the Tensor in part 3, extract the 1st and 3rd rows of the middle 3x4 array across 1st and 4th dimensions (both of which have size 2). Do this with one line of code.\n",
        "7. Tensor with scalar data can be added to a Tensor with size 2. (True/False)\n",
        "8. Tensor of size 2x3 can be added to another Tensor of size 2 with both having same datatype. (True or False)\n",
        "9. Tensor of size 2x3 can be added to another Tensor of size 3 with both having same datatype. (True or False)\n",
        "10. Tensor of size 2x1x3x1 can be elementwise multiplied with another Tensor of size 1x2 with both Tensors having same datatype. (True of False)\n",
        "11. What will be the size of the output of elementwise multiplying a Tensor of size 2x1x3x1 with   another Tensor of size 1x2 assuming both Tensors have same datatype?\n",
        "12. For the Tensor defined in part1, extract 1st row 2nd element, 5th row 1st element and 8th row 1st element with one line of code.\n",
        "13. For the Tensor defined in part1, extract all the corner elements i.e elements at positions (0, 0), (0, 2), (7, 0) and (7, 2).(only one line of code)\n",
        "14. For the tensor defined in part 1, extract all the rows whose mean is >= 2. (only one line of code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLq2ybXXPtzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KOWobzBQXtc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1739fb4-97b1-4238-94d8-82624a01b8cb"
      },
      "source": [
        "# 1. Create a 2-d Tensor of size 8x3. Extract all the even numbered rows by slicing and print it.\n",
        "t1 = torch.randn((8,3))\n",
        "t1_same = t1\n",
        "print(\"Ans1:\\nt1: \", t1)\n",
        "# indices1 = torch.LongTensor([i for i in range(t1.shape[0]) if not (i%2)])\n",
        "# res1 = t1.index_select(dim=0, index=indices1)\n",
        "print(\"res1: \", t1[::2,:])\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 2. Create a 2-d Tensor of size 8x3. Extract all the even numbered rows in reverse order by slicing and print it.\n",
        "t2 = torch.randint(low=0, high=100, size=[8,3])\n",
        "print(\"Ans2:\\nt2: \", t2)\n",
        "# indices2 = torch.LongTensor([i for i in range(t2.shape[0]-1, -1, -1) if not (i%2)])\n",
        "# res2 = t2.index_select(dim=0, index=indices2)\n",
        "print(\"res2: Negative step is not yet supported.\", )\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 3. Create a 4-d tensor named t of size 2x3x4x2. What does t[0, ..., 1] extract?\n",
        "t3 = torch.randn((2,3,4,2))\n",
        "t3_same = t3\n",
        "print(\"Ans3:\\nt3[0, ..., 1].shape: \", t3[0, ..., 1].shape, \"\\ntorch.equal(t3[0, ..., 1],t3[0,:,:,1]) = \", torch.equal(t3[0, ..., 1],t3[0,:,:,1]))\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 4. For the tensor in part 1, set the 2nd row (row index is 1) to the Tensor with data [2, 3, 4] with just one line of code.\n",
        "t1[1] = torch.Tensor([2, 3, 4])\n",
        "print(\"Ans4:\\nAfter setting: t1[1] = torch.Tensor([2, 3, 4])\\nt1: \", t1)\n",
        "print(\"t1[1]: \", t1[1])\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 5. For the Tensor in part3, set the 4th element to -22.7. (only one line of code)\n",
        "t3[0,0,1,1] = torch.Tensor([-22.7])\n",
        "print(\"Ans5:\\nAfter setting: t3[0,0,1,1] = torch.Tensor(-22.7):\\nt3: \", t3)\n",
        "print(\"t3[0,0,1,1]: \", t3[0,0,1,1]) \n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 6. For the Tensor in part 3, extract the 1st and 3rd rows of the middle 3x4 array across 1st and 4th dimensions (both of which have size 2).\n",
        "#    Do this with one line of code.\n",
        "t3 = t3_same\n",
        "print(\"Ans6:\\nres6 = t3[:,::2,:,:] = \", t3[:,::2,:,:])\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 7. Tensor with scalar data can be added to a Tensor with size 2. (True/False)\n",
        "t7_1 = torch.tensor([2], dtype=torch.float).cuda()\n",
        "t7_2 = torch.tensor([2,2], dtype=torch.float).cuda()\n",
        "print(\"Ans7:\\nt7_1 + t7_2 = \", t7_1+t7_2,\"\\nAns: True\")\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 8. Tensor of size 2x3 can be added to another Tensor of size 2 with both having same datatype. (True or False)\n",
        "t8_1 = torch.randn((2,3),dtype=torch.float).cuda()\n",
        "t8_2 = torch.tensor([1,1], dtype=torch.float)\n",
        "print(\"Ans8: False\")\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 9. Tensor of size 2x3 can be added to another Tensor of size 3 with both having same datatype. (True or False)\n",
        "t9_1 = torch.randn((2,3),dtype=torch.float).cuda()\n",
        "t9_2 = torch.tensor([1,1,1], dtype=torch.float).cuda()\n",
        "print(\"Ans9: True\")\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 10. Tensor of size 2x1x3x1 can be elementwise multiplied with another Tensor of size 1x2 with both Tensors having same datatype. (True of False)\n",
        "t10_1 = torch.randn((2,1,3,1), dtype=torch.float).cuda()\n",
        "t10_2 = torch.randn((1,2), dtype=torch.float).cuda()\n",
        "print(\"Ans10: .True.\")\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 11. What will be the size of the output of elementwise multiplying a Tensor of size 2x1x3x1\n",
        "#     with another Tensor of size 1x2 assuming both Tensors have same datatype?\n",
        "print(\"Ans11: Resultant size will be 2x1x3x2.\")\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 12. For the Tensor defined in part1, extract 1st row 2nd element, 5th row 1st element and 8th row 1st element with one line of code.\n",
        "t12 = t1_same\n",
        "# Also, res12 = t12[(0,4,7),(1,0,0)]\n",
        "print(\"Ans12:\\nres12 = torch.gather(t12.index_select(dim=0, index=torch.LongTensor([0,4,7])),dim=1,index=torch.LongTensor([[1],[0],[0]]))\")\n",
        "print(\"So, res12 = \", torch.gather(t12.index_select(dim=0, index=torch.LongTensor([0,4,7])),dim=1,index=torch.LongTensor([[1],[0],[0]])))\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 13. For the Tensor defined in part1, extract all the corner elements i.e elements\n",
        "# at positions (0, 0), (0, 2), (7, 0) and (7, 2).(only one line of code).\n",
        "t13 = t1_same\n",
        "print(t13)\n",
        "print(\"Ans13:\\nres13 = t13[::t13.shape[0]-1,::t13.shape[1]-1]\")\n",
        "# also, res13 = t13[(0,0,7,7),(0,2,0,2)]\n",
        "print(\"So, res13 = \", t13[::t13.shape[0]-1,::t13.shape[1]-1])\n",
        "print(\"................................................................................\\n\")\n",
        "\n",
        "# 14. For the tensor defined in part 1, extract all the rows whose mean is >= 2. (only one line of code)\n",
        "t14 = t1_same\n",
        "print(t14)\n",
        "# also, res14 = [t14[i] for i in range(t14.shape[0]) if t14[i].mean() >= 2]\n",
        "res14 = t14.index_select(dim=0,index = torch.LongTensor([i for i in range(t14.shape[0]) if t14[i,:].mean() >= 2]))\n",
        "print(\"Ans14:\\nres14 = \", res14)\n",
        "print(\"................................................................................\\n\")\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ans1:\n",
            "t1:  tensor([[ 0.4849, -0.6040, -1.1029],\n",
            "        [-1.7819,  1.0709, -0.6504],\n",
            "        [ 0.5057,  0.9325, -1.1939],\n",
            "        [-0.5370,  1.0823, -1.5110],\n",
            "        [ 1.6736, -1.1163,  0.6021],\n",
            "        [-1.1221,  1.2851, -1.8526],\n",
            "        [ 0.2900,  1.3086, -0.4059],\n",
            "        [-0.0078,  0.6413, -1.4778]])\n",
            "res1:  tensor([[ 0.4849, -0.6040, -1.1029],\n",
            "        [ 0.5057,  0.9325, -1.1939],\n",
            "        [ 1.6736, -1.1163,  0.6021],\n",
            "        [ 0.2900,  1.3086, -0.4059]])\n",
            "................................................................................\n",
            "\n",
            "Ans2:\n",
            "t2:  tensor([[97, 80, 23],\n",
            "        [19, 76, 62],\n",
            "        [25, 21,  7],\n",
            "        [32, 78, 74],\n",
            "        [68, 69, 52],\n",
            "        [51, 40, 14],\n",
            "        [33, 36, 16],\n",
            "        [63, 74, 77]])\n",
            "res2: Negative step is not yet supported.\n",
            "................................................................................\n",
            "\n",
            "Ans3:\n",
            "t3[0, ..., 1].shape:  torch.Size([3, 4]) \n",
            "torch.equal(t3[0, ..., 1],t3[0,:,:,1]) =  True\n",
            "................................................................................\n",
            "\n",
            "Ans4:\n",
            "After setting: t1[1] = torch.Tensor([2, 3, 4])\n",
            "t1:  tensor([[ 0.4849, -0.6040, -1.1029],\n",
            "        [ 2.0000,  3.0000,  4.0000],\n",
            "        [ 0.5057,  0.9325, -1.1939],\n",
            "        [-0.5370,  1.0823, -1.5110],\n",
            "        [ 1.6736, -1.1163,  0.6021],\n",
            "        [-1.1221,  1.2851, -1.8526],\n",
            "        [ 0.2900,  1.3086, -0.4059],\n",
            "        [-0.0078,  0.6413, -1.4778]])\n",
            "t1[1]:  tensor([2., 3., 4.])\n",
            "................................................................................\n",
            "\n",
            "Ans5:\n",
            "After setting: t3[0,0,1,1] = torch.Tensor(-22.7):\n",
            "t3:  tensor([[[[-2.7029e-01, -2.0081e+00],\n",
            "          [ 1.8619e+00, -2.2700e+01],\n",
            "          [ 2.6239e-01, -1.4883e+00],\n",
            "          [-4.5106e-01, -3.7506e-01]],\n",
            "\n",
            "         [[-5.0454e-01,  2.9407e-02],\n",
            "          [-6.8954e-01,  8.9879e-01],\n",
            "          [-3.3196e-01,  1.1532e+00],\n",
            "          [-4.9661e-01,  5.3762e-01]],\n",
            "\n",
            "         [[-4.2993e-02, -1.6360e+00],\n",
            "          [-1.5038e-01,  8.6768e-01],\n",
            "          [ 1.2716e-03,  2.3960e-01],\n",
            "          [ 2.0848e+00, -3.6800e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8148e-01,  8.0000e-01],\n",
            "          [-1.3558e+00,  1.0162e-01],\n",
            "          [ 3.1001e-01,  8.9543e-03],\n",
            "          [-1.5959e+00,  4.0478e-01]],\n",
            "\n",
            "         [[-1.1722e-01, -3.0291e-01],\n",
            "          [-4.1576e-01,  4.2163e-01],\n",
            "          [-1.7091e+00,  2.5840e+00],\n",
            "          [ 1.0279e+00,  1.9769e-01]],\n",
            "\n",
            "         [[ 5.5787e-01,  9.4713e-01],\n",
            "          [-5.9829e-01, -4.0683e-01],\n",
            "          [ 5.9618e-01, -1.5844e+00],\n",
            "          [ 1.0797e+00,  1.9328e-01]]]])\n",
            "t3[0,0,1,1]:  tensor(-22.7000)\n",
            "................................................................................\n",
            "\n",
            "Ans6:\n",
            "res6 = t3[:,::2,:,:] =  tensor([[[[-2.7029e-01, -2.0081e+00],\n",
            "          [ 1.8619e+00, -2.2700e+01],\n",
            "          [ 2.6239e-01, -1.4883e+00],\n",
            "          [-4.5106e-01, -3.7506e-01]],\n",
            "\n",
            "         [[-4.2993e-02, -1.6360e+00],\n",
            "          [-1.5038e-01,  8.6768e-01],\n",
            "          [ 1.2716e-03,  2.3960e-01],\n",
            "          [ 2.0848e+00, -3.6800e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8148e-01,  8.0000e-01],\n",
            "          [-1.3558e+00,  1.0162e-01],\n",
            "          [ 3.1001e-01,  8.9543e-03],\n",
            "          [-1.5959e+00,  4.0478e-01]],\n",
            "\n",
            "         [[ 5.5787e-01,  9.4713e-01],\n",
            "          [-5.9829e-01, -4.0683e-01],\n",
            "          [ 5.9618e-01, -1.5844e+00],\n",
            "          [ 1.0797e+00,  1.9328e-01]]]])\n",
            "................................................................................\n",
            "\n",
            "Ans7:\n",
            "t7_1 + t7_2 =  tensor([4., 4.], device='cuda:0') \n",
            "Ans: True\n",
            "................................................................................\n",
            "\n",
            "Ans8: False\n",
            "................................................................................\n",
            "\n",
            "Ans9: True\n",
            "................................................................................\n",
            "\n",
            "Ans10: .True.\n",
            "................................................................................\n",
            "\n",
            "Ans11: Resultant size will be 2x1x3x2.\n",
            "................................................................................\n",
            "\n",
            "Ans12:\n",
            "res12 = torch.gather(t12.index_select(dim=0, index=torch.LongTensor([0,4,7])),dim=1,index=torch.LongTensor([[1],[0],[0]]))\n",
            "So, res12 =  tensor([[-0.6040],\n",
            "        [ 1.6736],\n",
            "        [-0.0078]])\n",
            "................................................................................\n",
            "\n",
            "tensor([[ 0.4849, -0.6040, -1.1029],\n",
            "        [ 2.0000,  3.0000,  4.0000],\n",
            "        [ 0.5057,  0.9325, -1.1939],\n",
            "        [-0.5370,  1.0823, -1.5110],\n",
            "        [ 1.6736, -1.1163,  0.6021],\n",
            "        [-1.1221,  1.2851, -1.8526],\n",
            "        [ 0.2900,  1.3086, -0.4059],\n",
            "        [-0.0078,  0.6413, -1.4778]])\n",
            "Ans13:\n",
            "res13 = t13[::t13.shape[0]-1,::t13.shape[1]-1]\n",
            "So, res13 =  tensor([[ 0.4849, -1.1029],\n",
            "        [-0.0078, -1.4778]])\n",
            "................................................................................\n",
            "\n",
            "tensor([[ 0.4849, -0.6040, -1.1029],\n",
            "        [ 2.0000,  3.0000,  4.0000],\n",
            "        [ 0.5057,  0.9325, -1.1939],\n",
            "        [-0.5370,  1.0823, -1.5110],\n",
            "        [ 1.6736, -1.1163,  0.6021],\n",
            "        [-1.1221,  1.2851, -1.8526],\n",
            "        [ 0.2900,  1.3086, -0.4059],\n",
            "        [-0.0078,  0.6413, -1.4778]])\n",
            "Ans14:\n",
            "res14 =  tensor([[2., 3., 4.]])\n",
            "................................................................................\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDSvIygtZ-b-",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial we will focus on the requires_grad, grad and grad_fn attributes of a Tensor object. Further we will also look at 'backward' method. we will also look at torch.autograd package.\n",
        "\n",
        "Until now, all the Tensors we had created had requires_grad to be False by default. For example, see below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbnN7pCgas4s",
        "colab_type": "code",
        "outputId": "dd3d5bd4-90a3-4f21-f212-65ba8f09dd89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "t = torch.tensor([[1, 2], [3, 4]])\n",
        "t.requires_grad"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMKCYHkCbf18",
        "colab_type": "text"
      },
      "source": [
        "So, what is this requires_grad? Why is it required?\n",
        "<br>\n",
        "<br>\n",
        "In Pytorch, each Tensor has an associated directed acyclic graph (DAG) called computation graph that records the computation of that Tensor. For example, if we have a Tensor w that is sum of two other Tensors x and y, then the associated computation graph with w has leaves as the expression x and expression y and root as the expression x+y that computed w. The computation graph associated with the Tensor x is simply a leaf node with expression x and similarly the computation graph associated with Tensor y is again a leaf node with expression y. If we have a Tensor u which equals $z*w$, then the computation graph associated with u has leaves as expression x, expression y and expression z, internal node as the expression x+y that computed w and the root as the expression   $z*w$ that computed u. It is important to note that each Tenor has its associated computation graph. An expression like x+y involved in computing w is also part of computing u=$z*w$ and so, it is part of computation graphs associated with both w and u.\n",
        "<br>\n",
        "<br>\n",
        "Now, if the requires_grad attribute of a Tensor t is True, then 'backward' method defined in the Tensor class can be called on t to compute the gradients of t w.r.to Tensors involved in computing it. These gradients are available in the grad attribute of the Tensors.The 'backward' method automatically computes the gradients by evaluating the graph backwards using chain rule. This is how the AutoGrad (automatic gradients) works in Pytorch. Of course, gradient of t w.r.to a Tensor involved in computing it but whose requires_grad is False will be None. The general rule of propagation of requires_grad attribute is as follows: even if a single operand in an operation has its requires_grad to be True, the output also will have its requires_grad to be True. In other words, only when all the operands in an operation does not require grad (i.e their requires_grad equals False), does the ouput also not require gradient (its requires_grad will be False). \n",
        "<br>\n",
        "<br>\n",
        "We know that such gradient computations are required in deep learning implementations. That's why PyTorch introduced requires_grad attribute to a Tensor object. If that attribute is set to True for a Tensor, gradients of the Tensor w.r.to Tensors involved in computing it can be automatically obtained using the 'backward' method.\n",
        "<br>\n",
        "<br>\n",
        "Let us see some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMsUWXmAbSD9",
        "colab_type": "code",
        "outputId": "90d18280-bf3e-49c1-dcf6-1f9a41c8eab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1.)\n",
        "w = x+y\n",
        "print(f'requires_grad of x, y, w: {x.requires_grad}\\t{y.requires_grad}\\t{w.requires_grad}')\n",
        "z = torch.tensor(-1., requires_grad = True)\n",
        "u = z*w\n",
        "print(f'requires_grad of z, w, u: {z.requires_grad}\\t{w.requires_grad}\\t{u.requires_grad}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requires_grad of x, y, w: True\tFalse\tTrue\n",
            "requires_grad of z, w, u: True\tTrue\tTrue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da3x7eaCjVht",
        "colab_type": "text"
      },
      "source": [
        "Suppose we want to compute the gradients (derivatives here) of u w.r.to z, w, x and y respectively. Let us denote by $dp$ the entity $\\frac{d}{dp}(u)$. We should expect the following results:\n",
        "<br>\n",
        "$dz$ = $w$ = 3\n",
        "<br>\n",
        "$dw$ = $z$ = -1\n",
        "<br>\n",
        "$dy$ = $dw*\\frac{dw}{dy}$ = $-1*1$ = -1\n",
        "<br>\n",
        "$dx$ = $dw*\\frac{dw}{dx}$ = $-1*1$ = -1\n",
        "\n",
        "Let us check if we get these results by calling 'backward' on the Tensor u.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4C6i7R9i4IP",
        "colab_type": "code",
        "outputId": "9f3a0356-0dd8-4728-8a64-a942f2475872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "u.backward()\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz: 3.0\n",
            "dw: None\n",
            "dy: None\n",
            "dx: -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPe09xGGpe7N",
        "colab_type": "text"
      },
      "source": [
        "We got the right answer for $dz$ and $dx$ (note that the gradients are available with the grad attribute of the Tensors). But, what about $dw$ and $dy$? $dy$ is not available because its requires_grad is False. $dw$ is not available because, by default, gradients are accumulated only for the leaf Tensors in their grad attribute. Since $w$ is an internal node, its gradient is not accumulated. To accumulate the gradient of an internal node in the computation graph, we have to call 'retain_grad' method on the Tensor that enables its grad attribute. To set the requires_grad attribute of a Tensor, we can call 'requires_grad_' method.  See the revised example below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7wtsN56mdlz",
        "colab_type": "code",
        "outputId": "a55078ac-d283-4251-b47c-560801f27177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1.)\n",
        "y.requires_grad_(requires_grad = True)\n",
        "w = x+y\n",
        "w.retain_grad()\n",
        "print(f'requires_grad of x, y, w: {x.requires_grad}\\t{y.requires_grad}\\t{w.requires_grad}')\n",
        "z = torch.tensor(-1., requires_grad = True)\n",
        "u = z*w\n",
        "print(f'requires_grad of z, w, u: {z.requires_grad}\\t{w.requires_grad}\\t{u.requires_grad}')\n",
        "\n",
        "u.backward()\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requires_grad of x, y, w: True\tTrue\tTrue\n",
            "requires_grad of z, w, u: True\tTrue\tTrue\n",
            "dz: 3.0\n",
            "dw: -1.0\n",
            "dy: -1.0\n",
            "dx: -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIaN_Svt6s4",
        "colab_type": "text"
      },
      "source": [
        "Now, let us try to call backward on u once again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmiM6sFKrROk",
        "colab_type": "code",
        "outputId": "bbbf62b8-3f40-4d54-efae-25960ac3cc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "u.backward()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7c5391a3c07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJLtWLQeuEv-",
        "colab_type": "text"
      },
      "source": [
        "What happened??\n",
        "<br>\n",
        "<br>\n",
        "Backward propgagation is unable to proceed because the computation graph associated with the Tensor u has already been freed. In PyTorch, once the 'backward' method has been called on a Tensor, at the end of the call, its computation graph is freed. If we want to retain the graph to call backward once again, we should explicitly set the retain_graph parameter of 'backward' method to True. The reason Pytorch frees the graph after one 'backward' propagation is to allow the graph to be built dynamically during every forward propagation. This is a very powerful feature of PyTorch. But for situations where you want 'backward' to be done on the same graph, make sure retain_graph parameter in 'backward' call is set to True.\n",
        "<br>\n",
        "<br>\n",
        "Let us see an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH3FEFZ9sE7I",
        "colab_type": "code",
        "outputId": "80cedcb8-451a-48b8-ad21-7a72c15aa4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1., requires_grad = True)\n",
        "w = x+y\n",
        "w.retain_grad()\n",
        "print(f'requires_grad of x, y, w: {x.requires_grad}\\t{y.requires_grad}\\t{w.requires_grad}')\n",
        "z = torch.tensor(-1., requires_grad = True)\n",
        "u = z*w\n",
        "print(f'requires_grad of z, w, u: {z.requires_grad}\\t{w.requires_grad}\\t{u.requires_grad}')\n",
        "\n",
        "u.backward(retain_graph = True) # backward call; graph is not freed after this call\n",
        "print('Grads after first call to backward')\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}\\n\\n')\n",
        "\n",
        "u.backward() # backward call again; graph will be freed after this call\n",
        "print('Grads after second call to backward')\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requires_grad of x, y, w: True\tTrue\tTrue\n",
            "requires_grad of z, w, u: True\tTrue\tTrue\n",
            "Grads after first call to backward\n",
            "dz: 3.0\n",
            "dw: -1.0\n",
            "dy: -1.0\n",
            "dx: -1.0\n",
            "\n",
            "\n",
            "Grads after second call to backward\n",
            "dz: 6.0\n",
            "dw: -2.0\n",
            "dy: -2.0\n",
            "dx: -2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDqPuF4yxa_C",
        "colab_type": "text"
      },
      "source": [
        "Observe the grads after second call to 'backward'. They have doubled up. In fact Pytorch accumulates gradient in to the grad attribute of the Tensor. In the above example, we know that gradient of u w.r.to z is 3.0. So, after the first call to 'backward', the grad attribute of z is 3.0.  After the second call to 'backward', the gradient of u w.r.to z will still be 3.0 but when it gets accumulated into the grad attribute of z that already has 3.0, it becomes 6.0. \n",
        "<br>\n",
        "<br>\n",
        "One more 'backward' call on u will give error since during the second call we did not set the parameter retain_graph to True. So the graph will be freed after the second call. However, do not have the confusion that when graph is freed, all the associated Tensors are freed. Graph is not made of Tensors but built  using only the set of expressions that computed the Tensor. Tensors along with its attributes and methods are alive. To appreciate this point, let us call backward on the Tensor w. Remind yourself that each Tensor has its associated computation graph built using the set of expressions that computed it. And the computation graph associated with Tensor w has not yet been freed since we have not yet called backward on w."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqNS5OmUxKmt",
        "colab_type": "code",
        "outputId": "d2cbe7cb-1227-4088-a6d9-4d600027fe37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "w.backward() # backward call again; graph will be freed after this call\n",
        "print('Grads after call to backward on w')\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grads after call to backward on w\n",
            "dz: 6.0\n",
            "dw: -1.0\n",
            "dy: -1.0\n",
            "dx: -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyggimT41hLy",
        "colab_type": "text"
      },
      "source": [
        "Observe that $dz$ is still available (even though the graph associated with u has already been freed) and has not changed since z is not part of the computation graph of w. However, $dx$ and $dy$ have changed because w=x+y. Derivative of w w.r.to x is 1 and derivative of w w.r.to y is 1. Already, from the previous call, grad attributes of x and y have values -2.0 and -2.0 respectively. Since grad accumulates the gradients, from the current call, 1 is accumulated to -2.0 to give the latest grad attribute value for x and y as -1.0 and -1.0 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYwYdDxk3n1z",
        "colab_type": "text"
      },
      "source": [
        "Having got  familarity with requires_grad and grad attribute, now we will look at another attribute namely grad_fn associated with the PyTorch Tensor. Let us look at the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3OqPuhv1YDl",
        "colab_type": "code",
        "outputId": "2dcacf6c-7644-4d3b-bb18-b0c74e52706c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1., requires_grad = True)\n",
        "w = x+y\n",
        "print(f'Gradient functions associated with x and y: {x.grad_fn}\\t{y.grad_fn}')\n",
        "print(f'Gradient function associated with w: {w.grad_fn}')\n",
        "print(f'Type of the gradient function associated with w: {type(w.grad_fn)}')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient functions associated with x and y: None\tNone\n",
            "Gradient function associated with w: <AddBackward0 object at 0x7f07a1ca6d30>\n",
            "Type of the gradient function associated with w: <class 'AddBackward0'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KLfu_j2GV-Z",
        "colab_type": "text"
      },
      "source": [
        "As you can see, grad_fn associated with Tensor w is an object of type 'AddBackward0'. This is because w is obtained by adding x and y. So the 'Function' type (actually expression) required to compute w is addition. Formally, PyTorch builds the computation graph for w using these 'Function' objects as nodes. The leaves are expressions x and y that have their grad_fn attributes to be None. Root is the expression x+y associated with AddBackward0 function object. Note that by chain rule, gradient at a node is the product of incoming gradient and local gradient. The 'Function' objects stored in the grad_fn attribute facilitate AutoGrad to compute local gradients. One more example is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjpMX-ft45B7",
        "colab_type": "code",
        "outputId": "1d4841da-626b-4514-eb05-33218b7e11ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1., requires_grad = True)\n",
        "w = x+y\n",
        "z = torch.tensor(-1., requires_grad = True)\n",
        "u = z*w\n",
        "print(f'Gradient functions associated with x and y: {x.grad_fn}\\t{y.grad_fn}')\n",
        "print(f'Gradient function associated with w and z: {w.grad_fn}\\t{z.grad_fn}')\n",
        "print(f'Gradient function associated with u: {u.grad_fn}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient functions associated with x and y: None\tNone\n",
            "Gradient function associated with w and z: <AddBackward0 object at 0x7f07a1425240>\tNone\n",
            "Gradient function associated with u: <MulBackward0 object at 0x7f07a14253c8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzBizL6HJ0BH",
        "colab_type": "text"
      },
      "source": [
        "Until now we have dealt with a simple example that had only scalar Tensors (i.e Pytorch Tensors whose data are scalars). Let us look at another example. Say, we want to compute $u$ = $\\frac{1}{4}\\sum_{i=0}^{i=3}z_i$ where $z$ = $x+y$, $x$ and $y$ are 2 d Tensors of same type and size 2x2. So, $u$ is a function from $R^4$ to $R$ and $z$ is a function from $R^4 X R^4$ to $R^4$. The gradients of $u$ w.r.to $z$, $y$ and $x$ are as follows:\n",
        "<br>\n",
        "<br>\n",
        "$dz$ =  local_gradient * incoming_gradient = $J_z(u)^T*\\frac{d}{du}(u)$ \n",
        "<br>\n",
        "&emsp;&nbsp; &nbsp; = $[\\frac{du}{dz_1} \\hspace{0.25cm}...\\hspace{0.25cm}\\frac{du}{dz_4}]^T$ \n",
        "<br>\n",
        "&emsp;&nbsp; &nbsp; =  $[0.25\\hspace{0.25cm} 0.25 \\hspace{0.25cm}0.25\\hspace{0.25cm}0.25]^T*1.0$ \n",
        "<br>\n",
        "&emsp;&nbsp; &nbsp; = $\\begin{bmatrix} 0.25 \\\\ 0.25 \\\\ 0.25 \\\\0.25 \\end{bmatrix}$ \n",
        "<br>\n",
        "<br>\n",
        "where $J_z(u)$ is the Jacobian of $u$ w.r.to $z$.\n",
        "\n",
        "The resultant $dz$ when accumulated in to the grad attribute of z is reshaped in to 2x2 since $z$ is 2x2.\n",
        "<br>\n",
        "<br>\n",
        "$dy$ = local_gradient * incoming_gradient = $J_y(z)^T*dz$ = $\\begin{bmatrix} \\frac{dz_1}{dy_1} & ... & \\frac{dz_1}{dy_4}\\\\ \\vdots & \\vdots & \\vdots \\\\ \\frac{dz_4}{dy_1} & ... & \\frac{dz_4}{dy_4} \\end{bmatrix}^T * dz$ \n",
        "<br>\n",
        "&emsp;&nbsp; &nbsp; =  $\\begin{bmatrix} 1.0 & 0 & 0 & 0 \\\\ 0 & 1.0 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & 1.0 \\end{bmatrix}^T*\\begin{bmatrix} 0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{bmatrix}$ \n",
        "<br>\n",
        "&emsp;&nbsp; &nbsp; = $\\begin{bmatrix} 0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25 \\end{bmatrix}$\n",
        "<br>\n",
        "<br>\n",
        "The resultant $dy$ when accumulated in to the grad attribute of y is reshaped in to 2x2 since $y$ is 2x2.\n",
        "\n",
        "Similarly, $dx$ can be computed.\n",
        "<br>\n",
        "<br>\n",
        "In fact 'backward' method does the above set of computations. If the Tensor on which 'backward' is called is a scalar valued (like $u$), then the initial incoming gradient is assumed to be 1. Then, at every node, 'backward' computes the local gradient as the transpose of the Jacobian of the Tensor associated with the node from which the gradient comes in w.r.to the Tensor associated with the current node and then multiplies this with the incoming gradient to give the gradient at the current node i.e for eg, at node associated with z, transpose of the Jacobian of $u$ w.r.to $z$, $J_z(u)^T$, is computed and then multiplied with the incoming gradient 1.0 to give the output $dz$. The resultant gradient is accumulated in to the grad attribute of the Tensor associated with the current node. Note that the Jacobians are not explicitly available. It is internal to the 'backward' method.\n",
        "<br>\n",
        "<br>\n",
        "Let us code this example and check."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SFVDYmzI3RI",
        "colab_type": "code",
        "outputId": "d4116b0a-e157-45f9-b982-424e407c787e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad = True) \n",
        "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad = True)\n",
        "z = x+y\n",
        "z.retain_grad()\n",
        "u = torch.mean(z)\n",
        "\n",
        "u.backward()\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz: tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "\n",
            "dy: tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "\n",
            "dx: tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k5l-HKzolSy",
        "colab_type": "text"
      },
      "source": [
        "Suppose in the above example, $u$ = $2*z$ instead of $u$ = $\\frac{1}{4}\\sum_{i=0}^{i=3}z_i$, then $u$ is no more a scalar valued Tensor but a 2d 2x2 Tensor. In this case, backward explicitly requires the initial incoming gradient with same size and type as $u$ unlike the scalar valued case where it assumed it be 1. The following example will give error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpkL7QnsmUQ4",
        "colab_type": "code",
        "outputId": "1b676184-f8bc-4612-c9d9-25214e15035c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad = True) \n",
        "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad = True)\n",
        "z = x+y\n",
        "z.retain_grad()\n",
        "u = 2*z\n",
        "\n",
        "u.backward()\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-a18d198c984b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dz: {z.grad}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'dy: {y.grad}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcX9UcELpjif",
        "colab_type": "text"
      },
      "source": [
        "To debug, we will supply the initial incoming gradients through the gradient paprameter of the 'backward' method. See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vCxWqufphK-",
        "colab_type": "code",
        "outputId": "89ef9e79-fb16-4aaf-a88f-e16dd8d9785b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad = True) \n",
        "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad = True)\n",
        "z = x+y\n",
        "z.retain_grad()\n",
        "u = 2*z\n",
        "\n",
        "u.backward(gradient = torch.tensor([[1., 1.],[1., 1.]]))\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dy: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dx: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HJujOmUqHgv",
        "colab_type": "text"
      },
      "source": [
        "Note that the Jacobian at the node of the computation graph associated with z is:\n",
        "<br>\n",
        "<br>\n",
        "&emsp; &emsp; &emsp; $\\begin{bmatrix} \\frac{du_1}{dz_1} & ... & \\frac{du_1}{dz_4} \\\\ \\vdots & \\vdots & \\vdots \\\\  \\frac{du_4}{dz_1} & ... & \\frac{du_4}{dz_4}\\end{bmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyuG_rBMsElj",
        "colab_type": "text"
      },
      "source": [
        "Now, we will look at a couple of more important methods defined in the Tensor class.\n",
        "First, we will look at 'register_hook'  method. Using this method we can register a hook (i.e a handle to a function) w.r.to the Tensor on which it is called. The hook will be called everytime immediately after the gradient  w.r.to this Tensor is computed. The function must take grad attribute of this Tensor as the argument. It must not modify this grad in its body. It may optionally return a new gradient or None. If it returns a new gradient, this new gradient will be used for further backward propagation. \n",
        "<br>\n",
        "Let us revise the above example code by registering a hook for z. The hook will return a new gradient that is reverse (i.e negate) of the gradient at z. This new gradient will be used further in the backpropagation to compute gradients at x and y respectively. However, gradient at z will not be modified. To remove the hook, we can call 'remove' method on the hook. See the demo below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQCgU4eIp7d5",
        "colab_type": "code",
        "outputId": "59ed9739-c27b-4367-f53c-c7b3e164b35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "def hook_z(grad):\n",
        "    return grad * -1\n",
        "\n",
        "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad = True) \n",
        "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad = True)\n",
        "z = x+y\n",
        "z.retain_grad()\n",
        "u = 2*z\n",
        "\n",
        "h = z.register_hook(hook_z)\n",
        "u.backward(gradient = torch.tensor([[1., 1.],[1., 1.]]))\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')\n",
        "\n",
        "h.remove()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dz: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dy: tensor([[-2., -2.],\n",
            "        [-2., -2.]])\n",
            "\n",
            "dx: tensor([[-2., -2.],\n",
            "        [-2., -2.]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoSBQCnq1Dzf",
        "colab_type": "text"
      },
      "source": [
        "We will now look at 'detach' method. To understand this, let's imagine a situation where $z$ = $x+y$ and $u$ = $2*z$. Suppose we want to compute another Tensor $w$ = $z-v$ wherein we want to use the $z$ computed from $x+y$ but we do not require gradient of $w$ w.r.to $z$. Note that $z$ is part of computation graphs of both $u$ and $w$. If we explicitly make requires_grad attribute of $z$ to be False, then $dx$ and $dy$ will be None. If requires_grad of $z$ is True, gradient through 'backward' call on $w$ will also get accumulated into grad attribute of $z$ which in turn will affect $dx$ and $dy$. To resolve this situation, we can use 'detach' method on z. This will return a new Tensor whose requires_grad is False but the new Tensor shares the memory with original Tensor z. So, we have to be careful in not modifying the new Tensor as it will affect z and subsequently will have a cascade effect. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFA4TVj_vx8_",
        "colab_type": "code",
        "outputId": "ccecfb3f-b1b6-4ae3-df80-24771cac95ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "source": [
        "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad = True) \n",
        "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad = True)\n",
        "z = x+y\n",
        "z.retain_grad()\n",
        "u = 2*z\n",
        "\n",
        "v = torch.tensor([[9., 10.], [11., 12.]], requires_grad = True)\n",
        "w = z.detach() - v\n",
        "# w = z - v\n",
        "\n",
        "u.backward(gradient = torch.tensor([[1., 1.],[1., 1.]]), retain_graph = True)\n",
        "print('Grads after call to backward on u')\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')\n",
        "\n",
        "w.backward(gradient = torch.tensor([[1., 1.],[1., 1.]]))\n",
        "print('Grads after call to backward on w')\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dv: {v.grad}\\n')\n",
        "\n",
        "u.backward(gradient = torch.tensor([[1., 1.],[1., 1.]]))\n",
        "print('Grads after 2nd call to backward on u')\n",
        "print(f'dz: {z.grad}\\n')\n",
        "print(f'dy: {y.grad}\\n')\n",
        "print(f'dx: {x.grad}\\n')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Grads after call to backward on u\n",
            "dz: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dy: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dx: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "Grads after call to backward on w\n",
            "dz: tensor([[2., 2.],\n",
            "        [2., 2.]])\n",
            "\n",
            "dv: tensor([[-1., -1.],\n",
            "        [-1., -1.]])\n",
            "\n",
            "Grads after 2nd call to backward on u\n",
            "dz: tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "\n",
            "dy: tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "\n",
            "dx: tensor([[4., 4.],\n",
            "        [4., 4.]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDQtgeFA4uHO",
        "colab_type": "text"
      },
      "source": [
        "You can clearly see in the above example that $dz$ does not get modified when 'backward' is called on $w$. So, during the second call to 'backward' on $u$, gradients at $x$, $y$ and $z$ doubled up as expected. Check yourself the consequences if 'detach' is not called on z in the computation of w."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w-e3SLgcEcH",
        "colab_type": "text"
      },
      "source": [
        "We have seen that gradients accumulate in the grad attribute. If you do not want the accumulation to happen but get the current gradients, you can explicitly set the grad attribute to zero prior to further 'backward' calls. See the following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoABSKn94nWc",
        "colab_type": "code",
        "outputId": "4b365537-18d7-48ab-a3b7-001a289dbae5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "x = torch.tensor(2., requires_grad = True) \n",
        "y = torch.tensor(1., requires_grad = True)\n",
        "w = x+y\n",
        "w.retain_grad()\n",
        "print(f'requires_grad of x, y, w: {x.requires_grad}\\t{y.requires_grad}\\t{w.requires_grad}')\n",
        "z = torch.tensor(-1., requires_grad = True)\n",
        "u = z*w\n",
        "print(f'requires_grad of z, w, u: {z.requires_grad}\\t{w.requires_grad}\\t{u.requires_grad}')\n",
        "\n",
        "u.backward(retain_graph = True) # backward call; graph is not freed after this call\n",
        "print('Grads after first call to backward')\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}\\n\\n')\n",
        "\n",
        "z.grad.data = torch.zeros_like(z) # zero the gradient\n",
        "w.grad.data = torch.zeros_like(w) \n",
        "y.grad.data = torch.zeros_like(y) \n",
        "x.grad.data = torch.zeros_like(x) \n",
        "u.backward() # backward call again; graph will be freed after this call\n",
        "\n",
        "print('Grads after second call to backward')\n",
        "print(f'dz: {z.grad}')\n",
        "print(f'dw: {w.grad}')\n",
        "print(f'dy: {y.grad}')\n",
        "print(f'dx: {x.grad}')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requires_grad of x, y, w: True\tTrue\tTrue\n",
            "requires_grad of z, w, u: True\tTrue\tTrue\n",
            "Grads after first call to backward\n",
            "dz: 3.0\n",
            "dw: -1.0\n",
            "dy: -1.0\n",
            "dx: -1.0\n",
            "\n",
            "\n",
            "Grads after second call to backward\n",
            "dz: 3.0\n",
            "dw: -1.0\n",
            "dy: -1.0\n",
            "dx: -1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB9hGQtY9MkL",
        "colab_type": "text"
      },
      "source": [
        "Now we will look at few methods in torch.autograd package. I encourage you to go throught (https://pytorch.org/docs/stable/autograd.html?highlight=torch%20autograd#module-torch.autograd) for more details.\n",
        "<br><br>\n",
        "Suppose we want to compute higher order derivatives/grdients of a function. 'grad' method in torch.autograd will be very useful for this purpose. You can look at the documentation of 'grad' method for detailed explanation. I will show through two examples below how higher order gradients can be computed using this method.\n",
        "<br>\n",
        "<br>\n",
        "Let $y$ = $x^2$. Then $y^{'}$ = $2x$ , $y^{''} = 2$ and $y^{(3)} = 0$. To compute $y^{'}$, the derivative of y w.r.to x, we can call 'grad' method with ouputs parameter set to ($y$, ) and inputs parameter set to ($x$, ). Since $y$ is a scalar function, we need not explictily supply grad_outputs parameter. We should set create_graph parameter to True. By setting this parameter to True, graph for derivatives will be built which will facilitate computation of higher order derivatives.  Note that $x$ should have its requires_grad set to True. The result returned is a tuple with one Tensor, the derivative of $y$ w.r.to $x$, with its requires_grad set to True. To find $y^{''}$, all that you have to do is to set the ouputs parameter in call to 'grad' with the result obtained from the previous call. Similarly other higher order derivatives can be computed. See the example below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFGKGzazASS6",
        "colab_type": "code",
        "outputId": "db2ff010-59ef-42f1-bb42-11ba03ae5214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import torch.autograd\n",
        "x = torch.tensor(3., requires_grad = True) \n",
        "y = x**2\n",
        "\n",
        "dy_dx = torch.autograd.grad(outputs = (y, ), inputs = (x, ), create_graph = True)\n",
        "print(f'dy_dx at x={x.data}:\\t\\t{dy_dx[0].data}')\n",
        "\n",
        "d2y_dx2 = torch.autograd.grad(outputs = dy_dx, inputs = (x, ), create_graph = True)\n",
        "print(f'd2y_dx2 at x={x.data}:\\t{d2y_dx2[0].data}')\n",
        "\n",
        "d3y_dx3 = torch.autograd.grad(outputs = d2y_dx2, inputs = (x, ))\n",
        "print(f'd3y_dx3 at x={x.data}:\\t{d3y_dx3[0].data}')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dy_dx at x=3.0:\t\t6.0\n",
            "d2y_dx2 at x=3.0:\t2.0\n",
            "d3y_dx3 at x=3.0:\t0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIjUVLMWD3E9",
        "colab_type": "text"
      },
      "source": [
        "Given below is another example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3dbfrtcD7mx",
        "colab_type": "code",
        "outputId": "1c39f279-8c7f-4412-99fb-eb513a707609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad = True)\n",
        "y = torch.mean(x ** 2) * torch.ones(2, 3)\n",
        "print(y.requires_grad)\n",
        "dy_dx = torch.autograd.grad(outputs = (y, ), inputs = (x, ), grad_outputs = (torch.ones(y.size()), ), create_graph = True)\n",
        "print(f'dy_dx: {dy_dx}')\n",
        "d2y_dx2 = torch.autograd.grad(outputs = dy_dx, inputs = (x, ), grad_outputs = (torch.ones(y.size()), ))\n",
        "print(f'd2y_dx2: {d2y_dx2}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "dy_dx: (tensor([[ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.]], grad_fn=<MulBackward0>),)\n",
            "d2y_dx2: (tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.]]),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvtY_3paUDI2",
        "colab_type": "text"
      },
      "source": [
        "Finally, as part of this tutorial, we will look at two context-managers defined in autograd package namely, 'no_grad' and 'enable_grad'. Generally, when testing the model, we perform only forward computations and no backward computation. By invoking the context-manager 'no_grad' during such time, storing intermediate results of forward computations will be avoided since there is no follow-up backward. Otherwise memory will be wasted. See an example code below for using 'no_grad' context-manager. Note in the example that 'no_grad' can also be used as a function decorator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9r9yXTWVQlS",
        "colab_type": "code",
        "outputId": "f00b66e9-fe03-4904-a8e9-5da1fe4b2aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x = torch.tensor([1.], requires_grad=True)\n",
        "with torch.autograd.no_grad():    # we can also use torch.no_grad in place of torch.autograd.no_grad\n",
        "    y = x * 2\n",
        "print(y.requires_grad)\n",
        "\n",
        "@torch.autograd.no_grad()    # we can also use torch.no_grad in place of torch.autograd.no_grad\n",
        "def doubler(x):\n",
        "    return x * 2\n",
        "z = doubler(x)\n",
        "print(z.requires_grad)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhwRCGHLVuRS",
        "colab_type": "text"
      },
      "source": [
        "If for some reason, gradient computation need to be enabled witihin the 'no_grad' context, 'enable_grad' context-manager can be used. See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y2BGFLrWNqR",
        "colab_type": "code",
        "outputId": "e6065c40-cc99-4b59-af24-2b293a304209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x = torch.tensor([1.], requires_grad=True)\n",
        "with torch.autograd.no_grad():  # we can also use torch.no_grad in place of torch.autograd.no_grad\n",
        "    with torch.autograd.enable_grad():  # we can also use torch.enable_grad in place of torch.autograd.enable_grad\n",
        "        y = x * 2\n",
        "        \n",
        "print(y.requires_grad)\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "@torch.enable_grad()     # we can also use torch.enable_grad in place of torch.autograd.enable_grad\n",
        "def doubler(x):\n",
        "    return x * 2\n",
        "\n",
        "with torch.no_grad():    # we can also use torch.no_grad in place of torch.autograd.no_grad\n",
        "    z = doubler(x)\n",
        "print(z.requires_grad)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor([2.])\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG7htrVahzpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}