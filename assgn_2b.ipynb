{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization\n",
    "In this assignment you will implement dropout regularization. See the class notes for understanding dropout regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let' import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse # for command-line parsing\n",
    "import matplotlib # for plotting\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from abc import ABC, abstractmethod # for crating abstract classes\n",
    "\n",
    "from assign2_utils import load_train_data, load_test_data, flatten, df \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deviate from the function style implementations in the previous assigments to class style implementation in this assigment. The previous assignments would have made you little more familiar with python. You should feel comfortable with some aspects of basic python and numpy. However, note that these assignments will not teach you python. Whereever possible, we will try to touch different aspects of python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "We will create an abstract class called Layers. Every layer whether linear or non-linear or dropout or batchnorm should inherit from this class. Each such concrete layer should override the forward and backward method of the abstract class. A concrete layer should also override the update_params method of the abstract class if it has parameters like weights and biases to be learnt during training. Further, we will make the layers callable classes so that the forward method is automatically called when the class is called with parameters to forward method.\n",
    "\n",
    "So, here is the abstract class Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers(ABC): # Layers inherits from ABC.The class ABC in abc module is required to make a class abstract. \n",
    "                   # Native python does not support abstract classes\n",
    "    def __init__(self): \n",
    "        super().__init__() # calls the constructor of the super class ABC\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, *x): # should be overridden by every class that inherits from this class\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, *da): # should be overridden by every class that inherits from this class\n",
    "        pass    \n",
    "    \n",
    "    def update_params(self, learning_rate = None): # should be overridden by those inheriting classes that have\n",
    "                                                   # parameters to be learnt\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, *x): # makes the inheriting classes callable, calling the forward method of the class\n",
    "        return self.forward(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create the Linear layer. When an object of this layer is instantiated, the number of incoming features/nodes, number of outgoing activations/features, whether bias is required at this layer, whether regularization is required at this layer are initialized. Also, the weight matrix and the bias vector(if required) are initialized. Gradients of loss with respect to weights and biases are initialized to empty. Then, forward, backward and update_params are overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layers): # inherits from abstract class Layers\n",
    "    def __init__(self, in_features, out_features, bias = True, regularization = None): # constructor\n",
    "        super().__init__()\n",
    "        self.in_features = in_features # initialize number of incoming features\n",
    "        self.out_features = out_features # initialize number of outgoing features\n",
    "        self.weight = np.random.randn(out_features, in_features) * .01 # initialize weight matrix\n",
    "        if bias:\n",
    "            self.bias = np.zeros((out_features, 1)) # initialize bias if required\n",
    "        else:\n",
    "            self.bias = None # if bias not required, set it to None\n",
    "        self.regularization = regularization # initialize regularization\n",
    "        if self.regularization is not None: \n",
    "            self.reg_penalty = 0 # if valid regularization at this layer, set the regularization penalty\n",
    "                                 # at this layer initially to zero\n",
    "            \n",
    "        self.dw = np.empty_like(self.weight) # intialize dw to empty\n",
    "        self.db = np.empty_like(self.bias) if self.bias is not None else None \n",
    "                                            # initialize db to empty if bias == True\n",
    "        \n",
    "    def forward(self, x): # forward method overridden; x is the incoming activation. \n",
    "                          # shape of x is (num of activations, num of samples)\n",
    "            \n",
    "        m = x.shape[1]    # number of training examples\n",
    "        self.x = x # x is required for backward. We don't need a separate cache. We can store it in the object.\n",
    "        \n",
    "        output = self.weight @ x # computation of the linear part Wx                                 \n",
    "        if self.bias is not None:            \n",
    "            output += self.bias # add to Wx bias if bias == True\n",
    "                                # Note that we don't apply non-linearity here as this layer computes only Wx+b\n",
    "        \n",
    "        #update regularization penalty at this layer\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L2':\n",
    "            self.reg_penalty = args.lamda/(2*m) * np.sum(self.weight**2)\n",
    "        elif self.regularization == 'L1':\n",
    "            self.reg_penalty = args.lamda/m * np.sum(np.abs(self.weight))\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        return output # return forward output as the next layer in the model will require it     \n",
    "    \n",
    "    # Backward of this layer will receive dz. Note that at this layer z = Wx+b. So backward will compute\n",
    "    # dw, db and dx. To compute dW, x is required. That's why x was stored in forward. To compute dx, W is \n",
    "    # required. This is already available in self.weight\n",
    "    \n",
    "    def backward(self, dz): # backward method overridden\n",
    "        m = dz.shape[1]     # number of training examples\n",
    "        self.dw = dz @ self.x.T # compute dw\n",
    "        \n",
    "        # add derivative of regularization penalty at this layer w.r.to w\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L1':\n",
    "            signw = np.sign(self.weight)\n",
    "            signw[signw == 0] = 1\n",
    "            self.dw += args.lamda/m * signw\n",
    "        elif self.regularization == 'L2':\n",
    "            self.dw += args.lamda/m * self.weight\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        if self.bias is not None: # compute db if bias == True\n",
    "            self.db = np.sum(dz, axis = 1, keepdims = True) \n",
    "            \n",
    "        dx = self.weight.T @ dz  # compute dx     \n",
    "        return dx # we only return dx as this this required for chain rule in the next layer \n",
    "                  # in backward direction. dw and db are kept available in this object which will directly\n",
    "                  # be used by update_params for updating weights and biases\n",
    "    \n",
    "    # update parameters in this layer \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.weight -= learning_rate*self.dw\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate*self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you got the idea of building different layers. Now, you will implement the NonLinear layer class. During instantiation of an object of type NonLinear, the nonlinearity name denoted by fname is set to the one received by the contructor. This part is already done for you. You are required to override forward and backward methods of the parent abstract class. \n",
    "\n",
    "forward method will receive input as x and return the non-linear mapping of x where the non-linearity is specified by fname. Note that this input x would have come from the previous Linear layer.\n",
    "\n",
    "backward method will receive dx, the gradient from the previous layer and return the gradient of loss at this layer.\n",
    "\n",
    "update_params not required to be overridden here as this layer does not have any parameters to be learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(Layers):\n",
    "    def __init__(self, fname='ReLU'):\n",
    "        super().__init__()\n",
    "        self.fname = fname\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if self.fname == 'ReLU':\n",
    "            return np.maximum(x, 0)\n",
    "        elif self.fname == 'Sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fname == 'Tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError('Unknown non-linear function error')\n",
    "    \n",
    "    def backward(self, dx):    # implemented instead of using df(...)\n",
    "        if self.fname == 'ReLU':\n",
    "            return dx * (self.x > 0)\n",
    "        elif self.fname == 'Sigmoid':\n",
    "            sigmoid_x = 1 / (1 + np.exp(-self.x))\n",
    "            return dx * sigmoid_x * (1 - sigmoid_x)\n",
    "        elif self.fname == 'Tanh':\n",
    "            return dx * (1 - np.tanh(self.x)**2)\n",
    "        else:\n",
    "            raise ValueError('Unknown non-linear function error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you will implement Dropout layer class in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layers):\n",
    "    def __init__(self, keep_prob = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "    \n",
    "    def forward(self, x, train=True):\n",
    "        if train:    \n",
    "            d = np.random.rand(*x.shape)\n",
    "            d = (d < self.keep_prob)\n",
    "            self.d = d\n",
    "            x = x * d\n",
    "            x = x / self.keep_prob\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        return dx * self.d\n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all layers are built, we will build our model. Our model is a composition of these layers. Let's say our model is as follows:\n",
    "        \n",
    "        Ip layer(I)----->Linear(L1)------>Non-linear(R)------>Dropout(D)----->Linear(L2)----->Non-linear(S)\n",
    " \n",
    "Note that the first group of Linear, Non-linear and Dropout layers comprise the first hidden layer. The second group of Linear and Non-linear layers forms the output layer.\n",
    "\n",
    "- Ip features shape: nx$^{[0]}$ x m (a batch of m samples each of dim nx. In this assignment, nx$^{[0]}$ will be 64\\*64*3 = 12288).\n",
    "- weights between I and L1 have shape: nx$^{[1]}$ x nx$^{[0]}$. nx$^{[1]}$ = 32.\n",
    "- bias vector at L1 has shape: nx$^{[1]}$ x 1\n",
    "- non-linear layer R is ReLU.\n",
    "- weights between D and L2 have shape: nx$^{[2]}$ x nx$^{[1]}$. nx$^{[2]}$ = 1.\n",
    "- bias vector at L2 has shape: nx$^{[2]}$ x 1\n",
    "- non-linear layer R is Sigmoid.\n",
    "\n",
    "The model is shown below. It is self-explanatory provided you go through the code carefully. We have also added a loss method that computes and returns the logistic loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        # self.fc1 = Linear(in_features, 32, regularization = 'L2')\n",
    "        self.fc1 = Linear(in_features, 32)\n",
    "        self.relu = NonLinear('ReLU')\n",
    "        self.dp = Dropout()\n",
    "        self.fc2 = Linear(32, 1)\n",
    "        self.sigmoid = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.relu(x)\n",
    "        x = self.dp(x,train=train)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.sigmoid.backward(d_output)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dx = self.dp.backward(dx)\n",
    "        dz = self.relu.backward(dx)\n",
    "        dx = self.fc1.backward(dz)  \n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code including train function, test function and main function are shown below. The codes are self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "parser.add_argument('--print_freq', metavar='N', type=int, default=300, help='print model loss every print_freq iterations')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 3000 --alpha .008'.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y):\n",
    "    for i in range(args.miter):\n",
    "        output = model(x) # model is a callable object with call to its forward method.\n",
    "                          # we could also have written the rhs as model.forward(x)\n",
    "        L = model.loss(output, y)\n",
    "        model.backward(output, y)\n",
    "        model.update_params(args.alpha)\n",
    "        if not i%args.print_freq: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "                \n",
    "def test_model(model, x, y):\n",
    "    predictions = model(x, train=False)\n",
    "    predictions[predictions > 0.5] = 1\n",
    "    predictions[predictions <= 0.5] = 0\n",
    "    acc = np.mean(predictions == y)\n",
    "    acc = np.asscalar(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6963\n",
      "Loss at iteration 300:\t0.5974\n",
      "Loss at iteration 600:\t0.4506\n",
      "Loss at iteration 900:\t0.3342\n",
      "Loss at iteration 1200:\t0.2633\n",
      "Loss at iteration 1500:\t0.1985\n",
      "Loss at iteration 1800:\t0.1051\n",
      "Loss at iteration 2100:\t0.0779\n",
      "Loss at iteration 2400:\t0.0465\n",
      "Loss at iteration 2700:\t0.0385\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # Instantiate the model\n",
    "    my_model = Model(x.shape[0])\n",
    "    \n",
    "    # train the model\n",
    "    train(my_model, x, y)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
