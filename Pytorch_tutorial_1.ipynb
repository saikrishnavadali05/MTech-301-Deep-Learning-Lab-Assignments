{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_tutorial_1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"cvNpRzedPV0J","colab_type":"text"},"source":["# Pytorch Tutorial - Part 1\n","This set of tutorials is only a supplement to the existing tutorials available at [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/?utm_source=Google&utm_medium=PaidSearch&utm_campaign=%2A%2ALP+-+TM+-+General+-+HV+-+IN&utm_adgroup=PyTorch+Tutorials&utm_keyword=pytorch%20tutorials&utm_offering=AI&utm_Product=PyTorch&gclid=EAIaIQobChMIqPLjy8LA4wIVmQ4rCh1DpA7AEAAYASAAEgLy1fD_BwE). I strongly recommend you all to go through the official tutorials as well.\n","\n","Let's begin. Firstly, make sure the runtime type chosen in the colab environment is GPU. To do that, pull down Runtime menu on the toolbar above and choose Runtime type to GPU.\n","\n","To work with PyTorch, we have to import the module named 'torch'.\n","\n","Let's first check the version of PyTorch loaded."]},{"cell_type":"code","metadata":{"id":"v-9vb7WEPai-","colab_type":"code","outputId":"9a9a726f-477e-4b2c-c395-9b941af9ed6b","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1564046164332,"user_tz":-330,"elapsed":1963,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["import torch\n","import numpy as np\n","torch.__version__"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.1.0'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"pxgWlm1HQwnv","colab_type":"text"},"source":["So, the latest stable version is loaded. \n","\n","As stated in the official docs, PyTorch is a Python-based scientific computing package targeted at two sets of audiences:\n","\n"," - A replacement for NumPy to use the power of GPUs\n"," - a deep learning research platform that provides maximum flexibility and speed\n"," \n","Similar to ndarrays in numpy, the basic entity in PyTorch is Tensors. They are similar to ndarrays but they can be ported to GPU's and manipulated.\n"," \n"," So. let's create tensors."]},{"cell_type":"code","metadata":{"id":"vpR6NybHQnv8","colab_type":"code","outputId":"f478d625-47ec-4342-eee7-41844af91d20","colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"status":"ok","timestamp":1564046164337,"user_tz":-330,"elapsed":1943,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["# Use torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) to create a tensor.\n","\n","t0 = torch.tensor(data = 2, dtype = torch.uint8, device = 'cpu') # a zero dimensional tensor\n","print(f'Tensor {t0} of type {type(t0)} with size {t0.size()} and data type {t0.dtype} residing on {t0.device}. It has {t0.dim()} dimensions.')\n","\n","t1 = torch.tensor(data = [1, 2], dtype = torch.float, device = 'cpu') # a 1 dimensional tensor\n","print(f'\\nTensor {t1} of type {type(t1)} with size {t1.size()} and data type {t1.dtype} residing on {t1.device}. It has {t1.dim()} dimension.') \n","\n","t2 =torch.tensor(data = ((1, 2), (3, 4)), dtype = torch.int32) # a 2 dimensional tensor\n","print(f'\\nTensor {t2} of type {type(t1)} with size {t2.size()} and data type {t2.dtype} residing on {t2.device}. It has {t2.dim()} dimensions.')    \n","\n","import numpy as np\n","t3 = torch.tensor(data = np.array([[[1, -2, 3.0], [4, -5.2, 6.1]], [[1, -1, .2], [0, .9, -2.3]]]), dtype = torch.double) # a 3 dimensional tensor\n","print(f'\\nTensor {t3} of type {type(t3)} with size {t3.size()} and data type {t3.dtype} residing on {t3.device}. It has {t3.dim()} dimensions.')\n","\n","t4 = torch.tensor(data = []) # an empty tensor. Its size is 0.\n","print(f'\\nTensor {t4} of type {type(t4)} with size {t4.size()} and data type {t4.dtype} residing on {t4.device}. It has {t4.dim()} dimension.')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Tensor 2 of type <class 'torch.Tensor'> with size torch.Size([]) and data type torch.uint8 residing on cpu. It has 0 dimensions.\n","\n","Tensor tensor([1., 2.]) of type <class 'torch.Tensor'> with size torch.Size([2]) and data type torch.float32 residing on cpu. It has 1 dimension.\n","\n","Tensor tensor([[1, 2],\n","        [3, 4]], dtype=torch.int32) of type <class 'torch.Tensor'> with size torch.Size([2, 2]) and data type torch.int32 residing on cpu. It has 2 dimensions.\n","\n","Tensor tensor([[[ 1.0000, -2.0000,  3.0000],\n","         [ 4.0000, -5.2000,  6.1000]],\n","\n","        [[ 1.0000, -1.0000,  0.2000],\n","         [ 0.0000,  0.9000, -2.3000]]], dtype=torch.float64) of type <class 'torch.Tensor'> with size torch.Size([2, 2, 3]) and data type torch.float64 residing on cpu. It has 3 dimensions.\n","\n","Tensor tensor([]) of type <class 'torch.Tensor'> with size torch.Size([0]) and data type torch.float32 residing on cpu. It has 1 dimension.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8xghTY76Vuvv","colab_type":"text"},"source":["From the above examples, you should be clear about how data is supplied to tensor method (as a scalar or list or tupe or ndarray etc), type of the tensor created (the created tensor is an object of class torch.Tensor), the datatype of the tensor (as specified by the dtype attribute), the size of the tensor (as returned by the size() method on the tensor) and the residence of the tensor (currently cpu). With this you should not be finding any difference between the tensor and a numpy array. But, a tensor has another special attribute called requires_grad and also, tensors can reside on GPU. We will see more about requires_grad attribute of tensors and porting tensors to GPU very soon.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zzIpUso2Zlzo","colab_type":"text"},"source":["## Exercise\n","Create a 3 dimensional tensor of some size with appropriate number of long integers in it residing in CPU."]},{"cell_type":"code","metadata":{"id":"WTu1MClPZ9Ee","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2cfda27d-0579-4276-b4a6-ea18bc1e4e27","executionInfo":{"status":"ok","timestamp":1564046164341,"user_tz":-330,"elapsed":1931,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["# do the exercise here\n","t = torch.tensor(data=[[[i for i in range(2)]]], dtype=torch.long, device='cpu')\n","print(f' Tensor {t} of type {type(t)} with size {t.size()} and data type {t.dtype} residing on {t.device}. It has {t.dim()} dimensions.')"],"execution_count":3,"outputs":[{"output_type":"stream","text":[" Tensor tensor([[[0, 1]]]) of type <class 'torch.Tensor'> with size torch.Size([1, 1, 2]) and data type torch.int64 residing on cpu. It has 3 dimensions.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3D-eKUEDUQ-q","colab_type":"code","outputId":"a8dcff68-e0f7-4711-8af2-e31aaa441eb6","colab":{"base_uri":"https://localhost:8080/","height":675},"executionInfo":{"status":"ok","timestamp":1564046164344,"user_tz":-330,"elapsed":1917,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["# other ways of creating tensor\n","t0 = torch.zeros((2, 3), dtype = torch.uint8, device = 'cpu')\n","t1 = torch.ones((2, 3), dtype = torch.uint8)\n","t2 = torch.arange(start = 2.0, end = 5.0, step = 0.5)\n","t3 = torch.linspace(start = 2, end = 5, steps = 100)\n","t4 = torch.eye(3, dtype = torch.uint8)\n","t5 = torch.empty((2, 3)) #initialized to a 2x3 tensor with some random values\n","t6 = torch.empty_like(t0, dtype = torch.float32) # has same size as t0\n","t7 = torch.full(size = (2, 3), fill_value = 3.5, dtype = torch.int) # observe the result printed carefully\n","t8 = t7.new_tensor([[7, 8, 9, 13], [10, 11, -12, -14]]) #has same dtype as t7 but size is different; it is (2, 4)\n","\n","for i in range(8):\n","    s = 't'+str(i)\n","    print(f't{i}: {vars()[s]}\\n')\n","\n","\n","#convert from numpy to tensor\n","a = np.array([1, 2, 3])\n","t8 = torch.from_numpy(a) # t8 and a share same cpu memory \n","t8[0] = -1 \n","print(f't8: {t8} residing in {t8.device}')\n","print(f'a: {a}') # a is also modified\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["t0: tensor([[0, 0, 0],\n","        [0, 0, 0]], dtype=torch.uint8)\n","\n","t1: tensor([[1, 1, 1],\n","        [1, 1, 1]], dtype=torch.uint8)\n","\n","t2: tensor([2.0000, 2.5000, 3.0000, 3.5000, 4.0000, 4.5000])\n","\n","t3: tensor([2.0000, 2.0303, 2.0606, 2.0909, 2.1212, 2.1515, 2.1818, 2.2121, 2.2424,\n","        2.2727, 2.3030, 2.3333, 2.3636, 2.3939, 2.4242, 2.4545, 2.4848, 2.5152,\n","        2.5455, 2.5758, 2.6061, 2.6364, 2.6667, 2.6970, 2.7273, 2.7576, 2.7879,\n","        2.8182, 2.8485, 2.8788, 2.9091, 2.9394, 2.9697, 3.0000, 3.0303, 3.0606,\n","        3.0909, 3.1212, 3.1515, 3.1818, 3.2121, 3.2424, 3.2727, 3.3030, 3.3333,\n","        3.3636, 3.3939, 3.4242, 3.4545, 3.4848, 3.5152, 3.5455, 3.5758, 3.6061,\n","        3.6364, 3.6667, 3.6970, 3.7273, 3.7576, 3.7879, 3.8182, 3.8485, 3.8788,\n","        3.9091, 3.9394, 3.9697, 4.0000, 4.0303, 4.0606, 4.0909, 4.1212, 4.1515,\n","        4.1818, 4.2121, 4.2424, 4.2727, 4.3030, 4.3333, 4.3636, 4.3939, 4.4242,\n","        4.4545, 4.4848, 4.5152, 4.5455, 4.5758, 4.6061, 4.6364, 4.6667, 4.6970,\n","        4.7273, 4.7576, 4.7879, 4.8182, 4.8485, 4.8788, 4.9091, 4.9394, 4.9697,\n","        5.0000])\n","\n","t4: tensor([[1, 0, 0],\n","        [0, 1, 0],\n","        [0, 0, 1]], dtype=torch.uint8)\n","\n","t5: tensor([[2.1205e-36, 0.0000e+00, 3.3631e-44],\n","        [0.0000e+00,        nan, 0.0000e+00]])\n","\n","t6: tensor([[2.1205e-36, 0.0000e+00, 0.0000e+00],\n","        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n","\n","t7: tensor([[3, 3, 3],\n","        [3, 3, 3]], dtype=torch.int32)\n","\n","t8: tensor([-1,  2,  3]) residing in cpu\n","a: [-1  2  3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Y_XbVJ3rkxZ","colab_type":"text"},"source":["## Exercise\n","Create a new tensor of same data type as t7 defined above but with size 10 by 10 filled with the value -2. It should reside in CPU. Hint: use method [new_full] (https://pytorch.org/docs/stable/tensors.html#torch.Tensor) in Tensor class."]},{"cell_type":"code","metadata":{"id":"-jcXP3QHhnXZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":220},"outputId":"4158b8c2-999c-489a-f0fe-b15cbe0f52e3","executionInfo":{"status":"ok","timestamp":1564046164347,"user_tz":-330,"elapsed":1906,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["#complete exercise here\n","# new_full(size, fill_value, dtype=None, device=None, requires_grad=False) â†’ Tensor\n","t9 = torch.Tensor().new_full(size=(10, 10), fill_value=-2, dtype=t7.dtype, device='cpu')\n","print(f' Tensor {t9} of type {type(t9)} with size {t9.size()} and data type {t9.dtype} residing on {t9.device}. It has {t9.dim()} dimensions.')"],"execution_count":5,"outputs":[{"output_type":"stream","text":[" Tensor tensor([[-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2],\n","        [-2, -2, -2, -2, -2, -2, -2, -2, -2, -2]], dtype=torch.int32) of type <class 'torch.Tensor'> with size torch.Size([10, 10]) and data type torch.int32 residing on cpu. It has 2 dimensions.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XrDRsfWHO69D","colab_type":"text"},"source":["We will now do some operations on tensors. "]},{"cell_type":"code","metadata":{"id":"EDhuaj1jPFEU","colab_type":"code","outputId":"c65fb49c-bd70-4400-c386-ef82e3751bc8","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1564046164353,"user_tz":-330,"elapsed":1897,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["# operations on Tensors\n","\n","t0 = torch.tensor(data = [[1., 2.], [3., 4.]])\n","t1 = torch.full(size = (2, 2), fill_value = 5.)\n","\n","t2_1 = t0+t1 # adding Tensors t0 and t1\n","t2_2 = t0.add(t1) # this also adds Tensor t0 to t1; note that add is a method defined for Tensor object.\n","print(f't2_1: {t2_1}\\nt2_2: {t2_2}')\n","\n","t2_1.add_(t1) # inplace addition; result of adding t0 to t1 is stored in t0; \n","            # In pytorch, operations ending with underscore means inplace operation.\n","print(f't2_1: {t2_1}')\n","\n","t3 = t0*t1 # elementwise multilication of two tensors i.e Hadamard product\n","print(f't3: {t3}')\n","\n","data1 = [[[1., 2., 3.], [4., 5., 6.]], [[-1., 2., 0], [1., 0, -3.]]]\n","data2 = [[[0., 1.], [-1., 4.], [9., -2.]], [[1., 4.], [0, 1.], [-2., 0]]]\n","t4 = torch.tensor(data = data1)\n","t5 = torch.tensor(data = data2)\n","t6 = t4.matmul(t5) # multiplying two 3d tensors; t4 is 2 x 2 x 3; t5 is 2 x 3 x 2; result t6 is 2 x 2 x 2\n","                   # multiplication is happening here at dimensions 2 and 3.\n","                   # look at torch.matmul for more details\n","print(f't6: {t6}')\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["t2_1: tensor([[6., 7.],\n","        [8., 9.]])\n","t2_2: tensor([[6., 7.],\n","        [8., 9.]])\n","t2_1: tensor([[11., 12.],\n","        [13., 14.]])\n","t3: tensor([[ 5., 10.],\n","        [15., 20.]])\n","t6: tensor([[[25.,  3.],\n","         [49., 12.]],\n","\n","        [[-1., -2.],\n","         [ 7.,  4.]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_saaUjaDXb12","colab_type":"text"},"source":["For more arithmetic, logic, comparison, trignometric etc operations, you should look at (https://pytorch.org/docs/stable/tensors.html#torch.Tensor).  The following exercise will elicit some of the Tensor operations for you."]},{"cell_type":"markdown","metadata":{"id":"Ilq_8lknYIZj","colab_type":"text"},"source":["## Exercise\n","1. Create two 1d Tensors named t0 and t1 of dimensions 2x2x4 and 2x4x3 respectively . Use ''matmul' method of Tensor class to multiply these two tensors.\n","\n","2. Print the positions where maximum occurs along dimension 1 in t0 defined in part1. Hint: Look at 'argmax' method of Tensor class. Make sure that your output has dimesnions 2x1x4.\n","\n","3. Clamp the values in t0 defined in part 1 inplace between -10 and 20. You can use 'clamp_' method defined in Tensor class.\n","\n","4. Create an empty Tensor t2 that has same size as t1 defined in part 1. Copy the contents of t1 to t2 using 'copy_' method defined in Tensor class.\n","\n","5. Split t1 defined in part 1 in to a list of 3 Tensors along dimension 1 using 'chunk' method defined in the Tensor class. Provide another answer to this question using 'split' method defined in the Tensor class.\n","\n","6. Create an empty Tensor t2 that has same size as t1 defined in part 1. Fill it with the value 2.92. You can use 'fill_' method defined in the Tensor class.\n","\n","7. Flatten t1 defined in part 1 from dimension 1 to the ending dimension. You can use 'flatten' method defined in the Tensor class.\n","\n","8. Create a 4d Tensor named t4  of size 64x3x128x128 using torch.rand. Check if elements of  t4 are contiguously stored in the  memory. You can use 'is_contiguous' method defined in the Tensor class. If not stored contiguously, use 'contiguous' method defined in the Tensor class to contiguously store the elements of t4. If stored contiguously, still go to the docs and read about 'contiguous' method.\n","\n","9. Create a Tensor t5 that is a scalar with value -2.7. Use 'item' method defined in the Tensor class to convert Tensor t5 to the corresponding python number.\n","\n","10. Return the sub-tensor where t1 defined in part 1 is > 7 using 'masked_select' method defined in the Tensor class.\n","\n","11. Convert t1 defined in part1 to an ndarray using 'numpy' method defined in the Tensor class.\n","\n","12. In t1 defined in part1 you have two 4x3 matrices. Extract 1st row of 1st matrix in to a Tensor named t6. Tensor indexing is similar to numpy type indexing. You can use indexing to do this. Similarly, extract 1st row of 2nd matrix from t0 defined in part 1 in to a Tensor named t7. Compute the inner product of these two Tensors using the 'dot' method defined in the Tensor class.\n","\n","13.  Create two 3d Tensors named t8 and t9 of dimensions 2x2x4 and 2x3x4 respectively . Use 'matmul' method of Tensor class to multiply these two tensors. Do you get error? You should! Now, use 'permute method' defined in the Tensor class to permute the dimensions of Tensor t9 appropriately so that 'matmul' is possible now and the result is of dimensions 2x2x3.\n","\n","14. Create an empty Tensor of dimension 2x3. Fill it with random values generated from the uniform distribution on the interval [11, 12] using 'random_' method defined in the Tensor class.\n","\n","15. Create an empty Tensor of dimension 2x3. Fill it with random values generated from the continuous uniform distribution on the interval [0, 1] using 'uniform_' method defined in the Tensor class.\n","\n","16. Create an empty Tensor of dimension 2x3. Fill it with zeros using 'zero_' method defined in the Tensor class.\n","\n","17. Using 'transpose' method defined in the Tensor class, transpose dimensions 1 and 2 in t1 defined in part 1.\n","\n","18. Sum along dimension 1 the Tensor t1 defined in part1. The ouput should have same number of dimensions as t1. Now, find mimimum along dimension 2 of the output. Now squeeze out the dimensions with size 1. 'sum', 'min' and 'squeeze' methods defined in the Tensor class will be useful. Further, read about 'unsqueeze' method defined in the Tensor class.\n","\n"]},{"cell_type":"code","metadata":{"id":"JYDAI99HCxF7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e7db07ad-fc1e-47a4-b2ab-bea68f5701b1","executionInfo":{"status":"ok","timestamp":1564046164857,"user_tz":-330,"elapsed":2385,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["# complete the exercise here\n","\n","# 1. Create two 3d Tensors named t0 and t1 of dimensions 2x2x4 and 2x4x3 respectively . Use ''matmul' method of Tensor class to multiply these two tensors.\n","t0 = torch.full(size=(2,2,4), fill_value=30.)\n","t1 = torch.full(size=(2,4,3), fill_value=3.)\n","res1 = torch.Tensor(t0).matmul(t1)\n","print(\"res1: \", res1)\n","print(\"res1.size(): \", res1.size())\n","\n","\n","# 2. Print the positions where maximum occurs along dimension 1 in t0 defined in part1. Hint: Look at 'argmax' method of Tensor class.\n","#    Make sure that your output has dimesnions 2x1x4.\n","res2 = t0.argmax(dim=1, keepdim=True) \n","print(\"\\n\\nres2: \", res2)\n","print(\"res2.size(): \", res2.size())\n","\n","# 3. Clamp the values in t0 defined in part 1 inplace between -10 and 20. You can use 'clamp_' method defined in Tensor class.\n","res3 = t0.clamp_(min=-10, max=20)\n","print(\"\\n\\nres3: \", res3)\n","print(\"t0: \", t0)\n","\n","\n","# 4. Create an empty Tensor t2 that has same size as t1 defined in part 1. Copy the contents of t1 to t2 using 'copy_' method defined in Tensor class.\n","t2 = torch.empty(size=t1.size())\n","res4 = t2.copy_(t1)\n","print(\"\\n\\nres4: \", res4)\n","print(\"t2: \", t2)\n","\n","# 5. Split t1 defined in part 1 in to a list of 3 Tensors along dimension 1 using 'chunk' method defined in the Tensor class.\n","#    Provide another answer to this question using 'split' method defined in the Tensor class.\n","res5_1 = t1.chunk(chunks=3, dim=1)\n","print(\"\\n\\nres5_1: \", res5_1)\n","print(\"len(res5_1): \", len(res5_1))\n","res5_2 = t1.split(split_size=2, dim=1)\n","print(\"res5_2: \", res5_2)\n","print(\"len(res5_2): \", len(res5_2))\n","\n","# 6. Create an empty Tensor t2 that has same size as t1 defined in part 1. Fill it with the value 2.92. You can use 'fill_' method defined in the Tensor class.\n","res6 = torch.empty(size=t1.size()).fill_(2.92)\n","print(\"\\n\\nres6: \", res6)\n","\n","\n","# 7. Flatten t1 defined in part 1 from dimension 1 to the ending dimension. You can use 'flatten' method defined in the Tensor class.\n","t1 = t1.flatten(start_dim=1, end_dim=-1)\n","print(\"\\n\\n7. t1.size(): \", t1.size())\n","\n","\n","# 8. Create a 4d Tensor named t4 of size 64x3x128x128 using torch.rand. Check if elements of t4 are contiguously stored in the memory.\n","#    You can use 'is_contiguous' method defined in the Tensor class. If not stored contiguously, use 'contiguous' method defined in the Tensor class\n","#    to contiguously store the elements of t4. If stored contiguously, still go to the docs and read about 'contiguous' method.\n","res8_1 = torch.rand(64, 3, 128, 128)\n","print(\"\\n\\nres8_1.is_contiguous():  \", res8_1.is_contiguous())\n","if not res8_1.is_contiguous():\n","    res8_1.contiguous()\n","    print(\"\\n\\nres8_1.is_contiguous():  \", res8_1.is_contiguous())\n","\n","# 9. Create a Tensor t5 that is a scalar with value -2.7. Use 'item' method defined in the Tensor class to convert Tensor t5 to the corresponding python number.\n","res9_1 = torch.tensor(data=-2.7)\n","print(\"\\n\\nres9_1: \", res9_1)\n","res9_2 = res9_1.item()\n","print(\"type(res9_2): \", type(res9_2))\n","\n","\n","# 10. Return the sub-tensor where t1 defined in part 1 is > 7 using 'masked_select' method defined in the Tensor class.\n","t1 = torch.full(size=(2,4,3), fill_value=3.)\n","mask = t1.gt(7)\n","res10 = torch.masked_select(t1, mask)\n","print(\"\\n\\nres10: \", res10)\n","\n","\n","# 11. Convert t1 defined in part1 to an ndarray using 'numpy' method defined in the Tensor class.\n","res11 = t1.numpy()\n","print(\"\\n\\nres11: \", res11)\n","\n","\n","# 12. In t1 defined in part1 you have two 4x3 matrices. Extract 1st row of 1st matrix in to a Tensor named t6.\n","# Tensor indexing is similar to numpy type indexing. You can use indexing to do this. Similarly, extract 1st row of 2nd matrix \n","# from t0 defined in part 1 in to a Tensor named t7. Compute the inner product of these two Tensors using the 'dot' method defined in the Tensor class.\n","t6 = t1[0, 0, :]\n","t7 = t1[1, 0, :]\n","print(\"\\n\\n12. t6: \", t6)\n","print(\"t7: \", t7)\n","res12 = t6.dot(t7)\n","print(\"res12: \", res12)\n","\n","# 13. Create two 3d Tensors named t8 and t9 of dimensions 2x2x4 and 2x3x4 respectively .\n","# Use 'matmul' method of Tensor class to multiply these two tensors. Do you get error? You should! Now, use 'permute method' defined in the Tensor class\n","# to permute the dimensions of Tensor t9 appropriately so that 'matmul' is possible now and the result is of dimensions 2x2x3.\n","t8 = torch.rand(2,2,4)\n","t9 = torch.rand(2,3,4)\n","t9_permuted = t9.permute(0, 2, 1)\n","res13 = torch.matmul(t8, t9_permuted)\n","print(\"\\n\\nres13.size(): \", res13.size())\n","\n","\n","# 14. Create an empty Tensor of dimension 2x3. Fill it with random values generated from the uniform distribution on the interval [11, 12] \n","#     using 'random_' method defined in the Tensor class.\n","# random_ generates numbers from the dsicrete uniform distribution over [from, to-1]\n","res14 = torch.empty(size=(2,3))\n","tmp = res14.random_(11, 12+1)\n","print(\"\\n\\nres14: \", res14)\n","\n","# 15. Create an empty Tensor of dimension 2x3. Fill it with random values generated from the continuous \n","# uniform distribution on the interval [0, 1] using 'uniform_' method defined in the Tensor class.\n","res15 = torch.empty((2,3)).uniform_(0,1)\n","print(\"\\n\\nres15: \", res15)\n","\n","\n","# 16. Create an empty Tensor of dimension 2x3. Fill it with zeros using 'zero_' method defined in the Tensor class.\n","res16 = torch.empty((2,3)).zero_()\n","print(\"\\n\\nres16: \", res16)\n","\n","\n","# 17. Using 'transpose' method defined in the Tensor class, transpose dimensions 1 and 2 in t1 defined in part 1.\n","t1 = t1.transpose_(1,2)\n","print(\"\\n\\n17. t1.size(): \", t1.size())\n","\n","\n","# 18. Sum along dimension 1 the Tensor t1 defined in part1. The ouput should have same number of dimensions as t1.\n","# Now, find mimimum along dimension 2 of the output. Now squeeze out the dimensions with size 1.\n","# 'sum', 'min' and 'squeeze' methods defined in the Tensor class will be useful. Further, read about 'unsqueeze' method defined in the Tensor class.\n","t1 = torch.full(size=(2,4,3), fill_value=3.)\n","res18_1 = t1.sum(dim=1, keepdim=True)\n","res18_2, res18_3 = res18_1.min(dim=2)\n","res18_3 = res18_1.squeeze_()\n","print(\"\\n\\nres18_1.size(): \", res18_1.size())\n","print(\"res18_2: \", res18_2)\n","print(\"res18_3.size(): \", res18_3.size())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["res1:  tensor([[[360., 360., 360.],\n","         [360., 360., 360.]],\n","\n","        [[360., 360., 360.],\n","         [360., 360., 360.]]])\n","res1.size():  torch.Size([2, 2, 3])\n","\n","\n","res2:  tensor([[[1, 1, 1, 1]],\n","\n","        [[1, 1, 1, 1]]])\n","res2.size():  torch.Size([2, 1, 4])\n","\n","\n","res3:  tensor([[[20., 20., 20., 20.],\n","         [20., 20., 20., 20.]],\n","\n","        [[20., 20., 20., 20.],\n","         [20., 20., 20., 20.]]])\n","t0:  tensor([[[20., 20., 20., 20.],\n","         [20., 20., 20., 20.]],\n","\n","        [[20., 20., 20., 20.],\n","         [20., 20., 20., 20.]]])\n","\n","\n","res4:  tensor([[[3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.]]])\n","t2:  tensor([[[3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.],\n","         [3., 3., 3.]]])\n","\n","\n","res5_1:  (tensor([[[3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.]]]), tensor([[[3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.]]]))\n","len(res5_1):  2\n","res5_2:  (tensor([[[3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.]]]), tensor([[[3., 3., 3.],\n","         [3., 3., 3.]],\n","\n","        [[3., 3., 3.],\n","         [3., 3., 3.]]]))\n","len(res5_2):  2\n","\n","\n","res6:  tensor([[[2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200]],\n","\n","        [[2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200],\n","         [2.9200, 2.9200, 2.9200]]])\n","\n","\n","7. t1.size():  torch.Size([2, 12])\n","\n","\n","res8_1.is_contiguous():   True\n","\n","\n","res9_1:  tensor(-2.7000)\n","type(res9_2):  <class 'float'>\n","\n","\n","res10:  tensor([])\n","\n","\n","res11:  [[[3. 3. 3.]\n","  [3. 3. 3.]\n","  [3. 3. 3.]\n","  [3. 3. 3.]]\n","\n"," [[3. 3. 3.]\n","  [3. 3. 3.]\n","  [3. 3. 3.]\n","  [3. 3. 3.]]]\n","\n","\n","12. t6:  tensor([3., 3., 3.])\n","t7:  tensor([3., 3., 3.])\n","res12:  tensor(27.)\n","\n","\n","res13.size():  torch.Size([2, 2, 3])\n","\n","\n","res14:  tensor([[11., 12., 12.],\n","        [11., 11., 12.]])\n","\n","\n","res15:  tensor([[0.9047, 0.7214, 0.0993],\n","        [0.1753, 0.3046, 0.2736]])\n","\n","\n","res16:  tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","\n","\n","17. t1.size():  torch.Size([2, 3, 4])\n","\n","\n","res18_1.size():  torch.Size([2, 3])\n","res18_2:  tensor([[12.],\n","        [12.]])\n","res18_3.size():  torch.Size([2, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gXvE1dGDLHoS","colab_type":"text"},"source":["To reshape a Tensor, we can use 'view' method defined in the Tensor class. Remind yourself that python stores multi dimensional arrays (Tensors) in row major order."]},{"cell_type":"code","metadata":{"id":"SxFrgVOZLXnN","colab_type":"code","outputId":"1737d301-1d2f-4b8c-da85-535d76e9bab5","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564046164859,"user_tz":-330,"elapsed":2373,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t = torch.rand((2, 3, 4, 5)) # totally 120 elements\n","t_1 = t.view((2, 3, 20)) \n","print(f'size of t_1: {t_1.size()}')\n","t_2 = t.view((2, -1)) # one argument can be -1; In this case the correct value is automatically calculated by the method\n","                      # Here it is calculated as 60\n","print(f'size of t_2: {t_2.size()}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["size of t_1: torch.Size([2, 3, 20])\n","size of t_2: torch.Size([2, 60])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8FasAsVqR5z6","colab_type":"text"},"source":["Let us look at another example."]},{"cell_type":"code","metadata":{"id":"FJprFHjyR4pi","colab_type":"code","outputId":"cf033407-8b10-4a27-9c70-c9cef36857fc","colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1564046164863,"user_tz":-330,"elapsed":2363,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","t_1 = t.view((3, 2))\n","print(f't: {t}')\n","print(f't_1: {t_1}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["t: tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","t_1: tensor([[1, 2],\n","        [3, 4],\n","        [5, 6]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NqUPdo1uSe98","colab_type":"text"},"source":["In the above example, if you had intended to perform transpose of the Tensor,  you have failed. t_1 is not the transpose of t though it has dimensions 3x2, the dimensions transpose of t will have. What has happened? To understand the scenario here, note that in memory t is stored as 1, 2, 3, 4, 5, 6 in row major order. Total number of elements is 6. When you asked for a new view with shape 3x2, 'view' method knew that the new view also has 6 elements that is compatible with the number of elements in the original tensor t. Now 'view' method provides the new view by simply doing a linear scan of the storage of t, considering first 2 elements as elements of 1st row, next 2 elements as elements of 2nd row and last two elements as elements of 3rd row. \n","\n","An important point to note with 'view' method is that it does not change the layout of the Tensor in the memory but only provides a new view. This is unlike transposing two dimensions of a Tensor which will change the layout of the transposed tensor in memory. Both the new view of the Tensor and the transposed Tensor share the memory with the original Tensor. Any change to one of them will affect the other. An example is shown below."]},{"cell_type":"code","metadata":{"id":"iiPWa-n7NjIp","colab_type":"code","outputId":"6b0caf77-b588-416f-9ef5-6a2020b03004","colab":{"base_uri":"https://localhost:8080/","height":456},"executionInfo":{"status":"ok","timestamp":1564046164864,"user_tz":-330,"elapsed":2350,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n","print(f't is stored in memory as: {t.take(index = torch.arange(0, t.numel()))}\\n') # print the linear way it is stored in memory\n","t_transposed = t.transpose(dim0 = 0, dim1 = 1)\n","print(f't_transposed is stored in memory as: {t_transposed.take(index = torch.arange(0, t_transposed.numel()))}\\n') # print the linear way it is stored in memory\n","\n","t_view = t.view((4, 3))\n","print(f't_view is stored in memory as: {t_view.take(index = torch.arange(0, t_view.numel()))}\\n') # print the linear way it is stored in memory\n","\n","t_transposed[0, 1] = -2\n","print(f't_transposed after modifying 0th row 1st column element of t_transposed: {t_transposed}\\n')\n","print(f't after modifying 0th row 1st column element of t_transposed: {t}\\n') # t also is modified, not the 0th row 1st col element but 1st row 0th col element\n","\n","t_view[0, 1] = -2\n","print(f't_view after modifying 0th row 1st column element of t_view: {t_view}\\n')\n","print(f't after modifying 0th row 1st column element of t_view: {t}\\n') # t also is modified at exactly the same position\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["t is stored in memory as: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","\n","t_transposed is stored in memory as: tensor([ 1,  5,  9,  2,  6, 10,  3,  7, 11,  4,  8, 12])\n","\n","t_view is stored in memory as: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n","\n","t_transposed after modifying 0th row 1st column element of t_transposed: tensor([[ 1, -2,  9],\n","        [ 2,  6, 10],\n","        [ 3,  7, 11],\n","        [ 4,  8, 12]])\n","\n","t after modifying 0th row 1st column element of t_transposed: tensor([[ 1,  2,  3,  4],\n","        [-2,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n","t_view after modifying 0th row 1st column element of t_view: tensor([[ 1, -2,  3],\n","        [ 4, -2,  6],\n","        [ 7,  8,  9],\n","        [10, 11, 12]])\n","\n","t after modifying 0th row 1st column element of t_view: tensor([[ 1, -2,  3,  4],\n","        [-2,  6,  7,  8],\n","        [ 9, 10, 11, 12]])\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lyutqSsLWHqs","colab_type":"text"},"source":["Another important method defined in Tensor class is 'to' method. It can be used to create a copy of the exisiting Tensor with different data type. It can also be used to copy a tensor from cpu memory to gpu memory and vice-versa. The example below shows how 'to' method is useful for creating a copy of the Tensor with different datatype. Further examples later will show its usefulness for copying Tensor to gpu memory/cpu memory."]},{"cell_type":"code","metadata":{"id":"retxXdGjXlxo","colab_type":"code","outputId":"8143be1f-1581-4df9-eb5a-f375da85cc00","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"ok","timestamp":1564046164866,"user_tz":-330,"elapsed":2335,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t = torch.eye(3)\n","print(f't: {t}')\n","print(f'Datatype of t: {t.dtype}\\n')\n","\n","t_1 = t.to(dtype = torch.int16)\n","print(f't_1: {t_1}')\n","print(f'Datatype of t_1: {t_1.dtype}\\n')\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["t: tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","Datatype of t: torch.float32\n","\n","t_1: tensor([[1, 0, 0],\n","        [0, 1, 0],\n","        [0, 0, 1]], dtype=torch.int16)\n","Datatype of t_1: torch.int16\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ecesgKe0Yb7P","colab_type":"text"},"source":["'is_cuda' method in the Tensor class will check if the Tensor resides on GPU. For eg, see the code below."]},{"cell_type":"code","metadata":{"id":"Fv-Co0hsusSz","colab_type":"code","outputId":"dab5a7c5-ade2-44ce-e028-b04d31a72310","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1564046164867,"user_tz":-330,"elapsed":2322,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t = torch.ones(size = (2, 3))\n","t.is_cuda # currently t is residing on cpu"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"88a7Xy4UvC7R","colab_type":"text"},"source":["Currently Tensor t is residing in cpu. To copy it to gpu memory, we can do the following:"]},{"cell_type":"code","metadata":{"id":"CzQVLrzmYuuP","colab_type":"code","outputId":"4835aa37-7bbb-4e4c-d452-f4d491149aaa","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564046167578,"user_tz":-330,"elapsed":5019,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t_gpu = t.to(device = 'cuda')\n","print(f'Is t_gpu residing in gpu?\\t{t_gpu.is_cuda}')\n","print(f't_gpu is residing in the device: {t_gpu.device}')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Is t_gpu residing in gpu?\tTrue\n","t_gpu is residing in the device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KoRN6mcTZCDG","colab_type":"text"},"source":["To copy the Tensor back to cpu, we can do the following."]},{"cell_type":"code","metadata":{"id":"YAX_8YWaZIYf","colab_type":"code","outputId":"371a913f-9210-49d6-e7aa-b8fc4e246438","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564046167584,"user_tz":-330,"elapsed":5010,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t_cpu = t.to(device = 'cpu')\n","print(f'Is t_cpu residing in gpu?\\t{t_cpu.is_cuda}')\n","print(f't_cpu is residing in the device: {t_cpu.device}')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Is t_cpu residing in gpu?\tFalse\n","t_cpu is residing in the device: cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gsr4AU-FZU3u","colab_type":"text"},"source":["We can also use 'cuda' method defined in the Tensor class to copy the Tensor object to gpu memory and 'cpu' method to copy the Tensor object to cpu memory. See examples below."]},{"cell_type":"code","metadata":{"id":"Bps1Kfnmuyl7","colab_type":"code","outputId":"47219d2a-ff7a-46ce-e7cc-4237d81693f3","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564046167588,"user_tz":-330,"elapsed":5002,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t_gpu = t_cpu.cuda()\n","print(f'Is t_gpu residing in gpu?\\t{t_gpu.is_cuda}')\n","print(f't_gpu is residing in the device: {t_gpu.device}')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Is t_gpu residing in gpu?\tTrue\n","t_gpu is residing in the device: cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cTNW0YcsOABO","colab_type":"code","outputId":"266860ba-5332-4b13-8d59-9cc4a0dc1ffd","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1564046167592,"user_tz":-330,"elapsed":4995,"user":{"displayName":"ANKIT ANAND","photoUrl":"https://lh4.googleusercontent.com/-ugSx6T4agSM/AAAAAAAAAAI/AAAAAAAAAG4/7gkbmUPGhZU/s64/photo.jpg","userId":"03017525763706113925"}}},"source":["t_cpu = t_gpu.cpu()\n","print(f'Is t_cpu resing in gpu?\\t{t_cpu.is_cuda}')\n","print(f't_cpu is residing in the device: {t_cpu.device}')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Is t_cpu resing in gpu?\tFalse\n","t_cpu is residing in the device: cpu\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yjvyGevuax3H","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}