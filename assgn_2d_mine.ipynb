{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import HTML, display\n",
    "from assign2_utils import load_train_data, load_test_data, flatten\n",
    "from assign2_utils1 import Layers, Linear, NonLinear, Dropout, args\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augumentation\n",
    "Generally, deep learning models thrive on data. More the data (in the order of hundreds of millions), better can the models perform. It is easy to acquire large data in unsupervised scenario, for eg by scrapping from web. However, in supervised scenario, acquiring large amount of data along with labels is mostly infeasible. Labels or groundtruth is generally marked by human experts and expecting them to committ to mark labels for huge data is not always possible. In such scenario, data augumentation will be very helpful. Data augumentation are set of techniques that can artificially bloat up the size of the data. For eg, horizontally flipping a cat image will still be a valid cat image. We can scale the cat image or rotate the cat image, say within -15 degrees to 15 degrees, to obtain more valid cat images. Some examples of artificially obtainined valid cat images is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='orig_img1.png'></td><td><img src='rot_img.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='orig_img2.png'></td><td><img src='hf_img.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<table><tr><td><img src='orig_img1.png'></td><td><img src='rot_img.png'></td></tr></table>\"))\n",
    "display(HTML(\"<table><tr><td><img src='orig_img2.png'></td><td><img src='hf_img.png'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we want to horizontally flip the images and bloat up the training size, do we offline flip all the training images? Of course this will double the size. However, there is another idea. During every epoch, we can  randomly flip or not flip the image. Suppose we have 10 images in the training set. Offline flipping will double the size to 20 images that can be used during training across all epochs. If we assume an image may or may not be flipped (say with Bernoulli probability p) during an epoch, then the set of images seen during an epoch is a sample from set of 1024 sets of 10 images (with binomial probability) which is much larger than the 20 images obtained by offline flipping. This is the strategy we will follow wherever feasible. I say wherever feasible because the aforementioned scenario may not always be possible. For eg, let's say the size of the input image should be 128 x 128 but what is available is of size 144 x 144. So we can augument the data by cropping 128 x 128 from 144 x 144. If we fix our croppings to be from top left, top right, centre, bottom left and bottom right, then we will get 5 crops of size 128 x 128 each from every 144 x 144 image. There is no randomness here. Our data size will increase by factor of 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the class definition for performing random horizontal flip. It is a callable class. For dealing with images, we will use PIL package. So, in the below code it is assumed that the input 'img' in the '__call__' method is a PIL object (i.e image opened by PIL package). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip(object): #inherits from python's object class\n",
    "    def __init__(self, prob = 0.5):\n",
    "        self.prob = prob\n",
    "        \n",
    "    def __call__(self, img): # a callable class\n",
    "        if np.random.rand() < self.prob:\n",
    "            return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "You will write a class with name __RandomRotate__ to randomly rotate the given image. Obviously, angle to rotate the image (in degrees) and the probability to rotate the image needs to be specified to initialize an object of __RandomRotate__ type. It should be a callable class. Angle to rotate the image can be either specified as a single number like 5 or as a list or tuple of length 2 like [-3, 5] or (-3, 5). In the former case, an angle in degrees will be randomly chosen between -5 and 5. In the latter case, an angle in degrees will be randomly chosen between -3 and 5. Note that there are two levels of randomness involved here. First, whether to randomly rotate the image or not. Second, if image is to be randomly rotated, then choosing an angle randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class RandomRotate\n"
     ]
    }
   ],
   "source": [
    "# Complete the class RandomRotate here\n",
    "print(\"class RandomRotate\")\n",
    "\n",
    "class RandomRotate(object):\n",
    "    def __init__(self, prob=0.6, angle=[-5, 5]):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "        if isinstance(angle, list):\n",
    "            self.low = angle[0]\n",
    "            self.high = angle[1] + 1\n",
    "        if isinstance(angle, tuple):\n",
    "            self.low = angle[0]\n",
    "            self.high = angle[1] + 1\n",
    "        if isinstance(angle, int):\n",
    "            if angle < 0:\n",
    "                self.low = angle\n",
    "                self.high = -1 * angle\n",
    "            else:\n",
    "                self.low = -1 * angle\n",
    "                self.high = angle\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        if np.random.rand() < self.prob:\n",
    "            return img.rotate(np.random.randint(low=self.low, high=self.high))\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since multiple transforms can be applied to an image sequentially, we will write a callable class called __Compose__ that composes or applies the sequence of transforms sequentially on the input. The definition of the class is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):# transforms is a sequence of tranforms; \n",
    "                                # For eg, transforms = [RandomHorizontalFlip(0.6), RandomRotate(0.5, [-5, 6])]\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def  __call__(self, img): # img is a PIL object; i.e image opened by PIL package\n",
    "        for t in self.transforms:\n",
    "            img = t(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following model along with the train, test and main functions given below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()        \n",
    "        self.fc1 = Linear(in_features, 20)\n",
    "        self.relu1 = NonLinear('ReLU')\n",
    "        self.fc2 = Linear(20, 7)\n",
    "        self.relu2 = NonLinear('ReLU')\n",
    "        self.fc3 = Linear(7, 5)\n",
    "        self.relu3 = NonLinear('ReLU')  \n",
    "        self.fc4 = Linear(5, 1)        \n",
    "        self.sigmoid = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)          \n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.sigmoid.backward(d_output)        \n",
    "        dx = self.fc4.backward(dz)          \n",
    "        dz = self.relu3.backward(dx)\n",
    "        dx = self.fc3.backward(dz)\n",
    "        dz = self.relu2.backward(dx)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dz = self.relu1.backward(dx)\n",
    "        dx = self.fc1.backward(dz)  \n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "        self.fc3.update_params(learning_rate)\n",
    "        self.fc4.update_params(learning_rate)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, transforms = None):   \n",
    "    for i in range(args.miter):\n",
    "        x_transformed = np.empty_like(x, dtype = np.uint8)\n",
    "        if transforms:\n",
    "            for j in range(x.shape[0]):\n",
    "                if y[j]:                    \n",
    "                    img = Image.fromarray(x[j].astype(np.uint8))\n",
    "                    x_transformed[j] = np.asarray(transforms(img))\n",
    "                else:\n",
    "                    x_transformed[j] = x[j]\n",
    "        else:\n",
    "            x_transformed = x\n",
    "        \n",
    "        x_transformed = flatten(x_transformed)\n",
    "        x_transformed = x_transformed / 255.\n",
    "        output = model(x_transformed) # model is a callable object with call to its forward method.\n",
    "                          # we could also have written the rhs as model.forward(x)\n",
    "        del x_transformed\n",
    "        L = model.loss(output, y)\n",
    "        model.backward(output, y)\n",
    "        model.update_params(args.alpha)\n",
    "        if not i%args.print_freq: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "                \n",
    "def test_model(model, x, y):\n",
    "    predictions = model(x)\n",
    "    predictions[predictions > 0.5] = 1\n",
    "    predictions[predictions <= 0.5] = 0\n",
    "    acc = np.mean(predictions == y)\n",
    "    acc = np.asscalar(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "Loss at iteration 0:\t0.6912\n",
      "Loss at iteration 100:\t0.6302\n",
      "Loss at iteration 200:\t0.5829\n",
      "Loss at iteration 300:\t0.5422\n",
      "Loss at iteration 400:\t0.5213\n",
      "Loss at iteration 500:\t0.4627\n",
      "Loss at iteration 600:\t0.5156\n",
      "Loss at iteration 700:\t0.2571\n",
      "Loss at iteration 800:\t0.2610\n",
      "Loss at iteration 900:\t0.3994\n",
      "Loss at iteration 1000:\t0.1316\n",
      "Loss at iteration 1100:\t0.0970\n",
      "Loss at iteration 1200:\t0.1901\n",
      "Loss at iteration 1300:\t0.0597\n",
      "Loss at iteration 1400:\t0.6062\n",
      "Loss at iteration 1500:\t0.1154\n",
      "Loss at iteration 1600:\t0.0320\n",
      "Loss at iteration 1700:\t0.0201\n",
      "Loss at iteration 1800:\t0.0180\n",
      "Loss at iteration 1900:\t0.0144\n",
      "test accuracy: 84.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    print(x.shape)    \n",
    "    \n",
    "    transforms = Compose([RandomHorizontalFlip(), RandomRotate(angle = [-5, 5])])\n",
    "    \n",
    "    #instantiate model\n",
    "    my_model = Model(64*64*3)\n",
    "    # train the model\n",
    "    train(my_model, x, y, transforms)\n",
    "    \n",
    "    # test the model\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    #print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return my_model\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    my_model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
