{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient problem and Weight Initialization \n",
    "\n",
    "Well. You have implemented a simple logistic regression model and a two layer fully connected shallow network for cat vs non-cat classification. You would have found that the shallow network does not significantly outperform logistic regression. Further, you might conclude based on the shallow net's performance of close to 100% training accuracy and around 72% test accuracy that it is heavily overfitting. So you would have tried L1/L2 regularization or dropout regularization. To your surprise, you would have found that regularization (L1, L2 or dropout) did not have any effect or in fact exhibited a negative effect by pulling down model's generalization ability.\n",
    "\n",
    "Is it a surprise?? What was the performance of logistic regression? Is it not close to 100% training accuracy and around 70% test accuracy? So, is logistic regression overfitting? In fact it is the simplest model with smallest number of parameters. Zeroing out more parameters will increase the bias of the model thereby resulting in much lower training accuracy as well as test accuracy. Test this case for yourself by randomly setting few parameters in logistic regression model to zero prior to training or, even during training like dropout. Make sure the activations are scaled as in dropout if you are trying the latter case.\n",
    "\n",
    "This understanding suggests that model requires more capacity to generalize better. That is, we need to try a deeper network to improve generalization capability. Subsequently we can enforce regularization to constrain weights with smaller norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "i. Build a model based on the design in assgn_2b for the following architecture:\n",
    "        \n",
    "        I/p---->Linear1---->Sigmoid----->Linear2---->Sigmoid---->Linear3---->Sigmoid---->Linear4---->Sigmoid\n",
    "   where: \n",
    "       \n",
    "       Linear1 has 64*64*3=12288 incoming connections and 30 outgoing connections\n",
    "       Linear2 has 30 incoming connections and 30 outgoing connections\n",
    "       Linear3 has 30 incoming  connections and 30 outgoing connections \n",
    "       Linear4 has 30 incoming  connections and 1 outgoing connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse # for command-line parsing\n",
    "import matplotlib # for plotting\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from abc import ABC, abstractmethod # for crating abstract classes\n",
    "\n",
    "from assign2_utils import load_train_data, load_test_data, flatten, df \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layers(ABC): # Layers inherits from ABC.The class ABC in abc module is required to make a class abstract. \n",
    "                   # Native python does not support abstract classes\n",
    "    def __init__(self): \n",
    "        super().__init__() # calls the constructor of the super class ABC\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, *x): # should be overridden by every class that inherits from this class\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, *da): # should be overridden by every class that inherits from this class\n",
    "        pass    \n",
    "    \n",
    "    def update_params(self, learning_rate = None): # should be overridden by those inheriting classes that have\n",
    "                                                   # parameters to be learnt\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, *x): # makes the inheriting classes callable, calling the forward method of the class\n",
    "        return self.forward(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layers): # inherits from abstract class Layers\n",
    "    def __init__(self, in_features, out_features, bias = True, regularization = None): # constructor\n",
    "        super().__init__()\n",
    "        self.in_features = in_features # initialize number of incoming features\n",
    "        self.out_features = out_features # initialize number of outgoing features\n",
    "        self.weight = np.random.randn(out_features, in_features) * .01 # initialize weight matrix\n",
    "        if bias:\n",
    "            self.bias = np.zeros((out_features, 1)) # initialize bias if required\n",
    "        else:\n",
    "            self.bias = None # if bias not required, set it to None\n",
    "        self.regularization = regularization # initialize regularization\n",
    "        if self.regularization is not None: \n",
    "            self.reg_penalty = 0 # if valid regularization at this layer, set the regularization penalty\n",
    "                                 # at this layer initially to zero\n",
    "            \n",
    "        self.dw = np.empty_like(self.weight) # intialize dw to empty\n",
    "        self.db = np.empty_like(self.bias) if self.bias is not None else None \n",
    "                                            # initialize db to empty if bias == True\n",
    "        \n",
    "    def forward(self, x): # forward method overridden; x is the incoming activation. \n",
    "                          # shape of x is (num of activations, num of samples)\n",
    "            \n",
    "#         print(\"Linear Forward\")\n",
    "        m = x.shape[1]    # number of training examples\n",
    "        self.x = x # x is required for backward. We don't need a separate cache. We can store it in the object.\n",
    "        \n",
    "        output = self.weight @ x # computation of the linear part Wx                                 \n",
    "        if self.bias is not None:            \n",
    "            output += self.bias # add to Wx bias if bias == True\n",
    "                                # Note that we don't apply non-linearity here as this layer computes only Wx+b\n",
    "        \n",
    "        #update regularization penalty at this layer\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L2':\n",
    "            self.reg_penalty = args.lamda/(2*m) * np.sum(self.weight**2)\n",
    "        elif self.regularization == 'L1':\n",
    "            self.reg_penalty = args.lamda/m * np.sum(np.abs(self.weight))\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        return output # return forward output as the next layer in the model will require it     \n",
    "    \n",
    "    # Backward of this layer will receive dz. Note that at this layer z = Wx+b. So backward will compute\n",
    "    # dw, db and dx. To compute dW, x is required. That's why x was stored in forward. To compute dx, W is \n",
    "    # required. This is already available in self.weight\n",
    "    \n",
    "    def backward(self, dz): # backward method overridden\n",
    "        m = dz.shape[1]     # number of training examples\n",
    "#         print(\"Linear Backward\")\n",
    "        self.dw = dz @ self.x.T # compute dw\n",
    "        \n",
    "        # add derivative of regularization penalty at this layer w.r.to w\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L1':\n",
    "            signw = np.sign(self.weight)\n",
    "            signw[signw == 0] = 1\n",
    "            self.dw += args.lamda/m * signw\n",
    "        elif self.regularization == 'L2':\n",
    "            self.dw += args.lamda/m * self.weight\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        if self.bias is not None: # compute db if bias == True\n",
    "            self.db = np.sum(dz, axis = 1, keepdims = True) \n",
    "            \n",
    "        dx = self.weight.T @ dz  # compute dx     \n",
    "        return dx # we only return dx as this this required for chain rule in the next layer \n",
    "                  # in backward direction. dw and db are kept available in this object which will directly\n",
    "                  # be used by update_params for updating weights and biases\n",
    "    \n",
    "    # update parameters in this layer \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.weight -= learning_rate*self.dw\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate*self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(Layers):\n",
    "    def __init__(self, fname='ReLU'):\n",
    "        super().__init__()\n",
    "        self.fname = fname\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print(\"NonLinear Forward\")\n",
    "        self.x = x\n",
    "        if self.fname == 'ReLU':\n",
    "            return np.maximum(x, 0)\n",
    "        elif self.fname == 'Sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fname == 'Tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError('Unknown non-linear function error')\n",
    "    \n",
    "    def backward(self, dx):    # implemented instead of using df(...)\n",
    "#         print(\"NonLinear Backward\")\n",
    "        if self.fname == 'ReLU':\n",
    "            return dx * (self.x > 0)\n",
    "        elif self.fname == 'Sigmoid':\n",
    "            sigmoid_x = 1 / (1 + np.exp(-self.x))\n",
    "            return dx * sigmoid_x * (1 - sigmoid_x)\n",
    "        elif self.fname == 'Tanh':\n",
    "            return dx * (1 - np.tanh(self.x)**2)\n",
    "        else:\n",
    "            raise ValueError('Unknown non-linear function error')\n",
    "    \n",
    "    def update_params(self, learning_rate=None):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layers):\n",
    "    def __init__(self, keep_prob = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "    \n",
    "    def forward(self, x, train=True):\n",
    "        if train:    \n",
    "            d = np.random.rand(*x.shape)\n",
    "            d = (d < self.keep_prob)\n",
    "            self.d = d\n",
    "            x = x * d\n",
    "            x = x / self.keep_prob\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        return dx * self.d\n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        # self.fc1 = Linear(in_features, 32, regularization = 'L2')\n",
    "        self.fc1 = Linear(in_features, 30)\n",
    "        self.sigmoid1 = NonLinear('Sigmoid')\n",
    "        self.fc2 = Linear(30, 30)\n",
    "        self.sigmoid2 = NonLinear('Sigmoid')\n",
    "        self.fc3 = Linear(30, 30)\n",
    "        self.sigmoid3 = NonLinear('Sigmoid')\n",
    "        self.fc4 = Linear(30, 1)\n",
    "        self.sigmoid4 = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.sigmoid4.backward(d_output)\n",
    "        dx = self.fc4.backward(dz)\n",
    "        dz = self.sigmoid3.backward(dx)\n",
    "        dx = self.fc3.backward(dz)\n",
    "        dz = self.sigmoid2.backward(dx)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dz = self.sigmoid1.backward(dx)\n",
    "        dx = self.fc1.backward(dz)\n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "        self.fc3.update_params(learning_rate)\n",
    "        self.fc4.update_params(learning_rate)\n",
    "        \n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "parser.add_argument('--print_freq', metavar='N', type=int, default=300, help='print model loss every print_freq iterations')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 2000 --alpha .008'.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y):\n",
    "    for i in range(args.miter):\n",
    "        output = model(x) # model is a callable object with call to its forward method.\n",
    "                          # we could also have written the rhs as model.forward(x)\n",
    "        L = model.loss(output, y)\n",
    "        model.backward(output, y)\n",
    "        model.update_params(args.alpha)\n",
    "        if not i%args.print_freq: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "                \n",
    "def test_model(model, x, y):\n",
    "    predictions = model(x, train=False)\n",
    "    predictions[predictions > 0.5] = 1\n",
    "    predictions[predictions <= 0.5] = 0\n",
    "    acc = np.mean(predictions == y)\n",
    "    acc = np.asscalar(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6953\n",
      "Loss at iteration 300:\t0.6440\n",
      "Loss at iteration 600:\t0.6440\n",
      "Loss at iteration 900:\t0.6440\n",
      "Loss at iteration 1200:\t0.6440\n",
      "Loss at iteration 1500:\t0.6440\n",
      "Loss at iteration 1800:\t0.6440\n",
      "train accuracy: 65.55%\n",
      "test accuracy: 34.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # Instantiate the model\n",
    "    my_model = Model(x.shape[0])\n",
    "    \n",
    "    # train the model\n",
    "    train(my_model, x, y)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "ii. Build a model based on the design in assgn_2b for the following architecture:\n",
    "        \n",
    "        I/p---->Linear1---->Sigmoid----->Linear2---->Sigmoid---->Linear3----Sigmoid---->Linear4---->Sigmoid--->                                                                                         Linear5---->Sigmoid      \n",
    "    where: \n",
    "    \n",
    "       Linear1 has 64*64*3=12288 incoming connections and 30 outgoing connections\n",
    "       Linear2 has 30 incoming connections and 30 outgoing connections\n",
    "       Linear3 has 30 incoming  connections and 30 outgoing connections \n",
    "       Linear4 has 30 incoming  connections and 30 outgoing connections\n",
    "       Linear5 has 30 incoming  connections and 1 outgoing connection\n",
    "\n",
    "You can set learning_rate to 0.008 and run for around 2000 iterations.\n",
    "What is the training and test accuracies in both the cases? Are these deeper models performing better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        # self.fc1 = Linear(in_features, 32, regularization = 'L2')\n",
    "        self.fc1 = Linear(in_features, 30)\n",
    "        self.sigmoid1 = NonLinear('Sigmoid')\n",
    "        self.fc2 = Linear(30, 30)\n",
    "        self.sigmoid2 = NonLinear('Sigmoid')\n",
    "        self.fc3 = Linear(30, 30)\n",
    "        self.sigmoid3 = NonLinear('Sigmoid')\n",
    "        self.fc4 = Linear(30, 30)\n",
    "        self.sigmoid4 = NonLinear('Sigmoid')\n",
    "        self.fc5 = Linear(30, 1)\n",
    "        self.sigmoid5 = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid5(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.sigmoid5.backward(d_output)\n",
    "        dx = self.fc5.backward(dz)\n",
    "        dz = self.sigmoid4.backward(dx)\n",
    "        dx = self.fc4.backward(dz)\n",
    "        dz = self.sigmoid3.backward(dx)\n",
    "        dx = self.fc3.backward(dz)\n",
    "        dz = self.sigmoid2.backward(dx)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dz = self.sigmoid1.backward(dx)\n",
    "        dx = self.fc1.backward(dz)\n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "        self.fc3.update_params(learning_rate)\n",
    "        self.fc4.update_params(learning_rate)\n",
    "        self.fc5.update_params(learning_rate)\n",
    "        \n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "parser.add_argument('--print_freq', metavar='N', type=int, default=300, help='print model loss every print_freq iterations')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 2000 --alpha .008'.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6939\n",
      "Loss at iteration 300:\t0.6440\n",
      "Loss at iteration 600:\t0.6440\n",
      "Loss at iteration 900:\t0.6440\n",
      "Loss at iteration 1200:\t0.6440\n",
      "Loss at iteration 1500:\t0.6440\n",
      "Loss at iteration 1800:\t0.6440\n",
      "train accuracy: 65.55%\n",
      "test accuracy: 34.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # Instantiate the model\n",
    "    my_model = Model(x.shape[0])\n",
    "    \n",
    "    # train the model\n",
    "    train(my_model, x, y)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradient Problem\n",
    "To understand this, let's look at the plot shown below. This plot displays the size of dw, ie $||dw||_{2}$ at every neuron at hidden layers 1, 2 and 3 of the model that is solution to the first exercise above. \n",
    "<img src='dw_sigmoid_4layers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You find that, compared to layer3, size of $dw$ at the earlier layers (layers 1 and 2), particularly layer 1, have mostly very neglible value implying that $w$ at earlier layers update minimally compared to later layers. In fact, across training, you find from the above plot that most of the neurons from layer 1 update very minimally and so learn very slowly. Also, the magnitude of size of dw has come down by order of $10^3$ across training. Ok, hold this in your mind and let's look at the plots for solution of exercise 2 given above. This model is deeper than the model corresponding to exercise 1. \n",
    "<img src='dw_sigmoid_5layers.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see a similar behaviour but with more severity. The magnitudes have come down by an order of $10^5$. You must have observed that, when you trained your solutions to both exercises given above, training almost loss plateaus after few iterations and does not significantly decrease over iterations. To summarise, we find that earlier layers in a deeper network learns very slow and the rate of slowness in learning increases with depth. The figure below depicts the speed of learning w.r.to number of epochs across all hidden layers in the solution to exercise 2. We can see that earlier layers 1 and 2 do not learn right from the beginning. Later layers also stop updating significantly after around 400 epochs. Is this behaviour random or expected??? It cannot be random since this happened for your solutions and all your classmates solutions as well!! So why did this happen? \n",
    "<img src='speed_learning.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of loss w.r.to $w$, $dW$, decreases in magnitude significantly for earlier layers i.e the gradient is vanishing. You might wonder vanishing gradient is favourable since it seems to mean that optimum weights have been arrived at (from calculus, when derivative of f is zero at x , then x is a stationary point). But remember that weights have been initialized randomly and so it is highly unlikely that these weights will be the optimimum ones. These weights at earlier layers hardly get updated even though at later layers significant updates happen. Random initialization mean that input features are randomly combined at layer 1 leading to a significant likelihood of loss of input information. So, even if later layers learn, they will still underperform in recognizing input as they do not have enough information about input right from the beginning. so, why is the gradient vanishing fast for earlier layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this let's consider the $dW$ equation at layer $l$ in backpropagation:\n",
    "<br><br>\n",
    "&emsp;$dW^{[l]}$ = $dz^{[l]}$.${a^{[l-1]}}^T$ where . is matrix multiplication and T denotes transpose\n",
    "<br><br>\n",
    "Here, \n",
    "<br><br>\n",
    "&emsp;$da^{[l]}$ = ${dW^{[l+1]}}^T$.$dz^{[l+1]}$ for $1<=l<L$ and $da^{[L]}$ = $\\frac{1.0}{m}(a^{[L]}-y)*\\frac{1.0}{a^{[L]}*(1-a^{[L]})}$\n",
    "<br><br>\n",
    "Specifically, for the model in exercise 2, we have, \n",
    "<br><br>\n",
    "$dW^{[5]}$ = $dz^{[5]}$.${a^{[4]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $da^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[5]})}.{a^{[4]}}^T$\n",
    "<br><br>\n",
    "$dW^{[4]}$ = $dz^{[4]}$.${a^{[3]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $da^{[4]}*\\sigma ^{'}(z^{[4]}).{a^{[3]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = ${dW^{[5]}}^T.dz^{[5]}*\\sigma ^{'}(z^{[4]}).{a^{[3]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $({da^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[5]})}.{a^{[4]}}^T})^T.dz^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[4]})}.{a^{[3]}}^T$\n",
    "<br><br>\n",
    "$dW^{[3]}$ = $dz^{[3]}$.${a^{[2]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $da^{[3]}*\\sigma ^{'}(z^{[3]}).{a^{[2]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = ${dW^{[4]}}^T.dz^{[4]}*\\sigma ^{'}(z^{[3]}).{a^{[2]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $({({da^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[5]})}.{a^{[4]}}^T})^T.dz^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[4]})}.{a^{[3]}}^T})^T.dz^{[4]}*\\color{blue}{\\sigma ^{'}(z^{[3]})}.{a^{[2]}}^T$\n",
    "<br><br>\n",
    "$dW^{[2]}$ = $dz^{[2]}$.${a^{[1]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $da^{[2]}*\\sigma ^{'}(z^{[2]}).{a^{[1]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = ${dW^{[3]}}^T.dz^{[3]}*\\sigma ^{'}(z^{[2]}).{a^{[1]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $({({({da^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[5]})}.{a^{[4]}}^T})^T.dz^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[4]})}.{a^{[3]}}^T})^T.dz^{[4]}*\\color{blue}{\\sigma ^{'}(z^{[3]})}.{a^{[2]}}^T})^T.dz^{[3]}*\\color{blue}{\\sigma ^{'}(z^{[2]})}.{a^{[1]}}^T$\n",
    "<br><br>\n",
    "$dW^{[1]}$ = $dz^{[1]}$.${a^{[0]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $da^{[1]}*\\sigma ^{'}(z^{[1]}).{a^{[0]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = ${dW^{[2]}}^T.dz^{[2]}*\\sigma ^{'}(z^{[2]}).{a^{[0]}}^T$\n",
    "<br>\n",
    "&emsp;&nbsp; &nbsp; &nbsp; = $({({({({da^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[5]})}.{a^{[4]}}^T})^T.dz^{[5]}*\\color{blue}{\\sigma ^{'}(z^{[4]})}.{a^{[3]}}^T})^T.dz^{[4]}*\\color{blue}{\\sigma ^{'}(z^{[3]})}.{a^{[2]}}^T})^T.dz^{[3]}*\\color{blue}{\\sigma ^{'}(z^{[2]})}.{a^{[1]}}^T})^T.dz^{[2]}*\\color{blue}{\\sigma ^{'}(z^{[2]})}.{a^{[0]}}^T$\n",
    "\n",
    "From the above set of equations we can observe that as $i$ decreases from 5 to 1, the number of $\\sigma^{'}$ terms increases from 1 to 5 in $dW^{[i]}$. Note that $\\sigma^{'}$ is derivative of sigmoid function. It has maximum value of $0.25$. So, as $i$ decreases from 5 to 1, computation of $dW^{[i]}$ involves multiplication of increasing number of terms bounded by $0.25$. This will significantly bring down the magnitude of $dW^{[i]}$ as $i$ decreases from 5 to 1. This is a primary cause of vanishing gradients. This will hinder the update of $W$ at earlier layers thereby slowing the learning process. Sigmoid activation is the main culprit here. In fact, any activation that has similar behaviour to sigmoid will suffer from this problem. For eg, tanh also will exhibit this behaviour though tanh is slightly better as its outputs are centred around 0 unlike in sigmoid where outputs are centred around 0.5. Derivative of tanh has maximum value of 1.\n",
    "\n",
    "What about ReLU?? Let's try it. Below is the plot of size of $dW$ for all neurons across all layers w.r.to solution of exercise1 where ReLU is used everywhere except at the output layer. At output layer, sigmoid is used. \n",
    "<img src='dw_relu_4layers.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar plot for solution to exercise 2 with ReLU activation at all layers except at output layer where sigmoid is used is shown below. Also the speed of learning is depicted in this scenario.\n",
    "<img src='dw_relu_5layers.png'>\n",
    "<img src='speed_learning_relu.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be an improvement with ReLU usage since many neurons from layers 1 and 2 are visible in the plots. But note that the magnitudes of size of $dW$ is again negligible, in fact across layers, thereby disrupting the learning process. Why this behaviour even when ReLU is used??\n",
    "\n",
    "Remember the random initialization we have used for the layers: np.random.randn($n_{in}$, $n_{out}$)$*$0.01 where $n_{in}$ and $n_{out}$ are number of incoming and outgoing connections. Note that ReLU passes forward anything positive as it is and zeros out otherwise. Derivative of ReLU is constant $1$ for positive input and zero otherwise. We had argued in the introduction to this course that multiplication by 0.01 will prevent large weights and so avoid saturation of activation functions like sigmoid which in turn will produce non-zero gradients. But multiplication by 0.01 can make small values smaller and other values small relatively. This will cause inputs to ReLU to be small. Hence outputs from ReLU will be either small or zero. Since $dW^{[i]}$ depends on $a^{[i-1]}$ which are outputs from ReLU except at last layer, size of $dW^{[i]}$ will become small. This is possibly a reason for degradation in speed of learning even when ReLU is used.\n",
    "\n",
    "So, apart from activation, initialization can also influence vanishing gradient problem. We will look at better initialization strategies now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier/Glorot Initialization\n",
    "To make sure that weights are appropriately randomly initialized, Xavier Glorot opined that the variance in the input and output should be equal. For instance, consider the linear neuron scenario:\n",
    "$z$ = $\\sum_{i=1}^{n_{in}}{W_ia_i}+b$ where z is the output of a single linear neuron, $W_i$'s are the weights from the $n_{in}$ incoming connections and $a_i$'s are the $n_{in}$ inputs. Let us assume $W_i$'s are identically independantly distributed and centred around zero. Similarly, $a_i$'s are independant and centred around zero. The assumption on $a_i$'s will be valid for symmetric activations like tanh. Note that for sigmoid, the outputs will be centred around 0.5 and so for the assumption to become valid, the outputs need to be translated by 0.5 to get centred around zero. Then\n",
    "<br><br>\n",
    "$var(z)$ = $var(\\sum_{i=1}^{n_{in}}{W_ia_i}+b)$\n",
    "<br>\n",
    "&emsp;&emsp;&nbsp; &nbsp; = $\\sum_{i=1}^{n_{in}}{var({W_ia_i})}$\n",
    "<br>\n",
    "&emsp;&emsp;&nbsp; &nbsp; = $\\sum_{i=1}^{n_{in}}{E[W_i]^2var(a_i)+E[a_i]^2var(W_i)+var(W_i)var(a_i)}$ &emsp;&emsp;(see wikipedia article on variance)\n",
    "<br>\n",
    "&emsp;&emsp;&nbsp; &nbsp; = $\\sum_{i=1}^{n_{in}}{var(W_i)var(a_i)}$\n",
    "<br>\n",
    "&emsp;&emsp;&nbsp; &nbsp; = $n_{in}var(W_i)var(a_i)$\n",
    "<br><br>\n",
    "i.e\n",
    "<br>\n",
    "$var(output)$ = $n_{in}var(W_i)var(output)$\n",
    "<br>\n",
    "This implies $var(W_i)$ = $\\frac{1.0}{n_{in}}$\n",
    "\n",
    "Similarly, if it is insisted that in the backward propagation the variance of incoming gradients at a layer should equal variance of outgoing gradients at that layer, we can derive\n",
    "<br>\n",
    "var($W_i$) = $\\frac{1.0}{n_{out}}$ where $n_{out}$ is the number of outgoing connections (in the forward direction) from the layer under consideration.\n",
    "\n",
    "Now both equations can be simultanously satisfied only when $n_{in}$ and $n_{out}$ are equal which may not always be the case. Hence, as a compromise, Xavier Glorot suggested to initialize weights randomly from a distribution with mean zero and variance $\\frac{2.0}{n_{in}+n_{out}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He initialization for ReLU activation\n",
    "ReLU is not a symmetric activation. Therefore the assumption that $a_i$'s are centred around zero is not valid for ReLU. \n",
    "\n",
    "Let $z^{[l]}$ be the random variable denoting linear output at a neuron in layer $l$. Let$W^{[l]}_i$'s be the weights at layer $l$ from the $n^{[l]}_{in}$ incoming connections into the neuron under consideration and $a^{[l-1]}_i$'s be the inputs to the neuron. These are ReLU outputs from the previous neuron. Let us assume $W^{[l]}_i$'s are identically independantly distributed and centred around zero. Let $a^{[l-1]}_i$'s be independant and identically distributed. Let $b^{[l]}$ be the bias. \n",
    "\n",
    "Then\n",
    "<br>\n",
    "$z^{[l]}$ = $\\sum_{i=1}^{n^{[l]}_{in}}{W^{[l]}_ia^{[l-1]}_i}+b^{[l]}$ where $a^{[l-1]}_i$ = max(0, $z^{[l-1]}_i$)\n",
    "<br>\n",
    "$var(z^{[l]})$ = $n^{[l]}_{in}E[{(a^{[l-1]}})^2]var(W^{[l]})$&emsp;&emsp;(see wikipedia article on variance)\n",
    "\n",
    "If we assume $W^{[l-1]}$ are identically independantly distributed and centred around zero and $b^{[l]}$ = $0$, then $z^{[l-1]}$ is distributed symmetrically around zero. Hence $E[{(a^{[l-1]}})^2]$ = $\\frac{1}{2}var(z^{[l-1]})$. This implies\n",
    "<br><br>\n",
    "$var(z^{[l]})$ = $n^{[l]}_{in}\\frac{1}{2}var(z^{[l-1]})var(W^{[l]})$\n",
    "<br><br>\n",
    "Therefore,\n",
    "<br><br>\n",
    "$var(z^{[L]})$ = $var(z^{[1]})(\\prod_{l=2}^{L}{\\frac{1}{2}n^{[l]}_{in}var(W^{[l]})})$ \n",
    "\n",
    "For equating $var(z^{[L]})$ to $var(z^{[1]})$ we require $\\frac{1}{2}n^{[l]}_{in}var(W^{[l]})$ = 1 which implies $var(W^{[l]})$ = $\\frac{1.0}{2n^{[l]}_{in}}$.\n",
    "\n",
    "For the first layer (l = 1), we should have $n^{[l]}_{in}var(W^{[l]})$ = $1$ because there is no ReLU applied on the input signal. But the factor $\\frac{1}{2}$ does not matter if it just exists on one layer.\n",
    "\n",
    "Further, He et al. argues that either forward propagation analysis done above is sufficent or backward propagation analysis that can be done similarly is sufficent. Both are simultaneously not required. Therefore for ReLU activation, weights at layer $l$ can be intialized from a distribution with zero mean and variance equal to $\\frac{1.0}{2n^{[l]}_{in}}$. Bias should be initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Consider a model similar to exercise 2 but with ReLU activations in layers 1 to 4 and sigmoid activation at layer 5. Use He initialization and implement the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layers): # inherits from abstract class Layers\n",
    "    def __init__(self, in_features, out_features, bias = True, regularization = None): # constructor\n",
    "        super().__init__()\n",
    "        self.in_features = in_features # initialize number of incoming features\n",
    "        self.out_features = out_features # initialize number of outgoing features\n",
    "        sigma = np.sqrt(1./(2*in_features))\n",
    "        self.weight = sigma * np.random.randn(out_features, in_features) # initialize weight matrix\n",
    "        if bias:\n",
    "            self.bias = np.zeros((out_features, 1)) # initialize bias if required\n",
    "        else:\n",
    "            self.bias = None # if bias not required, set it to None\n",
    "        self.regularization = regularization # initialize regularization\n",
    "        if self.regularization is not None: \n",
    "            self.reg_penalty = 0 # if valid regularization at this layer, set the regularization penalty\n",
    "                                 # at this layer initially to zero\n",
    "            \n",
    "        self.dw = np.empty_like(self.weight) # intialize dw to empty\n",
    "        self.db = np.empty_like(self.bias) if self.bias is not None else None \n",
    "                                            # initialize db to empty if bias == True\n",
    "        \n",
    "    def forward(self, x): # forward method overridden; x is the incoming activation. \n",
    "                          # shape of x is (num of activations, num of samples)\n",
    "            \n",
    "        m = x.shape[1]    # number of training examples\n",
    "        self.x = x # x is required for backward. We don't need a separate cache. We can store it in the object.\n",
    "        \n",
    "        output = self.weight @ x # computation of the linear part Wx                                 \n",
    "        if self.bias is not None:            \n",
    "            output += self.bias # add to Wx bias if bias == True\n",
    "                                # Note that we don't apply non-linearity here as this layer computes only Wx+b\n",
    "        \n",
    "        #update regularization penalty at this layer\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L2':\n",
    "            self.reg_penalty = args.lamda/(2*m) * np.sum(self.weight**2)\n",
    "        elif self.regularization == 'L1':\n",
    "            self.reg_penalty = args.lamda/m * np.sum(np.abs(self.weight))\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        return output # return forward output as the next layer in the model will require it     \n",
    "    \n",
    "    # Backward of this layer will receive dz. Note that at this layer z = Wx+b. So backward will compute\n",
    "    # dw, db and dx. To compute dW, x is required. That's why x was stored in forward. To compute dx, W is \n",
    "    # required. This is already available in self.weight\n",
    "    \n",
    "    def backward(self, dz): # backward method overridden\n",
    "        m = dz.shape[1]     # number of training examples\n",
    "        self.dw = dz @ self.x.T # compute dw\n",
    "        \n",
    "        # add derivative of regularization penalty at this layer w.r.to w\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L1':\n",
    "            signw = np.sign(self.weight)\n",
    "            signw[signw == 0] = 1\n",
    "            self.dw += args.lamda/m * signw\n",
    "        elif self.regularization == 'L2':\n",
    "            self.dw += args.lamda/m * self.weight\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        if self.bias is not None: # compute db if bias == True\n",
    "            self.db = np.sum(dz, axis = 1, keepdims = True) \n",
    "            \n",
    "        dx = self.weight.T @ dz  # compute dx     \n",
    "        return dx # we only return dx as this this required for chain rule in the next layer \n",
    "                  # in backward direction. dw and db are kept available in this object which will directly\n",
    "                  # be used by update_params for updating weights and biases\n",
    "    \n",
    "    # update parameters in this layer \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.weight -= learning_rate*self.dw\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate*self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        # self.fc1 = Linear(in_features, 32, regularization = 'L2')\n",
    "        self.fc1 = Linear(in_features, 30, regularization='L2')\n",
    "        self.ReLU1 = NonLinear('ReLU')\n",
    "        self.fc2 = Linear(30, 30)\n",
    "        self.ReLU2 = NonLinear('ReLU')\n",
    "        self.fc3 = Linear(30, 30)\n",
    "        self.ReLU3 = NonLinear('ReLU')\n",
    "        self.fc4 = Linear(30, 30)\n",
    "        self.ReLU4 = NonLinear('ReLU')\n",
    "        self.fc5 = Linear(30, 1)\n",
    "        self.Sigmoid = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.ReLU1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.ReLU2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.ReLU3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.ReLU4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.Sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "#         print(\"Model Loss\")\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.Sigmoid.backward(d_output)\n",
    "        dx = self.fc5.backward(dz)\n",
    "        dz = self.ReLU4.backward(dx)\n",
    "        dx = self.fc4.backward(dz)\n",
    "        dz = self.ReLU3.backward(dx)\n",
    "        dx = self.fc3.backward(dz)\n",
    "        dz = self.ReLU2.backward(dx)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dz = self.ReLU1.backward(dx)\n",
    "        dx = self.fc1.backward(dz)\n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "        self.fc3.update_params(learning_rate)\n",
    "        self.fc4.update_params(learning_rate)\n",
    "        self.fc5.update_params(learning_rate)\n",
    "        \n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "parser.add_argument('--print_freq', metavar='N', type=int, default=300, help='print model loss every print_freq iterations')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 2200 --alpha .008 --lamda 1.5'.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6930\n",
      "Loss at iteration 300:\t0.6274\n",
      "Loss at iteration 600:\t0.5294\n",
      "Loss at iteration 900:\t0.4088\n",
      "Loss at iteration 1200:\t0.2337\n",
      "Loss at iteration 1500:\t0.0490\n",
      "Loss at iteration 1800:\t0.0130\n",
      "Loss at iteration 2100:\t0.0061\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 78.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # Instantiate the model\n",
    "    my_model = Model(x.shape[0])\n",
    "    \n",
    "    # train the model\n",
    "    train(my_model, x, y)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Consider exercise 2. Use Xavier initialization and implement the model. Make sure the outputs at each layer are translated to be centred around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layers): # inherits from abstract class Layers\n",
    "    def __init__(self, in_features, out_features, bias = True, regularization = None): # constructor\n",
    "        super().__init__()\n",
    "        self.in_features = in_features # initialize number of incoming features\n",
    "        self.out_features = out_features # initialize number of outgoing features\n",
    "        sigma = np.sqrt(2./(in_features + out_features))\n",
    "        self.weight = sigma * np.random.randn(out_features, in_features) # initialize weight matrix\n",
    "        if bias:\n",
    "            self.bias = np.zeros((out_features, 1)) # initialize bias if required\n",
    "        else:\n",
    "            self.bias = None # if bias not required, set it to None\n",
    "        self.regularization = regularization # initialize regularization\n",
    "        if self.regularization is not None: \n",
    "            self.reg_penalty = 0 # if valid regularization at this layer, set the regularization penalty\n",
    "                                 # at this layer initially to zero\n",
    "            \n",
    "        self.dw = np.empty_like(self.weight) # intialize dw to empty\n",
    "        self.db = np.empty_like(self.bias) if self.bias is not None else None \n",
    "                                            # initialize db to empty if bias == True\n",
    "        \n",
    "    def forward(self, x): # forward method overridden; x is the incoming activation. \n",
    "                          # shape of x is (num of activations, num of samples)\n",
    "            \n",
    "        m = x.shape[1]    # number of training examples\n",
    "        self.x = x # x is required for backward. We don't need a separate cache. We can store it in the object.\n",
    "        \n",
    "        output = self.weight @ x # computation of the linear part Wx                                 \n",
    "        if self.bias is not None:            \n",
    "            output += self.bias # add to Wx bias if bias == True\n",
    "                                # Note that we don't apply non-linearity here as this layer computes only Wx+b\n",
    "        \n",
    "        #update regularization penalty at this layer\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L2':\n",
    "            self.reg_penalty = args.lamda/(2*m) * np.sum(self.weight**2)\n",
    "        elif self.regularization == 'L1':\n",
    "            self.reg_penalty = args.lamda/m * np.sum(np.abs(self.weight))\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        return output # return forward output as the next layer in the model will require it     \n",
    "    \n",
    "    # Backward of this layer will receive dz. Note that at this layer z = Wx+b. So backward will compute\n",
    "    # dw, db and dx. To compute dW, x is required. That's why x was stored in forward. To compute dx, W is \n",
    "    # required. This is already available in self.weight\n",
    "    \n",
    "    def backward(self, dz): # backward method overridden\n",
    "        m = dz.shape[1]     # number of training examples\n",
    "        self.dw = dz @ self.x.T # compute dw\n",
    "        \n",
    "        # add derivative of regularization penalty at this layer w.r.to w\n",
    "        if self.regularization is None:\n",
    "            pass\n",
    "        elif self.regularization == 'L1':\n",
    "            signw = np.sign(self.weight)\n",
    "            signw[signw == 0] = 1\n",
    "            self.dw += args.lamda/m * signw\n",
    "        elif self.regularization == 'L2':\n",
    "            self.dw += args.lamda/m * self.weight\n",
    "        else:\n",
    "            raise ValueError(f'Regularization method{self.regularization} not defined')\n",
    "            \n",
    "        if self.bias is not None: # compute db if bias == True\n",
    "            self.db = np.sum(dz, axis = 1, keepdims = True) \n",
    "            \n",
    "        dx = self.weight.T @ dz  # compute dx     \n",
    "        return dx # we only return dx as this this required for chain rule in the next layer \n",
    "                  # in backward direction. dw and db are kept available in this object which will directly\n",
    "                  # be used by update_params for updating weights and biases\n",
    "    \n",
    "    # update parameters in this layer \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.weight -= learning_rate*self.dw\n",
    "        if self.bias is not None:\n",
    "            self.bias -= learning_rate*self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Layers):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = Linear(in_features, 30, regularization = 'L2')\n",
    "#         self.fc1 = Linear(in_features, 30)\n",
    "        self.sigmoid1 = NonLinear('Sigmoid')\n",
    "        self.fc2 = Linear(30, 30)\n",
    "        self.sigmoid2 = NonLinear('Sigmoid')\n",
    "        self.fc3 = Linear(30, 30)\n",
    "        self.sigmoid3 = NonLinear('Sigmoid')\n",
    "        self.fc4 = Linear(30, 30)\n",
    "        self.sigmoid4 = NonLinear('Sigmoid')\n",
    "        self.fc5 = Linear(30, 1)\n",
    "        self.sigmoid5 = NonLinear('Sigmoid')\n",
    "        \n",
    "    def forward(self, x, train=True):\n",
    "        x = self.fc1(x) # Note that we made classes callable which automatically calls forward method\n",
    "                        # That's why we could call fc1(x) instead of fc1.forward(x). Calls below \n",
    "                        # are in similar line.\n",
    "                        # we could call fc1.forward(x) also.\n",
    "        x = self.sigmoid1(x) - .5\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid2(x) - .5\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid3(x) - .5\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid4(x) - .5\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid5(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, y):\n",
    "        m = output.shape[1]\n",
    "        L = -(1./m) * np.sum(y*np.log(output) + (1-y)*np.log(1-output)) # compute loss\n",
    "        for att in self.__dict__:\n",
    "            if hasattr(att, 'reg_penalty'):\n",
    "                L += att.reg_penalty\n",
    "        return L\n",
    "    \n",
    "    def backward(self, output, y):\n",
    "        epsilon = 1e-6\n",
    "        m = output.shape[1]\n",
    "        d_output = (1./m) * (output-y) * (1./((output*(1-output))+epsilon)) # compute da        \n",
    "        dz = self.sigmoid5.backward(d_output)\n",
    "        dx = self.fc5.backward(dz)\n",
    "        dz = self.sigmoid4.backward(dx)\n",
    "        dx = self.fc4.backward(dz)\n",
    "        dz = self.sigmoid3.backward(dx)\n",
    "        dx = self.fc3.backward(dz)\n",
    "        dz = self.sigmoid2.backward(dx)\n",
    "        dx = self.fc2.backward(dz)\n",
    "        dz = self.sigmoid1.backward(dx)\n",
    "        dx = self.fc1.backward(dz)\n",
    "    \n",
    "    def update_params(self, learning_rate = 0.005):\n",
    "        self.fc1.update_params(learning_rate)\n",
    "        self.fc2.update_params(learning_rate)\n",
    "        self.fc3.update_params(learning_rate)\n",
    "        self.fc4.update_params(learning_rate)\n",
    "        self.fc5.update_params(learning_rate)\n",
    "        \n",
    "    \n",
    "    def __call__(self, *x, train=True):\n",
    "        return self.forward(*x, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t0.6930\n",
      "Loss at iteration 300:\t0.6531\n",
      "Loss at iteration 600:\t0.6458\n",
      "Loss at iteration 900:\t0.6444\n",
      "Loss at iteration 1200:\t0.6440\n",
      "Loss at iteration 1500:\t0.6439\n",
      "Loss at iteration 1800:\t0.6438\n",
      "Loss at iteration 2100:\t0.6437\n",
      "Loss at iteration 2400:\t0.6436\n",
      "Loss at iteration 2700:\t0.6435\n",
      "Loss at iteration 3000:\t0.6434\n",
      "Loss at iteration 3300:\t0.6433\n",
      "Loss at iteration 3600:\t0.6432\n",
      "Loss at iteration 3900:\t0.6430\n",
      "Loss at iteration 4200:\t0.6429\n",
      "Loss at iteration 4500:\t0.6428\n",
      "Loss at iteration 4800:\t0.6427\n",
      "Loss at iteration 5100:\t0.6425\n",
      "Loss at iteration 5400:\t0.6424\n",
      "Loss at iteration 5700:\t0.6423\n",
      "Loss at iteration 6000:\t0.6421\n",
      "Loss at iteration 6300:\t0.6419\n",
      "Loss at iteration 6600:\t0.6418\n",
      "Loss at iteration 6900:\t0.6416\n",
      "Loss at iteration 7200:\t0.6414\n",
      "Loss at iteration 7500:\t0.6412\n",
      "Loss at iteration 7800:\t0.6410\n",
      "Loss at iteration 8100:\t0.6407\n",
      "Loss at iteration 8400:\t0.6404\n",
      "Loss at iteration 8700:\t0.6401\n",
      "Loss at iteration 9000:\t0.6398\n",
      "Loss at iteration 9300:\t0.6395\n",
      "Loss at iteration 9600:\t0.6391\n",
      "Loss at iteration 9900:\t0.6387\n",
      "Loss at iteration 10200:\t0.6382\n",
      "Loss at iteration 10500:\t0.6377\n",
      "Loss at iteration 10800:\t0.6371\n",
      "Loss at iteration 11100:\t0.6364\n",
      "Loss at iteration 11400:\t0.6357\n",
      "Loss at iteration 11700:\t0.6349\n",
      "Loss at iteration 12000:\t0.6339\n",
      "Loss at iteration 12300:\t0.6329\n",
      "Loss at iteration 12600:\t0.6317\n",
      "Loss at iteration 12900:\t0.6303\n",
      "Loss at iteration 13200:\t0.6286\n",
      "Loss at iteration 13500:\t0.6267\n",
      "Loss at iteration 13800:\t0.6245\n",
      "Loss at iteration 14100:\t0.6219\n",
      "Loss at iteration 14400:\t0.6189\n",
      "Loss at iteration 14700:\t0.6152\n",
      "Loss at iteration 15000:\t0.6109\n",
      "Loss at iteration 15300:\t0.6056\n",
      "Loss at iteration 15600:\t0.5992\n",
      "Loss at iteration 15900:\t0.5915\n",
      "Loss at iteration 16200:\t0.5821\n",
      "Loss at iteration 16500:\t0.5705\n",
      "Loss at iteration 16800:\t0.5565\n",
      "Loss at iteration 17100:\t0.5395\n",
      "Loss at iteration 17400:\t0.5191\n",
      "Loss at iteration 17700:\t0.4949\n",
      "Loss at iteration 18000:\t0.4671\n",
      "Loss at iteration 18300:\t0.4357\n",
      "Loss at iteration 18600:\t0.4010\n",
      "Loss at iteration 18900:\t0.3635\n",
      "Loss at iteration 19200:\t0.3236\n",
      "Loss at iteration 19500:\t0.2791\n",
      "Loss at iteration 19800:\t0.2292\n",
      "Loss at iteration 20100:\t0.1840\n",
      "Loss at iteration 20400:\t0.1445\n",
      "Loss at iteration 20700:\t0.1126\n",
      "Loss at iteration 21000:\t0.0885\n",
      "Loss at iteration 21300:\t0.0707\n",
      "Loss at iteration 21600:\t0.0575\n",
      "Loss at iteration 21900:\t0.0477\n",
      "Loss at iteration 22200:\t0.0402\n",
      "Loss at iteration 22500:\t0.0345\n",
      "Loss at iteration 22800:\t0.0300\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 68.00%\n"
     ]
    }
   ],
   "source": [
    "# instantiate the ArgumentParser object\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "parser.add_argument('--print_freq', metavar='N', type=int, default=300, help='print model loss every print_freq iterations')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 23000 --alpha .01'.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required\n",
    "\n",
    "def main(): # main function to train and test the model    \n",
    "    \n",
    "    global args\n",
    "    # load train data\n",
    "    x, y = load_train_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # Instantiate the model\n",
    "    my_model = Model(x.shape[0])\n",
    "    \n",
    "    # train the model\n",
    "    train(my_model, x, y)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(my_model, x, y) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All solutions/answers/analysis to be done in this notebook only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
