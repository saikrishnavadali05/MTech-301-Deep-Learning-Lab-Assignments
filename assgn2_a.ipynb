{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 regularization\n",
    "You would have seen from the previous two assignments that training accuracy on cat classification was much higher (around 99%) compared to test accuracy (around 70%). Clearly this is a case of low bias high variance or overfitting. In this assignment you will attempt to improve the accuracy of the earlier model using regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the required packages. Note that we have moved functions like initialize_params, forward, f, df, predict and test_model into assign2_utils.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse # for command-line parsing\n",
    "import matplotlib # for plotting\n",
    "from matplotlib import pyplot as plt # for plotting\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from assign2_utils import load_train_data, load_test_data, flatten, initialize_params, forward\n",
    "from assign2_utils import df, update_params, test_model \n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two assignments we had initialized many hyperparameters and variables in the main function that trains the model. We will make a few changes in this assignment. First, we will use argparse module and make  hyperparameters/variables as command line arguments. Second, we will rename main to train. The new main function in this assignment will train the model and then test it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code adds some of the hyperparameters and variables as command-line arguments. The help keyword argument of add_argument method gives information about the hyperparameter/variable added as command-line argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 1]\n"
     ]
    }
   ],
   "source": [
    "# instantiate the ArgumentParser object; note that argparse had already been imported\n",
    "parser = argparse.ArgumentParser(description='Train a fully connected network with regularization')\n",
    "\n",
    "# add arguments\n",
    "parser.add_argument('--miter', metavar='N', type=int, default=200, help='max number of iterations to train')\n",
    "parser.add_argument('--alpha', metavar='LEARNING_RATE', type=float, default=0.001, help='initial learning rate')\n",
    "parser.add_argument('--epsilon', metavar='EPSILON', type=float, default=1e-6)\n",
    "parser.add_argument('--nl', metavar='N', default=2, type=int, help='number of layers')\n",
    "parser.add_argument('--nx', metavar='LIST', nargs='+', type=int, default=[32, 1], help='num of nodes in each layer')\n",
    "parser.add_argument('--fname_list', metavar='LIST', nargs='+', default=['ReLU', 'Sigmoid'], \n",
    "                    choices=['ReLU', 'Sigmoid', 'Tanh', 'Linear'], help='non-linearity to be applied at each layer')\n",
    "parser.add_argument('--reg_method', metavar='LIST', nargs='+', default=['L2', 'L2'],\n",
    "                    choices=['NoReg', 'L2', 'L1'], help='regularization to be applied at each layer (excluding ip layer)')\n",
    "parser.add_argument('--lamda', metavar='LAMBDA', type=float, default=1., help='regularization parameter')\n",
    "\n",
    "# parse the arguments. \n",
    "# Since we cannot invoke the code written in jupyter directly from command-line, \n",
    "# we can specify the required arguments in the call to parse_args as shown below with other arguments \n",
    "# left out to use their default values.\n",
    "args = parser.parse_args('--miter 8000 --alpha .0065 --lamda 4.5 '.split()) # you may play with this code by changing\n",
    "                                                                        # the arguments as required\n",
    "print(args.nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the previous cell that number of layers, number of nodes in each layer, regularization method to be applied at each layer, regularization constant etc. are passed as command-line arguments. When there is regularization at a layer, the derivative of loss with respect to weights at that layer will have an added term because of the regularization. You may look at the lecture notes for clarity on this. Below you will complete the blanks in backward method, train method and main method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(da, cache, fname = 'ReLU', reg_method = 'L2'):\n",
    "    \"\"\"\n",
    "      Backward propagates da through the current layer given da, cache and the non-linearity at the current layer\n",
    "      \n",
    "      da: derivative of loss with respect current layers activation a; shape is nx[l] x m\n",
    "      cache: a tuple that contains current layer's linear computation z, previous layer's activation aprev,\n",
    "                     current layer's activation a and weight matrix W between previous layer l-1 and current layer l\n",
    "      fname: name of the non-linearity at current layer l; this will be helpful for local gradient computation in \n",
    "             chain rule\n",
    "      reg_method: regularization method that was applied at this layer\n",
    "      \n",
    "      returns dW: derivative of loss with respect to W; shape is nx[l] x nx[l-1]\n",
    "              db: derivative of loss with respect to b; shape is nx[l] x 1\n",
    "    \"\"\"\n",
    "    m = da.shape[1]\n",
    "    z, aprev, a, W = cache       # extract from cache\n",
    "    \n",
    "    # replace ... with your answer\n",
    "    dz = da * df(z, fname)      # compute dz as incoming grad da * local grad. For local grad, function df defined be useful\n",
    "    dW = np.matmul(dz, aprev.T)  # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                                 # transposing\n",
    "        \n",
    "    # add derivative of regularization penalty at this layer w.r.to W to dW\n",
    "    # replace ... with your answer\n",
    "    if reg_method == 'NoReg':     # no regularization\n",
    "        dW += 0  # one line of code\n",
    "    elif reg_method == 'L1':      # L1 regularization\n",
    "        sgn_w = np.sign(W)\n",
    "        sgn_w[sgn_w == 0] = 1\n",
    "        dW += (args.lamda/m) * sgn_w  # few lines (atmost 4) of code; np.sign will be useful\n",
    "    elif reg_method == 'L2':\n",
    "        dW +=  (args.lamda/m) * W          # one line of code; \n",
    "    else:\n",
    "        raise ValueError('Unkown regularization method')\n",
    "        \n",
    "    # replace ... with your answer\n",
    "    db = np.sum(dz, axis = 1, keepdims=True)      # np.sum will be useful\n",
    "    daprev = W.T @ dz            # np.dot or np.amtmul or @ operator will be useful. Also .T will be useful for \n",
    "                                 # transposing\n",
    "    return daprev, dW, db\n",
    "\n",
    "\n",
    "def train(a0, y, Wlist, blist): # main function to train and test the model  \n",
    "    \"\"\"\n",
    "      Given the input and the labels, this function trains the model\n",
    "      \n",
    "      a0: Input which is a numpy array of shape (nx[0], m); m is the number of samples; nx[0] is num of features\n",
    "      y: label which is a numpy array of shape (m, )\n",
    "      Wlist: a list of all weight matrices\n",
    "      blist: a list of bias vectors\n",
    "      \n",
    "      returns None      \n",
    "    \"\"\"\n",
    "    \n",
    "    # set number of training examples to m\n",
    "    m = a0.shape[1]\n",
    "    epsilon = 1e-6\n",
    "   \n",
    "    # initialize list of caches for each layer, gradients of weights at each layer, gradients of biases at\n",
    "    # each layer to empty\n",
    "    cache, dWlist, dblist = ([None]*args.nl for i in range(3))\n",
    "    dw_list = [[0] * args.miter, [0] * args.miter, [0] * args.miter, [0] * args.miter]\n",
    "    \n",
    "    for i in tqdm(range(args.miter)):\n",
    "        a = a0\n",
    "        # forward propagate through each layer\n",
    "        for l in range(args.nl):\n",
    "            # replace ... with your answer\n",
    "            a, cache[l] = forward(a, Wlist[l], blist[l], args.fname_list[l])     # call forward function with appropriate arguments\n",
    "            \n",
    "        # replace ... with your answer\n",
    "        L = (-1/m) * np.sum((y * np.log(a) + (1-y) * np.log(1-a)))  # compute loss\n",
    "                \n",
    "        # Add regularization penalty to loss; replace ... with your answer\n",
    "        for l in range(args.nl):\n",
    "            if args.reg_method[l] == 'NoReg':\n",
    "                L += 0 # one line of code\n",
    "            elif args.reg_method[l] == 'L1':\n",
    "                L += (args.lamda/m) * np.linalg.norm(Wlist[l], ord=1)\n",
    "            elif args.reg_method[l] == 'L2':\n",
    "                L += (args.lamda/(2*m)) * np.power(np.linalg.norm(Wlist[l]),2)  ########  HERE USE FROBINIUS NORM      ######\n",
    "            else:\n",
    "                raise ValueError('Unknown regularization method error')\n",
    "                \n",
    "        # compute da\n",
    "        # replace ... with your answer\n",
    "#         print(\"SAI: \", L.shape)\n",
    "        da = (1/m) * (a-y) * (1/((a * (1-a)) + epsilon)) # not affected by regularization penalty\n",
    "        \n",
    "        # backward propagate through each layer to compute gradients\n",
    "        for l in range(args.nl-1, -1, -1):\n",
    "            # replace ... by your answer \n",
    "            da, dWlist[l], dblist[l] = backward(da, cache[l], args.fname_list[l], args.reg_method[l]) # call backward function with appropriate arguments                                                 \n",
    "            dw_list[l][i] = np.linalg.norm(dWlist[l])\n",
    "        \n",
    "        # update_params\n",
    "        # call update_params function with appropriate arguments\n",
    "        update_params(Wlist, blist, dWlist, dblist, args.alpha) \n",
    "        \n",
    "        if not i%100: # print loss every 100 iterations\n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.4f}')\n",
    "#                 for l in range(args.nl):\n",
    "#                     print(\"norm(dW)\",l, \": \", np.linalg.norm(dWlist[l]))\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93d51dbd2114826b469ba463d85668e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t1.1194\n",
      "Loss at iteration 100:\t1.0571\n",
      "Loss at iteration 200:\t1.0317\n",
      "Loss at iteration 300:\t0.9951\n",
      "Loss at iteration 400:\t0.9504\n",
      "Loss at iteration 500:\t0.9000\n",
      "Loss at iteration 600:\t0.8430\n",
      "Loss at iteration 700:\t0.7835\n",
      "Loss at iteration 800:\t0.7257\n",
      "Loss at iteration 900:\t0.6756\n",
      "Loss at iteration 1000:\t0.6445\n",
      "Loss at iteration 1100:\t0.6202\n",
      "Loss at iteration 1200:\t0.5932\n",
      "Loss at iteration 1300:\t0.5414\n",
      "Loss at iteration 1400:\t0.4929\n",
      "Loss at iteration 1500:\t0.4779\n",
      "Loss at iteration 1600:\t0.4394\n",
      "Loss at iteration 1700:\t0.4192\n",
      "Loss at iteration 1800:\t0.4079\n",
      "Loss at iteration 1900:\t0.3878\n",
      "Loss at iteration 2000:\t0.3773\n",
      "Loss at iteration 2100:\t0.3653\n",
      "Loss at iteration 2200:\t0.3551\n",
      "Loss at iteration 2300:\t0.3457\n",
      "Loss at iteration 2400:\t0.3378\n",
      "Loss at iteration 2500:\t0.3300\n",
      "Loss at iteration 2600:\t0.3225\n",
      "Loss at iteration 2700:\t0.3156\n",
      "Loss at iteration 2800:\t0.3089\n",
      "Loss at iteration 2900:\t0.3028\n",
      "Loss at iteration 3000:\t0.2968\n",
      "Loss at iteration 3100:\t0.2912\n",
      "Loss at iteration 3200:\t0.2857\n",
      "Loss at iteration 3300:\t0.2805\n",
      "Loss at iteration 3400:\t0.2755\n",
      "Loss at iteration 3500:\t0.2706\n",
      "Loss at iteration 3600:\t0.2660\n",
      "Loss at iteration 3700:\t0.2615\n",
      "Loss at iteration 3800:\t0.2571\n",
      "Loss at iteration 3900:\t0.2529\n",
      "Loss at iteration 4000:\t0.2488\n",
      "Loss at iteration 4100:\t0.2449\n",
      "Loss at iteration 4200:\t0.2410\n",
      "Loss at iteration 4300:\t0.2373\n",
      "Loss at iteration 4400:\t0.2337\n",
      "Loss at iteration 4500:\t0.2302\n",
      "Loss at iteration 4600:\t0.2268\n",
      "Loss at iteration 4700:\t0.2235\n",
      "Loss at iteration 4800:\t0.2202\n",
      "Loss at iteration 4900:\t0.2172\n",
      "Loss at iteration 5000:\t0.2141\n",
      "Loss at iteration 5100:\t0.2112\n",
      "Loss at iteration 5200:\t0.2083\n",
      "Loss at iteration 5300:\t0.2055\n",
      "Loss at iteration 5400:\t0.2029\n",
      "Loss at iteration 5500:\t0.2002\n",
      "Loss at iteration 5600:\t0.1977\n",
      "Loss at iteration 5700:\t0.1952\n",
      "Loss at iteration 5800:\t0.1928\n",
      "Loss at iteration 5900:\t0.1905\n",
      "Loss at iteration 6000:\t0.1882\n",
      "Loss at iteration 6100:\t0.1860\n",
      "Loss at iteration 6200:\t0.1838\n",
      "Loss at iteration 6300:\t0.1818\n",
      "Loss at iteration 6400:\t0.1797\n",
      "Loss at iteration 6500:\t0.1777\n",
      "Loss at iteration 6600:\t0.1758\n",
      "Loss at iteration 6700:\t0.1739\n",
      "Loss at iteration 6800:\t0.1721\n",
      "Loss at iteration 6900:\t0.1704\n",
      "Loss at iteration 7000:\t0.1687\n",
      "Loss at iteration 7100:\t0.1670\n",
      "Loss at iteration 7200:\t0.1654\n",
      "Loss at iteration 7300:\t0.1638\n",
      "Loss at iteration 7400:\t0.1623\n",
      "Loss at iteration 7500:\t0.1608\n",
      "Loss at iteration 7600:\t0.1593\n",
      "Loss at iteration 7700:\t0.1579\n",
      "Loss at iteration 7800:\t0.1565\n",
      "Loss at iteration 7900:\t0.1552\n",
      "\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 76.00%\n"
     ]
    }
   ],
   "source": [
    "def main(): # main function to train and test the model\n",
    "    \n",
    "    # certain assertions\n",
    "    assert args.nl >= 2, 'Number of layers in the network should be atleast 2 excluding ip layer.'\n",
    "    assert len(args.nx) == args.nl, '''Excluding ip layer, number of nodes for each layer must be specified.\n",
    "                                             Not more, not less.'''\n",
    "    assert len(args.fname_list) == args.nl, '''Non-linearities to be applied at each layer must be \n",
    "                                                         specified, excluding input layer. Not more, not less.'''\n",
    "    assert len(args.reg_method) == args.nl, '''Requirement of regularization must be mentioned for \n",
    "                                               every layer, excluding ip layer. Not more, not less.'''\n",
    "    \n",
    "    # load train data\n",
    "    a0, y = load_train_data()\n",
    "    a0 = flatten(a0)\n",
    "    a0 = a0/255. # normalize the data to [0, 1]     \n",
    "    \n",
    "    # insert num of features in i/p layers to args.nodelist\n",
    "    args.nx.insert(0, a0.shape[0])    \n",
    "    \n",
    "    # initialize weights and biases\n",
    "    Wlist, blist = initialize_params(args.nx)\n",
    "    \n",
    "    # train the model\n",
    "    train(a0, y, Wlist, blist)\n",
    "    \n",
    "    # test the model\n",
    "    print(f'train accuracy: {test_model(a0, y, Wlist, blist, args.fname_list) * 100:.2f}%')\n",
    "\n",
    "    x, y = load_test_data()\n",
    "    x = flatten(x)\n",
    "    x = x/255. # normalize the data to [0, 1]\n",
    "    print(f'test accuracy: {test_model(x, y, Wlist, blist, args.fname_list) * 100:.2f}%')\n",
    "    \n",
    "    return\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Did regularization have any effect at all? If not why? If yes, which regularization did better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ans: We see that regularization doesn't have the desired effect. So, we can say that the overfitting is not due to noise but mostly due to lack of representative samples in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the train data into 90% train data and 10% valid data randomly. For every training iteration, compute validation loss and validation accuracy. Plot train loss, valid loss, train accuracy and valid accuracy against the number of iterations. Make your observations based on these plots. For splitting, np.random.shuffle and np.random.randint may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_loss(a0, y, valid_data, valid_labels, Wlist, blist):\n",
    "\n",
    "    train_loss = [None] * args.miter\n",
    "    valid_loss = [None] * args.miter\n",
    "    train_accr = [None] * args.miter\n",
    "    valid_accr = [None] * args.miter\n",
    "    \n",
    "    m = a0.shape[1]\n",
    "    m1 = valid_data.shape[1]\n",
    "    epsilon = 1e-6\n",
    "   \n",
    "    cache, cache1, dWlist, dblist = ([None]*args.nl for i in range(4))\n",
    "    \n",
    "    for i in tqdm(range(args.miter)):\n",
    "        # FORWARD pass training data\n",
    "        a = a0\n",
    "        for l in range(args.nl):\n",
    "            a, cache[l] = forward(a, Wlist[l], blist[l], args.fname_list[l])     \n",
    "            \n",
    "        L = (-1/m) * np.sum((y * np.log(a) + (1-y) * np.log(1-a)))\n",
    "        \n",
    "        \n",
    "        # FORWARD pass for validation data\n",
    "        a1 = valid_data\n",
    "        for l in range(args.nl):\n",
    "            a1, cache1[l] = forward(a1, Wlist[l], blist[l], args.fname_list[l])\n",
    "        \n",
    "        L_valid = (-1/m1) * np.sum((valid_labels * np.log(a1) + (1-valid_labels) * np.log(1-a1)))         \n",
    "        \n",
    "        \n",
    "        for l in range(args.nl):\n",
    "            if args.reg_method[l] == 'NoReg':\n",
    "                L += 0\n",
    "                L_valid += 0\n",
    "            elif args.reg_method[l] == 'L1':\n",
    "                L += (args.lamda/m) * np.linalg.norm(Wlist[l], ord=1)\n",
    "                L_valid += (args.lamda/m1) * np.linalg.norm(Wlist[l], ord=1)\n",
    "            elif args.reg_method[l] == 'L2':\n",
    "                L += (args.lamda/(2*m)) * np.power(np.linalg.norm(Wlist[l]),2)  \n",
    "                L_valid += (args.lamda/(2*m1)) * np.power(np.linalg.norm(Wlist[l]),2)  \n",
    "            else:\n",
    "                raise ValueError('Unknown regularization method error')\n",
    "                \n",
    "        da = (1/m) * (a-y) * (1/((a * (1-a)) + epsilon)) \n",
    "        \n",
    "        for l in range(args.nl-1, -1, -1):\n",
    "            da, dWlist[l], dblist[l] = backward(da, cache[l], args.fname_list[l], args.reg_method[l]) \n",
    "\n",
    "        update_params(Wlist, blist, dWlist, dblist, args.alpha) \n",
    "        \n",
    "        train_loss[i] = L\n",
    "        train_accr[i] = test_model(a0, y, Wlist, blist, args.fname_list) * 100\n",
    "        valid_loss[i] = L_valid\n",
    "        valid_accr[i] = test_model(valid_data, valid_labels, Wlist, blist, args.fname_list) * 100\n",
    "        if not i%100: \n",
    "                print(f'Loss at iteration {i}:\\t{np.asscalar(L):.8f}')\n",
    "                \n",
    "    return train_loss, train_accr, valid_loss, valid_accr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12289, 209)\n",
      "(12289, 209)\n",
      "(12288, 189) (189,)\n",
      "(12288, 20) (20,)\n"
     ]
    }
   ],
   "source": [
    "train_valid_split = 10     # percentage of data for validation\n",
    "\n",
    "\n",
    "a0, y = load_train_data()\n",
    "a0 = flatten(a0)\n",
    "a0 = a0/255. # normalize the data to [0, 1]     \n",
    "\n",
    "y1 = np.reshape(y, (1,209))\n",
    "# print(a0.shape, y1.shape)\n",
    "x = np.append(arr=a0, axis=0, values=y1)\n",
    "print(x.shape)\n",
    "x = x.T\n",
    "np.random.shuffle(x)\n",
    "X = x.T\n",
    "x_valid = X[:, :int(209 * train_valid_split / 100)]\n",
    "x_train = X[:,int(209 * train_valid_split / 100):]\n",
    "train_labels = x_train[12288:,]\n",
    "valid_labels = x_valid[12288:,]\n",
    "# tmp = x[:,0]\n",
    "print(X.shape)\n",
    "\n",
    "train_labels = np.reshape(train_labels, (train_labels.shape[1],))\n",
    "valid_labels = np.reshape(valid_labels, (valid_labels.shape[1],))\n",
    "x_train = np.delete(arr=x_train, obj=12288, axis=0)\n",
    "x_valid = np.delete(arr=x_valid, obj=12288, axis=0)\n",
    "\n",
    "print(x_train.shape, train_labels.shape)\n",
    "print(x_valid.shape, valid_labels.shape)\n",
    "\n",
    "# validation_indices = np.random.randint(0, 209, 209 * train_valid_split)\n",
    "# valid_data = [None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34651da995764cf99b34b3e6cc98bd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0:\t1.16650175\n",
      "Loss at iteration 100:\t1.10001964\n",
      "Loss at iteration 200:\t1.07274304\n",
      "Loss at iteration 300:\t1.03299427\n",
      "Loss at iteration 400:\t0.98441755\n",
      "Loss at iteration 500:\t0.93449022\n",
      "Loss at iteration 600:\t0.87561971\n",
      "Loss at iteration 700:\t0.81255376\n",
      "Loss at iteration 800:\t0.75464543\n",
      "Loss at iteration 900:\t0.70429822\n",
      "Loss at iteration 1000:\t0.67075505\n",
      "Loss at iteration 1100:\t0.62484294\n",
      "Loss at iteration 1200:\t0.62071863\n",
      "Loss at iteration 1300:\t0.55944274\n",
      "Loss at iteration 1400:\t0.51108216\n",
      "Loss at iteration 1500:\t0.48181063\n",
      "Loss at iteration 1600:\t0.46719809\n",
      "Loss at iteration 1700:\t0.44503831\n",
      "Loss at iteration 1800:\t0.42124108\n",
      "Loss at iteration 1900:\t0.40702195\n",
      "Loss at iteration 2000:\t0.39372972\n",
      "Loss at iteration 2100:\t0.38036180\n",
      "Loss at iteration 2200:\t0.36952997\n",
      "Loss at iteration 2300:\t0.35961373\n",
      "Loss at iteration 2400:\t0.35029917\n",
      "Loss at iteration 2500:\t0.34219030\n",
      "Loss at iteration 2600:\t0.33424648\n",
      "Loss at iteration 2700:\t0.32619823\n",
      "Loss at iteration 2800:\t0.31886297\n",
      "Loss at iteration 2900:\t0.31199959\n",
      "Loss at iteration 3000:\t0.30563904\n",
      "Loss at iteration 3100:\t0.29927148\n",
      "Loss at iteration 3200:\t0.29320571\n",
      "Loss at iteration 3300:\t0.28746276\n",
      "Loss at iteration 3400:\t0.28188649\n",
      "Loss at iteration 3500:\t0.27664800\n",
      "Loss at iteration 3600:\t0.27155941\n",
      "Loss at iteration 3700:\t0.26653801\n",
      "Loss at iteration 3800:\t0.26182640\n",
      "Loss at iteration 3900:\t0.25724441\n",
      "Loss at iteration 4000:\t0.25280891\n",
      "Loss at iteration 4100:\t0.24850710\n",
      "Loss at iteration 4200:\t0.24439691\n",
      "Loss at iteration 4300:\t0.24031758\n",
      "Loss at iteration 4400:\t0.23637346\n",
      "Loss at iteration 4500:\t0.23265986\n",
      "Loss at iteration 4600:\t0.22896863\n",
      "Loss at iteration 4700:\t0.22554201\n",
      "Loss at iteration 4800:\t0.22211090\n",
      "Loss at iteration 4900:\t0.21875745\n",
      "Loss at iteration 5000:\t0.21546985\n",
      "Loss at iteration 5100:\t0.21248587\n",
      "Loss at iteration 5200:\t0.20946286\n",
      "Loss at iteration 5300:\t0.20647839\n",
      "Loss at iteration 5400:\t0.20372060\n",
      "Loss at iteration 5500:\t0.20089947\n",
      "Loss at iteration 5600:\t0.19826446\n",
      "Loss at iteration 5700:\t0.19569326\n",
      "Loss at iteration 5800:\t0.19333667\n",
      "Loss at iteration 5900:\t0.19078058\n",
      "Loss at iteration 6000:\t0.18857038\n",
      "Loss at iteration 6100:\t0.18621642\n",
      "Loss at iteration 6200:\t0.18399539\n",
      "Loss at iteration 6300:\t0.18189851\n",
      "Loss at iteration 6400:\t0.17979283\n",
      "Loss at iteration 6500:\t0.17791134\n",
      "Loss at iteration 6600:\t0.17596730\n",
      "Loss at iteration 6700:\t0.17410468\n",
      "Loss at iteration 6800:\t0.17225862\n",
      "Loss at iteration 6900:\t0.17047202\n",
      "Loss at iteration 7000:\t0.16868014\n",
      "Loss at iteration 7100:\t0.16702348\n",
      "Loss at iteration 7200:\t0.16540850\n",
      "Loss at iteration 7300:\t0.16385855\n",
      "Loss at iteration 7400:\t0.16240286\n",
      "Loss at iteration 7500:\t0.16095753\n",
      "Loss at iteration 7600:\t0.15948769\n",
      "Loss at iteration 7700:\t0.15816740\n",
      "Loss at iteration 7800:\t0.15683535\n",
      "Loss at iteration 7900:\t0.15543742\n",
      "\n",
      "train accuracy: 100.00%\n",
      "test accuracy: 72.00%\n"
     ]
    }
   ],
   "source": [
    "a0 = x_train\n",
    "# insert num of features in i/p layers to args.nodelist\n",
    "\n",
    "# args.nx.insert(0, a0.shape[0])    \n",
    "# initialize weights and biases\n",
    "Wlist, blist = initialize_params(args.nx)\n",
    "\n",
    "# train the model\n",
    "train_loss, train_accr, valid_loss, valid_accr = train_with_loss(a0, train_labels, x_valid, valid_labels, Wlist, blist)\n",
    "\n",
    "# test the model\n",
    "print(f'train accuracy: {test_model(a0, train_labels, Wlist, blist, args.fname_list) * 100:.2f}%')\n",
    "x, y = load_test_data()\n",
    "x = flatten(x)\n",
    "x = x/255. # normalize the data to [0, 1]\n",
    "print(f'test accuracy: {test_model(x, y, Wlist, blist, args.fname_list) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i for i in range(1, args.miter+1)]\n",
    "plt.plot(x_axis, train_loss, 'b', label='train_loss')\n",
    "plt.plot(x_axis, valid_loss, 'r', label='valid_loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_axis, train_accr, 'b', label='train_accr')\n",
    "plt.plot(x_axis, valid_accr, 'r', label='valid_accr')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy (in percentage)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that both the train loss and the validation loss decreases but validation loss remains much higher than the train loss.\n",
    "#### For the train and validation accuracies, we observe that initially they improve very sharply followed by some variations. Then, after touching their peak values, validation loss strarts to vary a lot fall showing over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All questions will be answered in the jupyter notebook only. Wherever code is required you write and run the code in a code cell. For text, write and render in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
